\documentclass[anon,12pt]{colt2019} % Anonymized submission
% \documentclass[12pt]{colt2019} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Komols Complexity, Coresets, and Sketches in Machine Learning]{Komols Complexity, Coresets, and Sketches in Machine Learning}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{ amssymb }

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
%\newtheorem*{definition*}{Definition}
%\newtheorem{definition}{Definition}
%\newtheorem{claim}{Claim}
%\newtheorem{theorem}{Theorem}


\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
%\usepackage{ntheorem}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\disc}{\text{disc}}
\renewcommand{\Pr}{\operatorname{Pr}}

% Authors with different addresses:
\coltauthor{%
 \Name{Zohar Karnin} \Email{zkarnin@amazon.com}%\\ \addr Address 1
 \AND
 \Name{Edo Liberty} \Email{libertye@amazon.com}%\\ \addr Address 2%
}


\begin{document}

\maketitle

\begin{abstract}
\el{I need to rewrite the abstract}
This paper defines the Komlos complexity for families of functions which is intimately related to its discrepancy properties.
We argue that this measure of complexity captures properties like minimal coreset sizes for different problems in machine learning. We show that for many well known problems the Komlos complexity is asymptotically lower than their Rademacher complexity. This proves the existence smaller coresets than those achievable by random sampling. 
New results for coresets for kernel density estimation, matrix covariance approximation, and logistic regression are presented both as examples of for providing mechanisms to compute the Komlos complexity of other classes.
For all of the above we match the state of the art or improve on it.
This makes an explicit connection between low discrepancy, streaming algorithms, coresets, and learnability. 
%We also asymptotically improve on the generic merge-reduce framework for creating mergeable sketches for all the above problems. 
\end{abstract}


\section{Introduction}

\el{I need to rewrite this section.}
In many machine learning and sketching problems our goal is often to approximate the sum (or expectation) of functions.
Specifically, function of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i \in \mathcal X$ are either training examples or stream items and $q \in \mathcal Q$ is either the model parameters or a query from a possible set $\Q$ which is known in advance. For simplicity and throughout this manuscript we will assume $f:\X\times\Q\rightarrow [0,1]$.
The goal is to produce weights $w$ with at most $k \ll n$ non-zeros such that $\sum_{i=1}^{n}w_i f(x_i,q) := \tilde F(q)$ approximates $F(q)$.
Approximation here means that $|\tilde F(q)  - F(q)| \le \eps n$ either for all possible queries $q \in \mathcal Q$ simultaneously or for every fixed $q$ with probability at least $1-\delta$. 
There are more complicated formulations such as weak coresets which we will not touch upon in this manuscript.

Generating a concise representation $\tilde F$ for $F$ allows one to optimize over $\tilde F$ instead of $F$ which is more efficient. 
Moreover, if the resulting sketches or coresets are mergable, this could be done on separate machines without the need for communication or assuming randomness in the partitioning.

%Here are some examples:
%\begin{itemize}
%\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else. Here both $x$ and $q$ belong to some finite domain.
%\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else. Here, $x$ and $q$ belong to a set which exhibits total ordering.
%\item in matrix column subset selection we have $f(x, q) = \langle x,q \rangle ^2$ where $q$ is any unit vector in $\R^d$ and $x$ are matrix rows. For simplicity, we will assume in this manuscript that $x$ is also unit norm.
%\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
%\item in logistic regression $f(x, q) = \operatorname{loss}(x,q) = \log(1 + \exp(-q^T x))$. 
%Here $q$ is the parameters of the linear model and $x$ is a training example vector multiplied by its label ($1$ or $-1$).
%\item Linear Classification $f(x,q) = 1$ if $x^Tq > 0$ and $0$ else. Here, like in Logistic Regression, $x$ is the example point multiplied by its label and $q$ is the weights of the linear classifier.
%\item Fractional Satisfiability $f(x,q)$ if $q$ satisfies $f$ and $0$ otherwise. Here $x$ in a clause in conjunctive normal form and $q$ is a boolean value assignment to the variables. For example $f(x,q) = 1$ iff $(q_1 \vee q_5  \vee \neg q_8)$ is true.
%\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q = \sum_{i,j\in E} (q_i - q_i)^2$ where $x$ is an edge and $L_x$ is the graph laplacian corresponding to the graph with a single edge $x$ and $q$ is a test vector.
%\item Probably also good for matrix approximation when we get entry updates. 
%\end{itemize}









For bounded functions $f$ like above, uniform sampling of $\ell = O(\log(1/\delta)/\eps^2)$ combined with a union bound over $|\mathcal Q|$ always provides a valid solution using $\ell = O(\log(|\mathcal Q|)/\eps^2)$ stream items. 
While $|\mathcal Q|$ is often infinite it reduces to a finite (albeit usually exponentially large) set through standard epsilon net arguments. 
%This is common practice in machine learning, specifically, PAC learning. However, it is often far from being the smallest cardinality coreset possible.
We argue that we can offer a solution for producing smaller coresets for all of the above problems in a unified manner. 
Moreover, our solution creates streaming algorithms with fully mergeable sketches. 
The size of the coreset above appears to be intimately tied to the discrepancy properties of the associated functions.
This is captured by its Komlos complexity.  


\section{Komlos Complexity}
We begin by formally defining the Komols complexity and by giving some general results about for problems with low Komols complexity. The Komols complexity of a sequence set is defined similarly to it Rademacher complexity.
%\begin{definition}
%Let $A \subset [0,1]^m$ and $\sigma \in \{-1,1\}^m$ uniformly. 
%The Rademacher complexity of $\F$ is 
%\[
%R_m(\F) =  \frac{1}{m}\E_{\sigma} \sup_{a \in A}  \sum_{i=1}^{m}\sigma_i a_i
%\]
%\end{definition}
%
%\begin{definition}
%Let $A \subset [0,1]^m$ and $\sigma \in \{-1,1\}^m$. 
%The Komlos complexity of $A$ is 
%\[
%K_m(A) =  \frac{1}{m}\min_{\sigma} \sup_{a \in A}  \left|\sum_{i=1}^{m}\sigma_i a_i\right|
%\]
%\end{definition}

\begin{definition}
Let $\F$ be a family of functions $f:\X\rightarrow\R$ and $\sigma \in \{-1,1\}^m$ uniformly. 
The Rademacher complexity of $\F$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
R_m(\F) =  \frac{1}{m}\E_{\sigma} \sup_{f \in \F}  \sum_{i=1}^{m}\sigma_i f(x_i)
\]
\end{definition}
%
\begin{definition}
Let $\F$ be a family of functions $f:\X\rightarrow\R$ and $\sigma \in \{-1,1\}^m$. 
The Komlos complexity of $\F$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
K_m(\F) =  \frac{1}{m}\min_{\sigma} \sup_{f \in \F}  \left|\sum_{i=1}^{m}\sigma_i f(x_i)\right|
\]
The Komlos Complexity $K_m(\F)$ without a reference set $\{x_1,\ldots,x_m\}$ is the upper bound on any such set.
\end{definition}



\noindent It well known that $R_m$ is a good measure for generalization when data is chosen uniformly at random (or drawn from an unknown distribution). See \cite{Bartlett:2003:RGC:944919.944944} for other notions of complexity and relationships between them. We claim that $K_m$ is the measure more suitable for active learning, for creating coresets, and for creating mergeable sketches.

\el{Add a much longer explanation about the connection to discrepancy and the ideas behind this construction} The name Komlos Complexity is due to the Komlos Conjecture. It claims that $\min_\sigma \max_f \|\sum_i \sigma_i f_i\|_\infty = O(1)$ for unit vectors $f$. The correctness of the Komlos Conjecture is still a fundamental open problem in the study of discrepancy theory.  The definition is useful, however, regardless of the correctness or provability of the conjecture.  
It is obvious that  $K_m(\F) \le R_m(\F)$ for any family $\F$. We show later that $K_m(\F) = o(R_m(\F))$ for a wide range of interesting problems in machine learning.


%\begin{theorem}
%Let $F(a) = \sum_i a_i$ for $a \in A$ and let $K_m(A) = O(c/m^\gamma)$ for any constant $\gamma \in (0,1]$.
%There exist a coreset for $A$ such that $\tilde{F}(a) = \sum_{i} w_i a_i$, $\operatorname{supp}(w) \le m$ and 
%$$
%\forall \; a \in A \;\;\;  |F(a) - \tilde F(a)| \le \eps n
%$$
%\end{theorem}
%\begin{proof}
%By halving the points repeatedly \el{TODO}.
%\end{proof}
\subsection{General Framework for Coresets}
Consider the function $F = \sum_{i=1}^{m} f(x_i,q)$ and the signed sum of error function $E(q) = \sum_{i=1}^{m} \sigma_i f(x_i,q)$ where $\sigma_i \in \{-1,1\}$.
Now consider $F_1(q) = F(q) + E(q) = \sum_{i=1}^{m} f(x_i,q)  + \sum_{i=1}^{m} \sigma_i f(x_i,q)  = 2 \sum_{i | \sigma_i=1} f(x_i,q)$ and similarly $F_{-1}$. We have that both $F_1(q), F_{-1}(q)$ are approximations for $F(q)$ obtained by coresets of item weights of $2$ and an error at most $|F_1(q) - F(q)| =  |E(q)|$.
%
The above is true for any choice of signs $\sigma_i$, specifically, for those minimizing $\max_q | E(q)|$.
Note that we can select signs such that $|E(q)| \le m K_m(\F)$.
A function is sketch-able if $K_m(\F) = O(m^\gamma)$ for any constant $\gamma \in [0,1)$.
Typically we see that $K_m(\F) = O(c/m)$ or $K_m(\F)= c/\sqrt{m}$ for some $c$ which does not depend on $m$.

\begin{fact}
For any function family $\F$ with a corresponding Komlos Complexity $K_m(\F) = O(c/m)$ there exists a coreset of size 
$O(c/\eps)$ whose error is at most $\eps n$.
\end{fact}
\begin{fact}
For any function family $\F$ with a corresponding Komlos Complexity $K_m(\F) = O(c/\sqrt{m})$ there exists a coreset of size 
$O(c/\eps^2)$ whose error is at most $\eps n$.
\end{fact}

\noindent Proving both facts is trivial. 
At step one, create a coreset of size at most $n/2$ of items of weight $2$ and incur an error of at most $c$ or $c \sqrt{n}$.
Then, we repeat the process and create a coreset of size $n/4$ of items of weight $4$. Here, you incur error of $2c$ or $2c\sqrt{n/2}  = \sqrt{2n}$.
Note that the sum of errors is asymptotically dominated by last iterations. 
Halting the compression at $\Theta(c/\eps)$ or $\Theta(c/\eps^2)$ items respectively achieves the goal.


%\begin{theorem}
%Let $A$ be of the special form $a_i = f(x_i,q) \in [0,1]$ and $K_m(A) = O(c/\sqrt{m})$.
%There exist and streaming mergeable coreset for $A$ with cardinality at most $\ell = O(c\log^{3}\log(1/\delta)/\eps^2)$.
%That is, let $F(q) = \sum_i f(x_i,q)$. One can process a stream of item $x_1,\ldots,x_n$ and produce $\tilde{F}(q) = \sum_{i} w_i a_i$ with $\operatorname{supp}(w) \le \ell$ such that 
%$$
%\forall \; q \in Q \text{\;\;with probability\;\;} 1-\delta  \;\;\;  |F(q) - \tilde F(q)| \le \eps n
%$$
%\end{theorem}
%
%
%\begin{theorem}
%Let $A$ be of the special form $a_i = f(x_i,q) \in [0,1]$ and $K_m(A) = O(c/m)$.
%There exist and streaming mergeable coreset for $A$ with cardinality at most $\ell = O(c\log^{2}\log(1/\delta)/\eps)$.
%That is, let $F(q) = \sum_i f(x_i,q)$. One can process a stream of item $x_1,\ldots,x_n$ and produce $\tilde{F}(q) = \sum_{i} w_i a_i$ with $\operatorname{supp}(w) \le \ell$ such that 
%$$
%\forall \; q \in Q \text{\;\;with probability\;\;} 1-\delta  \;\;\;  |F(q) - \tilde F(q)| \le \eps n
%$$
%\end{theorem}

\subsection{General Framework for Sketching}


The high level idea is the following. 
Consider a stream of the data points $x_i$. 
We maintain a buffer that can hold up to $m$ points.
Once collecting $m$ data points it performs a \emph{compact} operation that outputs at most $m/2$ data points of weight $2$ representing the original $m$ inputs. The compact operation partitions the $m$ points into two sets whose corresponding sums are very close. 
This is made formal with the following definitions: 

 %
In what follows, we prove the following.
\begin{theorem} \label{thm:streaming}
For any function $f$ with a corresponding Komlos Complexity $K_m = O(c/m)$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(c\log^2\left(\min(n, \log(1/\delta)) \right)/\eps\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require a priori knowledge of the stream length $n$.
\end{theorem}


\begin{theorem}
For any function $f$ with a corresponding Komlos Complexity of $K_m = O(c/\sqrt{m})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(c\log^3\left(\min(n, \log(1/\delta)) \right)/\eps^2\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require a priori knowledge of the stream length $n$.
\end{theorem}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
%$O(\log^2(\log(1/\delta))/\eps^2)$ whose error is at most $\eps n$ and failure probability at most $\delta$.
%\end{fact}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(1)$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps)$ whose error is at most $\eps n$.
%\end{fact}
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps^2)$ whose error is at most $\eps n$.
%\end{fact}


\section{The Complexity of Analytic Functions of Dot Products}

We provide a coreset suitable for analytical functions of the inner product $\ip{q,x}$ or distance $\|q-x\|$. The idea is to find a set of signs that simultaneously balance $\ip{q,x}^k$ for all powers $k$ and unit vectors $q$. By controlling all powers of $\ip{q,x}$ we control any sum of these powers. It follows that this coreset can be used to control, for example, the logistic loss function $L(q,x) = \log(1+\exp(\ip{q,x}))$ or the gaussian Kernel $K(q,x) = \exp(-\lambda \|q-x\|^2)$. 


We start with some notation and trivial properties. 
For a vector $q \in \R^d$ let $q^{\otimes k}$ represent the $k$-dimensional tensor obtained from the outer product of $q$ with itself $k$ times. For a $k$ dimensional tensor with $d^k$ entries $X$ we consider the measure
$\|X\|_{T_k} = \max_{q \in \R^d, \|q\|=1} \left| \langle X, q^{\otimes k}\rangle \right|$.
\begin{fact}
$\|X\|_{T_k}$ is a norm
\end{fact}
\begin{proof}
We prove the claim directly from the definition of a norm.
Notice that for any $X \neq 0$, $\ip{X, q^{\otimes}}$ is a non-zero polynomial in $q$. It follows that there must be $q$ for which its value is non-zero, meaning that $\|X\|_{T_k}=0$ iff $X=0$. For a scalar $a$, we clearly have by definition that
$\|aX\|_{T_k} = |a|\|X\|_{T_k}$.  Lastly, by the max definition we clearly have
$ \|X+Y\|_{T_k} =  \max_q \left| \langle X+Y, q^{\otimes k}\rangle \right| \leq 
\max_q \left| \langle X, q^{\otimes k}\rangle \right| + \max_q\left| \langle Y, q^{\otimes k}\rangle \right| = \|X\|_{T_k} + \|Y\|_{T_k}$
\end{proof}

We are now ready for the lemma controlling all powers of inner products simultaneously. 

\begin{lemma}\label{uc}
For any set of vectors $x_i \in \R^d$ there exist a set of signs $\sigma_i$ such that for all $k$ simultaneously $\left\| \sum_i \sigma_i x_i^{\otimes k} \right\|_{T_k} \le O(\sqrt{d k\log^{3}{k}})$ (the $3$ power of the term $\log(k)$ can be reduced to any constant power larger than $2$). 
\end{lemma}
\begin{proof}
The proof will use Banaszczyk's theorem. 
Let $\mathcal K$ be a convex body in Euclidean space with Gaussian measure at least 1/2 ($\Pr[g \in \mathcal K] \ge 1/2$ when $g$ is i.i.d.\ Gaussian).
Let $x_1,\ldots,x_n$ be vectors with $\|x_i\| =O(1)$. 
Then, there exist signs $\sigma_i$ such that $\sum \sigma_i x_i \in C \mathcal K$ for some constant $C$.

To use Banaszczyk's theorem we begin with defining our convex body.
Define the norm $\|\psi\|_T$ of a vector $\psi$ as follows. Look at the first $d$ coordinates of $\psi$ as a vector $\psi_1$, the next $d^2$ coordinates of $\psi$ as a matrix $\psi_2$ the next $d^3$ coordinates as a three tensor $\psi_3$ etc.
We define $\|\psi\|_T = \max_k \|\psi_k\|_{T_k} /\sqrt{\log(k)}$. 
Here, $\|\cdot\|_{T_k}$ is the special spectral norm defined in the beginning of the section 
The maximum over norms of subvectors is clearly a norm in itself, meaning that $\|\cdot \|_T$ is indeed a norm. It follows that  the set $\mathcal K  = \{\psi \; | \; \|\psi\|_T \le c\sqrt{d}\}$ is convex. 

We now need to show that the Gaussian measure of $\mathcal K$ is at least $1/2$. 
That is, with probability at least $1/2$ a vector of random Gaussian entrees $g$ belongs to $\mathcal K$.
Consider a random i.i.d.\ Gaussian Tensor $g_k \in \R^{d^k}$. 

A trivial modification of Theorem 1 from \cite{tomioka2014spectral} shows that $\Pr[\|g_k\|_{T_k} \ge c\sqrt{d\log(k)}] \le 1/10k^2$ for some constant $c$. The only change needed in the proof is the size of the epsilon net which changes from $(2\log(3/2)/k)^{kd}$ for \cite{tomioka2014spectral} to $(2\log(3/2)/k)^d$, where the reason we require a net over a smaller space is due to us bounding the inner product with a rank one tensor rather than rank $k$. Union bounding on all values of $k$ we get $\sum_k 1/10k^2 \le 1/2$ which shows $g = [g_1, \operatorname{flat}(g_2), \operatorname{flat}(g_3), \ldots]$ belongs to $\mathcal K$ with probability at least $1/2$. 
%
We now define a mapping $\psi(x)$ of $x\in \R^d$ to a high dimensional space. The function $\operatorname{flat}(X)$ simply strings the entry values of the tensor $X$ into a vector.
$$\psi(x) = [x, \frac{\operatorname{flat}(x^{\otimes 2})}{\sqrt{2\log^2(2)}}, \frac{\operatorname{flat}(x^{\otimes 3})}{\sqrt{3\log^2(3)}}, \ldots,\frac{\operatorname{flat}(x^{\otimes k})}{\sqrt{k\log^2(k)}},\ldots]$$

Finally, note that for $\|x\| \le 1$ we have $\|\psi(x)\|_2 = (\sum_k  1/k\log^2(k))^{1/2} = O(1)$.


We are now ready to apply Banaszczyk's theorem. 
There exist signs $\sigma_i$ such that $\psi  = \sum_i \sigma_i \psi(x_i) \in C \mathcal K$.
Since $\psi_k = \sum_i \sigma_i x_i^{\otimes k}/\sqrt{k \log^2{k}}$ we get that $\max_k \|\sum_i \sigma_i  x_i^{\otimes k}\|_{T_k} \le \sqrt{d k \log^{3}(k)}$
\end{proof}

\begin{lemma} \label{lem:komlos anl}
Let $f$ be a function of $\langle x,q\rangle$ such that $f(\langle x,q\rangle) = \sum_k \alpha_k \langle x,q\rangle^k$ is its Taylor expansion. The Komlos Complexity of the Family of functions $f_q(x) = f(\ip{q,x})$, indexed by $\|q\| \leq 1$, is bounded by
\[
K_m = \min_\sigma \sum_i \sigma_i f(x_i,q) =O\left( \sqrt{d} \sum_k  |\alpha_k|\sqrt{k\log^3(k)}\right)
\]
For general $\|q\| \leq R$ we get
\[
K_m = \min_\sigma \sum_i \sigma_i f(x_i,q) =O\left( \sqrt{d} \sum_k  |\alpha_k| R^k \sqrt{ k\log^3(k)}\right)
\]
\end{lemma}
\begin{proof}
The proof follows from combining the above.
$$
\sum_i \sigma_i f(x_i,q) = \sum_k \alpha_k \sum_i \sigma_i \langle x_i,q\rangle^k =  \sum_k \alpha_k  \langle  \sum_i \sigma_i x_i^{\otimes k},q^{\otimes k}\rangle \le \sum_k |\alpha_k| \cdot \| \sum_i \sigma_i x_i^{\otimes k}\|_{T_k} \cdot \|q\|^k
$$
By Lemma \ref{uc} we can find signs $\sigma$ such that 
$\| \sum_i \sigma_i x_i^{\otimes k}\|_{T_k} \le c\sqrt{d k \log^3(k)}$. Substituting into the above, the lemma follows.
\end{proof}

\begin{theorem}\label{analitic1}
Let $f:\R\rightarrow\R$ be analytic. There exist a radius $R$ such that the function family of functions $f_q(x) = f(\ip{q,x})$, indexed by $\|q\| \leq R$, has Komlos complexity of $O(\sqrt{d}/m)$. 
\end{theorem}
\begin{proof}
Recall that for analytic functions $f$ we have $\left| \frac{d^k f}{dz^k}(z) \right|  \leq C^{k+1} k! $
for some constant $C$. Considering the taylor expansion of $f$ near zero, for $R < 1/C$ the sum
$ \sum_k  |\alpha_k| R^k \sqrt{ k\log^3(k)} \leq C \sum_k  (CR)^k \sqrt{ k\log^3(k)}$
corresponding to Lemma~\ref{lem:komlos anl} converges to a constant. The result follows.
\end{proof}

\begin{corollary}
The Komlos complexity of the Logistic function $f(\ip{q,x}) = \log(1+\exp(\ip{q,x}))$ in dimension $d$, for $\|q\| \leq 1$ is $O(\sqrt{d}/m)$.
\end{corollary}

\begin{corollary}
The Komlos complexity of the covariance function $f(\ip{q,x}) = \ip{q,x}^2$ in dimension $d$, for $\|q\| \leq 1$ is $O(\sqrt{d}/m)$. This gives coresets for matrix column subset selection such that $\|XX^T - \tilde X \tilde X^T\| \le \eps n$ where $\tilde X$ contains $O(\sqrt{d}/\eps)$ rescales columns of the matrix $X$.
\end{corollary}



\begin{theorem} \label{thm:analytic2}
Let $f:\R\rightarrow\R$ be analytic. There exist a radius $R$ such that the function family of functions $f_q(x) = f(\|x-q\|^2)$, indexed by $\|q\| \leq \sqrt{R}$, has Komlos complexity of $O(\sqrt{d}/m)$. 
\end{theorem}
\begin{proof}
By transforming $x$ to $\tilde{x} = (1, \sqrt{2}x, \|x\|^2)$ and $q$ to $\tilde{q} = (\|q\|^2, -\sqrt{2}q, 1)$ we get $\ip{\tilde{x},\tilde{q}} = \|q-x\|^2$. Moreover, $\|q\| \le \sqrt{R}$ gives $\|\tilde q\| \le R+1$. The result follows from applying Theorem~\ref{analitic1} to $f(\ip{ \tilde q, \tilde x}) = f(\|q-x\|^2)$.
\end{proof}

\begin{corollary}
The Komlos complexity of the Gaussian kernel $K(q,x) = \exp(-\gamma \|x-q\|^2)$ in dimension $d$ is $O((1+\gamma)\exp(\gamma)\sqrt{d}/m)$.
This improves upon the recent result of \cite{DBLP:journals/corr/abs-1802-01751} by proving the existence of $\eps$ approximation corsets of size $\sqrt{d}/\eps$ for Gaussian kernel density, in the case where $\gamma$ is constant. For the constant $\gamma$ setting this resolves the open problem raised by \cite{DBLP:journals/corr/abs-1802-01751} and matches their lower bound.   
\end{corollary} 
\begin{proof}
W.l.o.g. the maximum distance $\|q-x\|$ is 1. The taylor series of $K$ becomes
$$ \sum_{k=0}^\infty \frac{(-\gamma)^k}{k!} $$
Plugging into the equation in the proof of Theorem~\ref{thm:analytic2} we get that the sum determining the constant is upper bounded by
$$ \sum_{k=0}^\infty \frac{\gamma^{k}\sqrt{ k\log^3(k)}}{k!} = O\left((1+\gamma) \exp(\gamma)\right)$$
\end{proof}
%\end{proof}


\section{A Simple Algorithm for Kernel Density Estimation}

From section \ref{} we know that the Komlos complexity of the Gaussian kernel is $K_m = O(\sqrt{d}/m)$. Here, we show that for any positive kernel $K_m = O(1/\sqrt{m})$. This bound is better when $d > m$. More importantly though, there is a very simple, intuitive, and deterministic algorithm for computing the signs $\sigma$. 
Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $f: \R^d \rightarrow \R$ of a point $q$ is defined as 
$$ f(q) = \sum_{i=1}^{n} K(x_i,q) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include
$$ K(x,q) = \exp(- \|x-y\|_2^2/\lambda^2)\;\;\; K(x,q) = \exp(- \|x-q\|/\lambda) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-q\|/_2^2/\lambda^2)^{-1}$$
where $\lambda$ is a scaling parameter. For simplicity, we assume that $K(x,x) \leq 1$ for all data points. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.

Using our framework, we need to bound $K_m = \frac{1}{m}\sum_{i=1}^m \sigma_i K(x_i,q)$. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,q) = \ip{\phi(x), \phi(q)} $$
Using this function $\phi$ our objective function becomes
\[
\sum_{i=1}^m \sigma_i K(x_i,q) = \sum_{i=1}^m \sigma_i \ip{\phi(x_i), \phi(q)} =  \ip{ \sum_{i=1}^m \sigma_i \phi(x_i), \phi(q)} \leq  \|\phi(q)\| \cdot \left\|  \sum_{i=1}^m \sigma_i \phi(x_i) \right\| %\leq \left\|  \sum_{i=1}^m \sigma_i \phi(x_i) \right\| 
\]
This upper bound allows us to find values for $\sigma_i$ which bound $K_n$ using a simple algorithm.
Set $\sigma_1 = 1$. Then for $i=2,\ldots,m$ set $\sigma_i = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j  K(x_j, x_i))$.
We will show by induction that $\| \sum_{j=1}^i \sigma_j \phi(x_j) \|^2 \le \sum_{j=1}^i \|\phi(x_j)\|^2$ for all $i$.
This is trivially true for $i=1$. 
Assume by induction that $\| \sum_{j=1}^{i-1} \sigma_j \phi(x_j)\|^2 \le \sum_{j=1}^{i-1} \|\phi(x_j)\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}\sigma_j \phi(x_j)\|^2 &=& \|\sum_{j=1}^{i-1}\sigma_j \phi(x_j)\|^2 + \|\phi(x_i)\|^2 + 2\langle \sum_{j=1}^{i-1}\sigma_j \phi(x_j), \sigma_i \phi(x_i)\rangle \\
&\le& \sum_{j=1}^{i-1} \|\phi(x_j)\|^2 + \|\phi(x_i)\|^2 + 2\sigma_i \sum_{j=1}^{i-1}\sigma_j K(x_j, x_i) \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|\phi(x_j)\|^2 - 2|\sum_{j=1}^{i-1}\sigma_j K(x_j, x_i)| \le \sum_{j=1}^{i} \|\phi(x_j)\|^2
\end{eqnarray*}
This completes the proof that $\| \sum_{j=1}^{m}\sigma_j \phi(x_j)\| \le \sqrt{\sum_{j=1}^{i} \|\phi(x_j)\|^2} \le \sqrt{m}$ for our choice of values for $\sigma$ and shows that the Komlos complexity of any positive kernel is $O(1/\sqrt{m})$. 

Using the framework above this provides a deterministic coreset construction for kernel density estimation of size $O(1/\eps^2)$ such that $\forall \;q\;\; |\tilde f(q) - f(q)| \le \eps n$. This matches, and greatly simplifies, the results achieved by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751}.









%\section{Linear Regression}

%\section{Optimal Design of Experiments}
%We discuss the problem of linear regression with mean squared error. In the classic setting we are given $n$ points $x_1,\ldots,x_n \in \R^d$ along with $n$ labels $y_1,\ldots,y_n \in \R$ and wish to find $w \in \R^d$ minimizing
%$$ \sum_i (w^Tx_i - y_i)^2 $$
%In the field of optimal design of experiments, the labels are not given with the data. Instead, the learner must choose in advance a subset of datapoints for which they query the labels. The subset is chosen to provide the least error in a generalization bound, assuming $(x_1,y_1),\ldots,(x_n,y_n)$ are i.i.d from an unknown distribution.
%
%The way this is done in optimal design of experiments is as follows. Let $X$ be the set of $n$ points and consider a subset $B \subseteq X$. The empirical risk minimizer (ERM) for $B$ is 
%$$ w_B = \left( \sum_{x \in B} xx^T \right)^{\dagger} y_B $$
%%It is known that $w_B$ is an unbiased estimator of $w^*$, the optimal regressor for the distribution, where the noise comes from the realizations of the $y_i$'s. It follows that to minimize 
%%$$ E_{x \sim {\cal D}} \ip{w-w^*,x}^2 $$
%%which is the variance of $w$ measured by the distribution of data points, it suffice to minimize the second moment
%%$$ E_{x \sim {\cal D}} \ip{w,x}^2  \approx w^T \left( \frac{1}{n} \sum_{i=1}^n x_ix_i^T \right) w $$
%
%
%One can show that for $w_B$,
%$$ \sum_i (w_B^Tx_i - y_i)^2 = C \cdot \text{Tr} \left(\left( \sum_{x \in B} xx^T \right)^{\dagger} \right) $$
%\zk{This only works when $\sum_{x \in B} xx^T$ is full rank, otherwise we need to introduce some regularization and then things get messy.}
%
%We can use the results above for optimal design in the case where $w$ is restricted to have a unit Euclidean norm. Something that is commonly practiced. Since we will solve the problem for the subset we choose in an optimal way for gaussian noise, we end up with an unbiased estimator for $w^*$. The error is the variance of that unbiased estimator and minimizing the variance for an unbiased estimator is the same as minimizing its second moment we can aim to minimize
%$$ F(w) = w^T \left( \frac{1}{n} \sum_{i=1}^n x_ix_i^T \right) w $$
%This leads to the exact same objective as in column subset selection. Meaning that an $\eps$ core-set for the $x$'s gives a guaranteed error for a subset in the optimal design setting. Specifically we get that we can obtain a core-set of size
%$$ \min(1/\eps^2, \sqrt{d}/\eps)$$
%For the case of $\eps < 1/\sqrt{d}$, which is an interesting case in linear regression, a random sample of $\sqrt{d}/\eps$ points will not give a generalization bound of $\eps$ (there are known lower bounds, http://jmlr.org/papers/volume16/shamir15a/shamir15a.pdf contradicting this), meaning that we have a provable non-trivial guarantee for active learning. I'm not sure what statements are known already. 
%
%In the optimal design setting, they require keeping the covariance matrix. We could come up with a hueristic or even sketching algorithm that avoids maintaining $d^2$ points.


\section{Logistic Regression}
The following paper claims no streaming algorithms or coresets are available \cite{DBLP:journals/corr/abs-1805-08571}.
This paper claims to have done that \cite{DBLP:conf/nips/HugginsCB16}.

In \cite{DBLP:journals/corr/abs-1802-01751} (see paragraph before Lemma 2.2) they manage to obtain a coreset (with equal weights) of size $\sqrt{d \log(1/\eps)}/\eps$ for any kernel with certain properties. They do so by considering a matrix whose rows correspond to all the vectors in an $\eps$-net over the space, and columns correspond to the $n$ data points. Calling that matrix $A$, $A_{q,x} = K(q,x)$. They bound the discrepancy of the matrix defined as $\min_s \|As\|_\infty$ where $s$ is a sign vector by using a bound 
$$ \min_s \|As\|_\infty \leq \sqrt{\log(N)} \|A\|_{\gamma_2} $$
Here, $N$ is the size of the $\eps$ net and the $\gamma_2$ norm is 
$$ \|A\|_{\gamma_2} = \min_{B,C: A=BC^T} \text{max norm of a row of B or C}$$
They use the fact that for psd kernel $K$, $\|A\|_{\gamma_2} \leq 1$ (in fact it is equal to 1). Now, back to our case. For any fixed power $k$ we know that $K(x,q)=\ip{x,q}^k$ is a psd kernel, hence its corresponding matrix $A_k$ has $\gamma_2$ norm bounded by $1$. Now, the logistic function can be written in a taylor expansion whose coefficients, in absolute value, sum to a constant $C$. It follows that the $A$ matrix for the logistic function has a $\gamma_2$ norm bounded by $C$. The same arguments as in  \cite{DBLP:journals/corr/abs-1802-01751} can now prove that its discrepancy is bounded by $\sqrt{\log(N)} \approx \sqrt{d\log(1/\eps)}$

A useful fact about the $\gamma_2$ norm is that it is indeed a norm. This is proven in \cite{tomczak1989banach}. In \cite{matouvsek2014factorization} this is listed in Section 2, Proposition 2.2 along with other known properties of the $\gamma_2$ norm. 





\section{Coreset for linear functions (based on $\gamma_2$ norm)}

Let $x_1,\ldots,x_n \in R^d$ be and $y_1,\ldots,y_n$ be labels associated to the data points. Let $K$ be a psd kernel. Consider a problem of learning a linear function aiming to minimize the loss of 
$$\min_q \sum_{i=1}^n L(K(x_i, q), y_i)$$

Our objective is to find a core-set of the $n$ data points. That is, we aim to find a set $S \subseteq [n]$ such that for every $q$ it holds that 
$$ \left| \sum_{i=1}^n L(K(x_i, q), y_i) - \frac{n}{|S|} \sum_{i \in S} L(K(x_i, q), y_i) \right| \leq \eps n $$

For a function $f(q,x)$, let $A_f$ be a matrix whose rows are an $\eps$-net over the Euclidean unit ball of $R^d$, and columns correspond to the different $x_i$'s. The value of $A_f$ at an entry corresponding to $(q,x)$ if $f(q,x)$.

\begin{definition}
For a matrix $A$,
$$ \|A\|_{\gamma_2} = \min_{B,C: A=BC^T} \|B\|_{2 \to \infty} \|C\|_{2 \to \infty}$$
is the $\gamma_2$ norm of $A$. 
\end{definition}
 \cite{tomczak1989banach} prove that this is indeed a norm, in particular that 
 $\|A_1+A_2\|_{\gamma_2} \leq \|A_1\|_{\gamma_2} + \|A_2\|_{\gamma_2}$. 


\begin{lemma} \label{lem:powers}
For $k \geq 0$ let $f_k(q,x) = \ip{q,x}^k$. If holds that $\|A_{f_k}\|_{\gamma_2} \leq 1$
\end{lemma}
\begin{proof}
For $k=0$ the statement is obvious. For $k>0$, notice that $f_k$ is a kernel, and in particular there exists a mapping $\phi_k: \R^d \to \R^{d^k}$ such that
$$ \ip{q,x}^k = \ip{\phi_k(q), \phi_k(x)} $$
Furthermore, 
$$ \|\phi_k(x)\| = \sqrt{\ip{\phi_k(x), \phi_k(x)}} = \sqrt{\|x\|^{2k}} = \|x\|^k \leq 1 $$
Since all $x,q$ have at most unit norm we get a bound over the $\gamma_2$ norm by viewing the $B,C$ matrices obtained from the $\phi_k$ embeddings of the different $x,q$'s
\end{proof}

\begin{lemma}
Let $L:\R \to \R$ be a function that can be written as 
$$ L(z) = \sum_{k=0}^\infty \alpha_k z^k$$
with $\sum_{k=0}^{\infty} |\alpha_k| \leq C$. For $f_L(q,x) = L(\ip{q,x})$ it holds that 
$$ \|A_{f_L}\|_{\gamma_2} \leq C $$
\end{lemma}
\begin{proof}
This is a direct corollary from $\gamma_2$ being a norm, and Lemma~\ref{lem:powers}
\end{proof}

This lemma in particular holds for the logistic function as its Taylor series has elements that converge. In particular, evaluating the function at $i$ and summing the real and imaginary parts of the result gives the sum of the absolute values of the coefficients, which is (roughly) $0.461$
\begin{corollary}
For $L(z) = \log(1+e^z)$ we have $\|A_{f_L}\|_{\gamma_2} \leq 0.5$
\end{corollary}

\begin{definition}
For a matrix $A$ of dimensions $m \times n$, $\disc(A)$ is defined as 
$$ \min_{s \in \{-1,1\}^n} \|As\|_\infty $$
\end{definition}
In \cite{matouvsek2014factorization} they prove that 
$$\|A\|_{\gamma_2} / \log(m) \leq \disc(A) \leq \|A\|_{\gamma_2}  \sqrt{\log(m)}$$
This gives a way to control the discrepancy of a matrix by representing it as a sum of matrices with known discrepency or $\gamma_2$ norm. In particular, for the logistic function we get that 
$$\disc(A_{f_L}) \leq \sqrt{\log(|Q|)} $$
where $Q$ is an $\eps$-net over the Euclidean unit sphere. This gives the bound of 
$$\disc(A_{f_L}) \leq O(\sqrt{d\log(1/\eps)}) $$

Let's discuss what this means. For a logistic regression problem, by observing the weights and labels we can change the signs of the inputs so that the logistic loss is always $\log(1+\exp(\ip{q,x}))$, and we can now convert the discrepancy result into a coreset of size $\sqrt{d\log(1/\eps)}/\eps$, as opposed to a random sample of $1/\eps^2$. 

Let $K$ be a psd kernel, meaning for $x,q \in \R^d$ we have for some mapping $\phi$ into a Hilbert space $K(q,x) = \ip{\phi_K(q), \phi_K(x)}$. Notice that for $\phi_k$ defined above
$$ \ip{\phi_k(\phi_K(q)), \phi_k(\phi_K(x))} = \ip{\phi_K(q), \phi_K(x)}^k = K(q,x)^k $$
meaning that $K^k(q,x)$ is a psd kernel for all $k \geq 0$. By using the exact same arguments before we can define $A_{K,k} \in \R^{|Q|,n}$ with $A_{K,k}(q,i) = K(q, x_i)^k$ and conclude that $\|A_{K,k}\|_{\gamma_2} \leq 1$. Then, for the matrix $A_{K,f_L}$ with
$$ L(z) = \sum_{k=0}^\infty \alpha_k z^k, A_{K,f_L}(q,i) = L(K(q,x_i))$$
we have 
$$\|A_{K,f_L}\|_{\gamma_2} \leq \sum_{k=0}^{\infty} |\alpha_k| $$

\begin{corollary}
For $L(z)=\log(1+\exp(z))$ and $L(z) = z^2$, and any psd kernel $K:\R^d \times \R^d \to \R$ we have
$$\|A_{K,f_L}\|_{\gamma_2} \leq 1 $$ 
and
$$\disc(A_{K,f_L}) \leq O(\sqrt{d\log(1/\eps)}) $$ 
\end{corollary}


%\bibliographystyle{plain}
\bibliography{density}

\section{Appendix 1}
For completeness we recap a result from \cite{barany2008}.
\begin{theorem}[Simplified from B\'ar\'any \cite{barany2008} Theorem 4.1] \label{thm:random tensor}
For any set of vector $x_1,...,x_n$ in $\R^d$ such that $\|x_i\| \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i \sigma_i x_i || \le \sqrt{d}$.
\end{theorem}
\begin{proof}
Consider the feasible region for $\sum_i \alpha_i x_i = 0$ and all $\alpha_i \in [-1,1]$.
This feasible region is not empty because it contains the origin. 
Consider an extreme point $\alpha^*_i$ and set $\sigma_i = 1$ w.p.\ $(1+\alpha^*_i)/2$ and $\sigma_i = -1$ w.p.\ $(1-\alpha^*_i)/2$.
$$
\E[\|\sum \sigma_i x_i\|^2] =  \E[\|\sum (\sigma_i - \alpha^*_i) x_i\|^2] = \sum \E[(\sigma_i - \alpha^*_i)^2] \|x_i\|^2= \sum (1-(\alpha_i^*)^2) \le d
$$
The first equality is due to $\sum_i \alpha_i x_i = 0$. The second used $\E[\sigma_i] = \alpha^*_i $.
The last inequality holds because the values $\alpha^*_i$ are non-integer in at most $d$ places. 
Assuming otherwise would entail more than $d$ linearly independent vectors in $\R^d$ due to $\alpha^*$ being an extreme point.
\end{proof}

Extension to the apx low rank case: The vectors are $[x_1,y_1],\ldots,[x_n,y_n]$. The dimension of $x$'s is $d$ and of $y$'s is infinite. Both $\|x_i\|, \|y\| \leq 1$. However, the overall norm of the $y$'s is bounded $\sum \|y_i\|^2 \leq \rho n$. Naively we can ignore the $y$'s and get a bound on the norm of $\sqrt{d + \sqrt{\rho} n}$ or something like that. If $y$ is obtained as the low components of PCA we can squeeze some more.
Consider another approach: Asking for $\sum x_i \alpha_i =0 $ gives $d$ linear restrictions. Let $\alpha^{(1)}$ be a feasible point. Let $\alpha^{(2)}$ be another feasible point that is restricted to being orthogonal to $\alpha^{(1)}$, and so on up to $\alpha^{(d)}$. For all these $\alpha$'s we have a bound of $2d$ to their rounding w.r.t to $x$. Now, for the $y$'s we can choose any one of these $\alpha$'s

However, we can search for $\alpha \in [-1,1]^n$ minimizing


%\section{Attempt at Cleaner Definitions}
%Let $\mathcal F$ be a family functions from a domain $\X$ to $\R$.
%We define the discrepancy of $\F$. Let $f \subset{\F}$ to a subset of $\F$.
%$$\mathcal D_m(\F) := \sup_{x\in\X} \min_{s\in \{0,1\}^m} \sup_{f\subset \F, |f|=m}  \sum_{i}\sigma_i f_i (x)$$  \el{need to double check the order here...}
%
%We claim that a very common task is to approximate the function $F = \sum_i f_i$ using a concise representation.  
%Specifically, using coresets we can approximate $F$ with $\tilde F = \sum_i w_i f_i$ where $w$ is non-zero in at most $k$ places.
%
%In this paper we show that if a family of functions exhibits low discrepancy then finding a good coreset is always possible.
%The size of the coresets we find with the proposed algorithm is always smaller than that achievable by uniform sampling.
%Moreover, we show that this framework includes a wide variety of problems including quantile approximation, approximate counting, machine learning classification and regressing, kernel density estimation and many more. 
%Finally, we show that coresets can always be created in a streaming fashion using fully mergable sketches with only a $\operatorname{poly}\log\log$ factor loss in space.
%
%
%In what follows, we subscript the function $f$ with its parametrization.
%\begin{itemize}
%\item In approximate counting we have $f_a(x) = 1$ if $a=x$ and zero else. Here both $a$ and $x$ belong to some finite domain.
%\item In quantile approximation we have $f_a(x) = 1$ if $a<x$ and zero else. Here, $a$ and $x$ belong to a set which exhibits total ordering.
%\item In matrix column subset selection we have $f_z(a,q) = \langle z,x \rangle ^2$ where $a,x \R^d$. For simplicity, we assume $\|a\| = \|x\| = 1$.
%\item In density estimation $f_a(x) = \exp(- \|a-x\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
%\item In logistic regression $f_a(x) = \operatorname{loss}(a,x) = \log(1 + \exp(-a^T x))$. 
%Here $x$ is the parameters of the linear model and $a$ is a training example vector multiplied by its label ($1$ or $-1$).
%\item Linear Classification $f_a(x) = 1$ if $a^Tx > 0$ and $0$ else. Here, like in Logistic Regression, $a$ is the example point multiplied by its label and $x$ is the weights of the linear classifier.
%\item Fractional Satisfiability $f(x,q)$ if $q$ satisfies $f$ and $0$ otherwise. Here $x$ in a clause in conjunctive normal form and $q$ is a boolean value assignment to the variables. For example $f(x,q) = 1$ iff $(q_1 \vee q_5  \vee \neg q_8)$ is true.
%\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q = \sum_{i,j\in E} (q_i - q_i)^2$ where $x$ is an edge and $L_x$ is the graph laplacian corresponding to the graph with a single edge $x$ and $q$ is a test vector.
%\item Probably also good for matrix approximation when we get entry updates. 
%\end{itemize}
%

\appendix

\section{Proofs for Streaming Coresets}

\begin{lemma} \label{lem:compactor}
Let $k$ be an integer and assume we have black box access to a solver that for any $k$ inputs $x_1,\ldots,x_k$ obtains $k$ signs $\sigma_1,\ldots,\sigma_k$ such that
$$\max_q \left| \sum_{i=1}^{k} \sigma_i f(x_i, q)\right| \leq K_k$$
Note that such signs are guarantied to exist by the definition of the Komols Complexity. 
In fact, the quantity above is potentially much smaller than the Komlos Complexity because it is computed for a specific set of points $x_i$ as opposed to the worst such possible set.


Then there exist a streaming algorithm requiring a memory buffer of $k$ input items that given a stream of length $n$ outputs a stream $z_1,\ldots,z_m$ with the following properties
\begin{itemize}
\item $\{z_i\}_i$ is a subset of $\{x_i\}$
\item $\E[m] = n/2$
\item $\Pr[m \geq (1+1/\log^2(n))n/2] \leq \exp \left( -O\left(\frac{n}{k\log(n)^4}\right)\right)$
\item For any fixed query $q$, the error associated with $q$, defined as
$$\text{Err}(q) = 2\sum_{i=1}^m f(z_i,q) - \sum_{i=1}^n f(x_i,q)  $$
Can be decomposed as a sum
$$\text{Err}(q) = \sum_{j=1}^{n/k} \text{Err}_j(q)$$
where the different $\text{Err}_j(q)$ are independent, have mean 0 and $|\text{Err}_j(q)| \leq K_k$
\end{itemize}
The last property ensures that deterministically $|\text{Err}(q)| \leq (K_k/k) n$, and that w.h.p.\ (via Chernoff's inequality) for a fixed $q$ 
$$|\text{Err}(q)| \lesssim \rho k\sqrt{n/k} = \rho \sqrt{kn}$$
\end{lemma}
\begin{proof}
The algorithm operates as follows. It keeps a buffer of $k$ items. Once the buffer fills with $x_1,\ldots,x_k$ it obtains the signs guaranteeing
$$\max_q \left| \sum_i \sigma_i f(x_i, q)\right| \leq K_k$$
Then, the algorithm output to the stream the with probability $1/2$ each data points $\{x_i \; | \; \sigma_i = 1\}$ or $\{x_i \; | \; \sigma_i = -1\}$ with twice the weight.
$$\sum_{i ,\; \sigma_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} \sigma_i f(x_i, q)$$
$$\sum_{i ,\; \sigma_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} \sigma_i f(x_i, q)$$

%Now, consider the set $P=\{i:\sigma_i=1\}$ and $N=\{i:\sigma_i=-1\}$.
%$$ \sum_i \sigma_i f(x_i, q) = \sum_i \sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in P} f(x_i,q) - \sum_i f(x_i, q)   $$
%$$ -\sum_i \sigma_i f(x_i, q) = \sum_i -\sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in N} f(x_i,q) - \sum_i f(x_i, q)   $$
%Thus, we draw a random coin and either output to the stream the data points of $P$ or $N$. We call this operation a single compaction. 
%\el{this is clunky way to say this...}
The expected output length is $k/2$ for every $k$ inputs, insuring $\E[m]=n/2$. Moreover, the output length is a sum of $n/k$ independent random variables with mean $k/2$ and a maximum value of $k$. Chernoff-Hoeffding's bound can guarantee the stated concentration around $\E[m]$.
As for the error term w.r.t.\ a fixed query $q$, it is clearly the sum of errors accumulated in each compaction. The error incurred in a compaction is either 
$\sum_i \sigma_i f(x_i, q)$ or $-\sum_i \sigma_i f(x_i, q)$ with equal probability. It follows that it is a Radamacher r.v. scaled by a magnitude of at most $K_k$ as required.
\end{proof}


\begin{proof} [Proof of Theorem~\ref{thm:streaming}]
Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.

The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ we analyze the error
$$\text{Err}(q) = \sum_{h=0}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \ .$$
Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. 

To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
$$ \text{Err}(q) \leq \rho(k) \log_2(n/k) n $$

If however, we are interested in a smaller sketch with high probability of success of $1-\delta$, we decompose the error to two components
$$\text{Err}(q) = \sum_{h=0}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho k_h Y_{ij} + \sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}$$
 Here, $H' = H - \ceil{\log(c \log(1/ \delta))}$ where $c$ is some universal constant we set later. Notice that the right summand contains roughly $\log(1/\delta)$ random variables. Since they are all Bernoulli we get that any combination of them is possible with probability $\geq \delta$ hence we must use a deterministic bound for these upper layers. We thus set $k_h=k$ for $h > H'$ and use the bound
$$ \text{Err}^{\text{top}}(q) = \left|\sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}\right| \leq O\left( \rho(k) \left(H-H'\right) n \right) =  O\left( \rho(k) \log \log(1/ \delta) n \right) $$
Now, for the bottom layers we use Chernoffs bound
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h)^2 k_h^2}   \right) =$$
$$\exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} } \right) $$

\zk{$n_h$ being random is a problem. The weight of the stream might increase with time. The analysis now works for a completely balanced case }
Consider the sum in the denominator. 
$$ \sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} = O(\sum_{h=0}^{H'} 2^{2h} \rho(k_h)^2 k_h n_h ) $$
Since $n_h \leq n_{h-1}/2$ and we have $k_h \leq k$ we have 
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2  ) $$
Now, if $\rho(k_h) \geq 1/\sqrt{k_h}$ we set $k_h = 2$ for all $h \leq H'$ and obtain
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h  ) = O(n^2/ (k \log(1/\delta))) $$
and we get
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) \right)   \right) $$
Hence, by setting $k \geq c/\eps^2$ for sufficiently large constant $c$ the probability is upper bounded by $\delta$.

Now, for $\rho(k) < 1/\sqrt{k}$ we have $\eta(k) = \rho(k)^2 k = o(1)$. We can exploit that to allow for $k = o(1/\eps^2)$. For $\eta(k)=O(k^{-p})$ we\footnote{We do not discuss other scenarios as we are not aware of any setting where they occur. Also, extending the analysis to them is a purely technical exercise} use $k_h = \max(2, \ceil{(2/3)^{H'-h}k})$, leading to a bound of
$$\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2 \leq \sum_{h=0}^{H'} 2^h \eta(k) \cdot (3/2)^{H'-h} = (3/2)^{H'} \eta(k) \sum_{h=0}^{H'} (4/3)^h \leq  $$
$$(3/2)^{H'} \eta(k) \cdot (4/3)^{H'} \cdot 3 = 3 \cdot 2^{H'} \eta(k) = O(\eta(k)n/(k\log(1/\delta))$$
Plugging this in the probability bound gives
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) / \eta(k) \right)   \right) = $$
$$ \exp \left( -O\left( \eps^2 \log(1/\delta) / \rho(k)^2 \right)   \right) $$
Hence, setting $k$ large enough so that $\rho(k) \leq c\eps$ for some universal constant $c$ leads to the required bound.

In particular for $\rho(k) = O(1/k)$ we see that $k=1/\eps$ suffices, for general $p>1/2$ with $\rho(k) = k^{-p}$ we require $k=\eps^{-1/p}$.


Now, consider the overall error and memory of the entire process. For the bottom layers, for any $\rho(k)$ we require $O(k+H')=O(k+\log(n))$ memory. A closer inspection actually shows that once $k_h=2$ we are simply performing uniform sampling, and all of those layers can be done with constant memory. Hence, the memory footprint of the bottom layers is $O(k)$. The top layers require $O(\log\log(1/\delta)k)$ memory leading to a total memory usage of $O(\log\log(1/\delta)k)$.

The error for the bottom layers is $O(\min\{\rho(k), 1/\sqrt{k}\} n)$. The error of the top layers is $O(\log\log(1/\delta) \rho(k) n)$, meaning that the overall error is $O(\log\log(1/\delta) \rho(k) n)$.

Hence, if we aim for an error of $\eps n$ w.p.\ $1-\delta$ we set $k$ large enough such that $\rho(k) \leq c \eps / \log\log(1/\delta)$. Denoting by $\rho^{\dagger}$ the inverse function to $\rho$ this leads to a total memory requirement of 
$$ \rho^{\dagger}(c \eps / \log\log(1/\delta)) \log\log(1/\delta) $$

\end{proof}


%
%
%
%
%
%
%
%
%
%
%


\subsubsection{take 3}


\begin{proof} [Proof of Theorem~\ref{thm:streaming}]
Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.

The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ and hierarchy level $h$ we analyze the error
$$\text{Err}_h(q) = 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \ .$$
Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. $K_h$ is used to denote the discrepancy of each compaction using a buffer of $k_h$ points. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. The overall error for the query $q$ is the sum over all levels of its errors
$$ \text{Err}(q) = \sum_{h=0}^H \text{Err}_h(q)$$

To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
$$ \text{Err}(q) \leq (K_k/k) \log_2(n/k) n $$

We are however interested in a high probability bound. Specifically we would like to guarantee for $\delta > 0$ that for any query $q$ the error is bounded by $\eps n$ w.p.\ at least $1-\delta$. To this end we consider a different analysis for the top and bottom layers. For the top layers, the stream can be arbitrarily short so we will use the deterministic analysis as above. For the bottom layers, we can guarantee a minimal length to the stream and obtain tighter bounds.

The bottom layers will be those of height $h=0$ up to $h=H'$ where $H'$ is set adaptively in a way that $H'$ is the maximum integer such that for all $h \leq H'$, 
\begin{equation} \label{eq:Htag_nh}
 \forall h \leq H' , \ \ n_h \geq c_1 k \log^2(n) \log(1/(c_2 \delta \log(n)))
\end{equation}
for some sufficiently large constant $c_1$ and small constant $c_2$ to be determined later.

According to Lemma~\ref{lem:compactor} we have for $ h\leq H'$ that for some appropriate constant $c_1$ in  \eqref{eq:Htag_nh},
$$ \Pr[n_{h+1} \geq (1+1/\log(n))n_{h}/2] \leq \exp\left( -\frac{n_{h}}{k_h \log^2(n)}  \right) \leq c_2\delta / \log(n) $$
We can now union bound over all 
$$h \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta\log(n))) \right)\right) + 2 \leq \log_2(n)$$
and conclude that w.p.\ at least $1-\delta/2$
\begin{equation} \label{eq:Htag}
H' \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) + 2 \leq \log_2(n)
\end{equation}
and
\begin{equation} \label{eq:nh small}
\forall h \leq H', \ \ n_{h} \leq 3 \cdot n/2^{h}
\end{equation}


Let's proceed to bound the error of layer $h$. Since this error is a sum of i.i.d.\ bounded variables of mean zero we can use Chernoff- Hoeffding's inequality and obtain that
$$ \Pr\left[   \left| 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( O\left( -\frac{ \eps^2 n^2}{ 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } K_h^2}   \right)\right) =$$
$$\exp \left( -O\left(\frac{\eps^2 n^2}{n_h 2^{2h} K_h^2/k_h  } \right) \right) 
$$
We now make use of the the bound on $n_h$ in Equation~\eqref{eq:nh small}. Notice that the random events determining the length $n_h$ do not affect the realization of the random variables used in layer $h$, so we can indeed use the bound on $n_h$ for controlling the error at level $h$. We get that since $n_h \leq 3n/2^h$
$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq 
\exp \left( -O\left(\frac{\eps^2 n}{ 2^h K_h^2/k_h  } \right) \right) 
$$
In particular, for $\eps_h = \sqrt{\frac{c 2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}}$ for some sufficiently large constant $c$ it holds that
$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq \delta/4(H'-h+1)^2$$
and a union bound ensures that with probability at least $1-\delta$, in addition to Equations~\eqref{eq:Htag} and~\eqref{eq:nh small} we have for all $h \leq H'$ that
$$\left| \text{Err}_h(q) \right| \leq \eps_h n$$
and 
$$\left| \sum_{h=0}^{H'} \text{Err}_h(q) \right| \leq \left(\sum_{h=0}^{H'} \eps_h\right)n $$

Let's proceed to bound the error on the top layers $H'+1$ to $H$. In these top layers we use a deterministic algorithm guaranteeing the stream is cut at least times 2 from layer to layer. We assign all top layers a budget of $k_h=k$ for the compactors. Since $2^{H+1} < n/k$ this gives a bound of 
$$
H-H' \leq \log_2(n/k2^{H'}) = O\left( \log\left( c_1 \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) = 
$$
$$
O\left( \log \log(n/\delta) \right)
$$
leading to a bound of 
$$ \left| \sum_{h=H'+1}^H \text{Err}_h(q) \right| \leq O\left( \log \log(n/\delta) (K_k/k) n \right) $$
and an overall bound of
\begin{equation}
|\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(K_k/k) + \sum_{h=0}^{H'} \eps_h \right)
\end{equation}

We are now left with the task of defining $k_h$ for the lower layers; we do so differently depending on the value of $K_k$ as a function of $k$. Recall that we aim to deal with two settings. In the first $K_k=\min\{d,k\}$ and in the second $K_k=\min\{d\sqrt{k}, k\}$. In both cases  $d$ is independent of $k$. We start by dealing with the first scenario.

We set $k_h = \max\{2, \ceil{(2/3)^{H'-h}k}\}$. Since $k_{H'} = k$ by using the definition of $H'$ in Equation~\eqref{eq:Htag} we see that
\begin{align*}
\eps_h = & O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k^2 \log((H'-h+1)^2/\delta)}{k_h K_k^2 n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)) \\
  	   &  O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k }{k_h K_k^2 2^{H'}}} \right)  \\
    	   &  O\left( \frac{K_k}{k} \sqrt{\frac{K_h^2 k }{k_h K_k^2 2^{H'-h}}} \right)
\end{align*}
We move to control the sum over $\eps_h$ by bounding the multiple of $K_k/k$. For $k_h \geq d$
$$
\frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} \leq \frac{d^2 k}{2^{H'-h} d^2 (2/3)^{H'-h}k} =
(3/4)^{H'-h}
$$
For $k_h \leq d$ we must have $h \leq H''$ with $(2/3)^{H'-H''} \leq d/k$, meaning that $2^{H'-H''} \geq k/d$. For such $h$ we get
\begin{align*}
 \frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} = &  O\left( \frac{ k_h k}{2^{H'-h} d^2 } \right) =  \\
    	   &  O\left( \frac{ k_h k}{2^{H''-h} kd } \right) = & k_h \leq d\\
	   &  O\left( (1/2)^{H''-h}   \right) 
\end{align*}

If follows that 
$$ \sum_{h=0}^{H'} \eps_h = O(K_k/k) = O(d/k) $$
translating to an overall bound for the error of
$$ |\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(d/k)  \right)$$
The overall memory consumption for the bottom layers is $O(k)$ since all layers of size 2 can be implemented jointly by sampling. The top layers require $O(\log\log(n/\delta)k)$ memory. If follows that by setting $k=c d \log\log(n/\delta) / \eps$ for sufficiently large $c$, the error is bounded by $\eps n$ w.p. $1-\delta$ and the overall memory requirement is 
$$ O\left( \log \log(n/\delta)^2 (d/\eps)  \right) $$


We are now ready for the case where $K_k = d\sqrt{k}$. We assume that $\sqrt{k} > d$ as otherwise we can result to a random sample. Here we set $H'' = H' - \ceil{\log_2(k/d^2)}$. For $h \geq H''$ we set $k_h=k$ and for $k \leq H''$ we set $k_h=2$. For $k \leq H''$ we have
\begin{align*}
\eps_h = & O\left( \sqrt{\frac{2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)), \ k_h=k=2 \\
  	   &  O\left( \sqrt{2^{h-H'}} \right)  = & 2^{H'-H''} \geq k/d^2 \\
    	   &  O\left(\frac{\sqrt{k}}{d} \sqrt{2^{h-H''}} \right)  
\end{align*}
and 
$$ \sum_{h=0}^{H''} \eps_h= O(\sqrt{k}/d) $$
\end{proof}
For $H'' \leq h \leq H'$ we use the deterministic algorithm and its error bound of
$$\sum_{h=H''}^{H'} |\text{Err}_h(q)| \leq O(\log(k/d^2) d/\sqrt{k})$$
Summing the errors of all layers we obtain a bound of
$$|\text{Err}(q)| \leq \frac{d}{\sqrt{k}} \cdot O(\log(k/d^2) + \log\log(n/\delta))$$
The memory requirement is $k \cdot O(\log(k/d^2) + \log\log(n/\delta))$ hence by setting 
$$k \approx \frac{d^2\left(\log^2(1/\eps)+ \log^2\log(n/\delta) \right)}{\eps^2}$$ 
we get a bound of $\eps n$ for the error and a memory usage of 
$$ O\left( \frac{d^2\left( \log^3(1/\eps) + \log^3\log(n/\delta) \right)}{\eps^2}\right) $$



\section{Table of Complexities for Common Classes}

\newcommand{\ccc}[1]{\parbox}
\begin{table}[htp]
\begin{center}
\begin{tabular}{|c|c|c|c|} \hline
Problem				& $f$ 											& $R_m$ 			& $K_m$ \\ \hline
Counting 				& $f(x, q) = 1$ if $x=q$ and zero else 					& $1/\sqrt{m}$		& $1/m$ \\ \hline
Quantiles				& $f(x, q) = 1$ if $x\le q$ and zero else  					& $1/\sqrt{m}$		& $1/m$ \\ \hline
Column Selection		& $f(x, q) = \langle x,q \rangle ^2$ where  $q,x\in \R^d$		& $1/\sqrt{m}$		& $\sqrt{d}/m$ \\ \hline
Gaussian Kernel Density 			& $f(x, q) = \exp(- \|x-y\|_2^2/\lambda^2)$ where  $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline
Laplacian Kernel Density 			& $f(x, q) = \exp(- \|x-y\|_2/\lambda)$ where  $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline
Positive Kernel Density 			& $f(x, q) = k(x,q)$ for any positive kernel $k$ and $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline

%Logistic Regression     	& $f(x, q) = \log(1 + \exp(q^T x))$ where  $q,x\in \R^d$ 						& $1/\sqrt{m}$ 		& $\sqrt{d}/m$ \\ \hline
%Hinge Regression     		& $f(x, q) = [1-q^T x]_+$ where  $q,x\in \R^d$							& $1/\sqrt{m}$ 		& $\sqrt{d}/m$ \\ \hline
Linear Regression		& $f(x,q) = (z^Tq - y)^2$ where $x = (z\in\R^d,y\in \R)$ and $q\in \R^d$					& ?				& ? \\ \hline
Linear Classification		& $f(x,q) = 1$ if $x^Tq > 0$ and $0$ else. $q,x\in \R^d$					& ?				& ? \\ \hline
Graph Laplacian 		& $f(x, q) = w_{ij}(q_i - q_i)^2$ for $x = (w_{ij},i,j)$, $q \in \R^n$ & ?				& ? \\ \hline
Matrix Approximation  	& $f(x, q) = a_{ij}u_iv_j$ for $x = (a_{ij},i,j)$  and $q = (u,v)$ 	& ?				& ? \\ \hline
\end{tabular}
\caption{Example problems in machine learning and sketching that fall into this framework.}
\end{center}
\label{default}
\end{table}%



\section{Row (or Column) Subset Selection}
Assume you are getting the rows $x_i$  of matrix $X$ in a stream and you want to compute $Z$ such that $\|Z^TZ - X^TX\| \le \eps \operatorname{Tr}(X^TX)$.
For simplicity, assume $x_i$ are all unit length so $\eps \operatorname{Tr}(X^TX) = \eps n$. Moreover, assume each vector in $Z$ must be one of the rows in $X$ up to some constant factor.
This is called the row subset selection problem. 
Since you are trying to minimize the quadratic form, you get that $f(x, q) = \langle x,q \rangle ^2$ and 
$$E(q) = \sum_i \sigma_i \langle x_i,q \rangle ^2 = q^T (\sum_i \sigma_i x_i x_i^T ) q \le \|\sum_i \sigma_i x_i x_i^T\| \le c\sqrt{d}$$

\noindent The last inequality is a direct application of the following Theorem.
\begin{theorem}\label{BansalInDaHouz}
For any set of matrices $X_1,...,X_n$ in $\R^{d_1 \times d_2}$ such that $\|X_i\|_{F} \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i \sigma_i X_i||_{2} \le O(\sqrt{d_1 + d_2})$.
\end{theorem}
\begin{proof}
First, recount Banaszczyk's Theorem~\cite{Banaszczyk}.
Let $\mathcal K$ be a convex body in $\R^d$ with Gaussian measure at least 1/2 ($\Pr[g \in \mathcal K] \ge 1/2$ where $g$ is i.i.d.\ Gaussian).
Let $x_1,\ldots,x_n$ be vectors in $\R^d$ with $\|x_i\| \le 1$. 
Then, there exist signs $\sigma_i$ such that $\sum \sigma_i x_i \in C \mathcal K$ for some universal constant $C$.

To invoke Banaszczyk's theorem here we set $x_i \in \R^{d_1d_2}$ to be the flattening of $X_i$ into a vector by concatenating its rows. 
Let $\mathcal K$ be the set of vectors in $\R^{d_1d_2}$ whose stacking (the inverse of flattening) results in a matrix whose spectral norm is less then $c\sqrt{d_1 + d_2}$.
More accurately $\mathcal K = \{x  \in \R^{d_1d_2} |\;\; ||\operatorname{stack}(x)||_2 \le c\sqrt{d_1+d_2}\}$. 
Due to $\operatorname{stack}$ being linear it is obvious that $\mathcal K$ is convex. 
Moreover $\Pr[g \in \mathcal K] \ge 1/2$ due to standard results on the operator norm of random Gaussian matrices (see Theorem~\ref{thm:random tensor}).
Banaszczyk's theorem says that there exist signs $\sigma_i$ such that $\sum \sigma_i \operatorname{flatten}(X_i) \in C \mathcal K$.
In other words $\| \sum \sigma_i X_i\| = \| \operatorname{stack}( \sum \sigma_i \operatorname{flatten}(x_i x_i^T)) \| \le C \sqrt{d}$. 

\end{proof}


The algorithm for finding the signs above is, unfortunately, not as simple as with kernels. 
In fact, Banaszczyk's theorem was not constructive and a matching algorithms is a very recent breakthrough result by Bansal, Dadush, Garg, and Lovett \cite{DBLP:conf/stoc/BansalDGL18}.

Using the general framework above gives a matrix covariance coreset of cardinality $O(\sqrt{d}/\eps)$.
Alternatively, we obtain a streaming mergeable algorithm for creating coresets of size $\log^{2}(\min(n, \log(1/\delta)) \sqrt{d}/\eps$.



\el{Selecting $o(1/\eps^2)$ is claimed to be hard by \cite{DBLP:conf/focs/DeshpandeR10} and \cite{DBLP:conf/soda/DeshpandeRVW06}. We need to see if these hold in our setting or only hold for sampling.  Also, \cite{DBLP:conf/soda/GhashamiP14} showed that the impossibility result is limited to frequent directions like settings. Their assumptions don't hold here.} 

\section{Streaming Graph Approximation}
Streaming graphs have received a lot of attention in the last several years \zk{cite something here \cite{}}. 
In the standard setting, the edges of a graph $G(V,E)$ are given to an algorithm one ofter the other in arbitrary order.
Here we show how to approximate all quadratic forms $\sum_{(i,j) \in E} (q_i - q_j)^2$ where the values $q$ are not known in advance.
Let $L_e$ be the graph Laplacian of the edge $e$. More accurately, $L_e(i,i) = L_e(j,j) = 1$ and  $L_e(i,j) = L_e(j,i) = -1$ and all other values are zeros.
Then, $\sum_{(i,j) \in E} (q_i - q_j)^2 = q^T \sum_{e \in E}L_e q = q^T L q$. 
In this formulation, this problem reduced to that of row (column) subset selection. 
Note that $L_e = v_ev_e^T$ where $v_e(i) = 1$, $v_e(j)=-1$ and zero else.
Applying the results of the last section we get a streaming mergeable algorithm of approximating graph quadratic forms.
The number of retained edges in the coreset $E'$ is $|E'| = O(\log^{2}(n) \sqrt{n}/\eps)$ and it guaranties that 
$|\sum_{(i,j) \in E} (q_i - q_j)^2  - c \sum_{(i,j) \in E'} (q_i - q_j)^2| \le \eps n$ for all unit norm vectors $q$.


\end{document}



%Specifically, we set $\sigma_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $\sigma_j$'s up to and not including $i$,
%$$\E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} \sigma_j z_j$.
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_{j=1}^k \sigma_j z_j \|^2] = \E[\| v + \sum_{j=i}^k \sigma_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $\sigma_i$ deterministically,
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + \sigma_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] $$
%we need
%$$ 2\ip{m,\sigma_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + \sigma_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ \sigma_i = -\text{sign} \ip{m,z_i}$$

