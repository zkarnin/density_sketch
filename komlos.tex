\documentclass[anon,12pt]{colt2019} % Anonymized submission
% \documentclass[12pt]{colt2019} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Discrepancy, Coresets, and Sketches in Machine Learning]{Discrepancy, Coresets, and Sketches in Machine Learning}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{ amssymb }

\usepackage[]{algorithm2e}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
%\newtheorem*{definition*}{Definition}
%\newtheorem{definition}{Definition}
%\newtheorem{claim}{Claim}
%\newtheorem{theorem}{Theorem}


\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

%\newcommand{\zk}[1]{}
%\newcommand{\el}[1]{}


\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
%\usepackage{ntheorem}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\disc}{\text{disc}}
\renewcommand{\Pr}{\operatorname{Pr}}

\newcommand{\komconstant}{c}


% Authors with different addresses:
\coltauthor{%
 \Name{Zohar Karnin} \Email{zkarnin@amazon.com}%\\ \addr Address 1
 \AND
 \Name{Edo Liberty} \Email{libertye@amazon.com}%\\ \addr Address 2%
}


\begin{document}

\maketitle

\begin{abstract}
This paper defines the discrepancy for families of functions.  It shows that low discrepancy classes admit small offline and streaming coresets.
We provide general techniques for bounding the discrepancy of machine learning problems and specifically does so for matrix covariance approximation, logistic regression, kernel density and any analytic function of the dot product or the squared distance.
Our result resolves a long standing open question regarding minimal cardinality coresets for Gaussian kernel density estimation.  
We provide two more related but independent results. 
First, an exponential improvement of the widely used merge-and-reduce trick which gives improved streaming sketches for any low discrepancy problem.
Second, an extremely simple algorithm for finding low discrepancy sequences (and therefore coresets) for any positive semi-definite kernel. 
This paper makes an explicit connection between low discrepancy, coresets, learnability, and streaming algorithms. 
\end{abstract}


\section{Introduction}
In both machine learning and in streaming and sketching problems our goal is (often) to approximate sums or expectations of well behaved functions.
Specifically, we need to approximate $\E_x f(x)$ or $\frac{1}{n}\sum_{i=1}^{n} f(x_i)$ for every $f\in \F$ where $\F$ is a family of functions and $x_i \in \mathcal X$ are either sampled training examples or an arbitrary set of stream items. 
Standard generalization results show that for a large enough value of $n$ the average approximates the mean if the complexity of $\F$ is bounded and the samples $x_i$ are drawn i.i.d.\ from underlying distribution. We therefore focus on approximating the average, or rather, the sum $\sum_{i=1}^{n} f(x_i)$. For notational convenience we use a parameter $q \in \mathcal Q$ to index into $\F$ explicitly $F(q) = \sum_{i=1}^{n} f(x_i, q)$. 
One should think about $q$ as either the model parameters or a query for the sketch. 

The goal is to produce a coreset. This is a set of weights $w$ with at small number of non-zeros such that $\sum_{i=1}^{n}w_i f(x_i,q) := \tilde F(q)$ approximates $F(q)$.
Approximation here means that $|\tilde F(q)  - F(q)| \le \eps n$ either for all $q \in \mathcal Q$ simultaneously or for every fixed $q$ with probability at least $1-\delta$. 
There are more complicated formulations such as weak coresets which we will not touch upon in this manuscript. 
Generating a concise representation $\tilde F$ for $F$ allows one to optimize over $\tilde F$ instead of $F$ which is more efficient. 
Moreover, if the resulting coresets are mergeable, this could be done on separate streams without the need for communication or assuming randomness in the partitioning.

For bounded functions $f$, uniform sampling of $O(\log(1/\delta)/\eps^2)$ combined with a union bound over $|\mathcal Q|$ always provides a valid solution using $O(\log(|\mathcal Q|)/\eps^2)$ items. 
While $|\mathcal Q|$ is often infinite it reduces to a finite (albeit usually exponentially large) set through standard epsilon net arguments. 
%
We present a mechanism for producing coresets which are much smaller than those achieved by sampling for a large class of problems in a unified manner. 
Moreover, our solution creates streaming algorithms with fully mergeable sketches. 
The size of the coreset above appears to be intimately tied to the discrepancy properties of the associated functions.


\section{Discrepancy of a Function Class}
We begin by giving three equivalent definitions of complexity based on discrepancy for sets, functions, and function families. We will use all three interchangeably throughout the manuscript.
\begin{definition}
Let $A \subset \R^m$ and $\sigma \in \{-1,1\}^m$ the discrepancy of $A$ is 
\[
K_m(A)  = \min_\sigma \max_{a\in A} \left| \frac{1}{m} \sum_i  \sigma_i  a_i\right| 
\]
\end{definition}
%
\begin{definition}
Let $f:\X,\Q\rightarrow\R$ and $\sigma \in \{-1,1\}^m$. 
The discrepancy of $f$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
K_m(f) =  \min_{\sigma} \max_{q \in \Q}  \left|\frac{1}{m} \sum_{i=1}^{m}\sigma_i f(x_i,q)\right|
\]
\end{definition}
%
\begin{definition}
Let $\F$ be a family of functions $f:\X\rightarrow\R$ and $\sigma \in \{-1,1\}^m$. 
The discrepancy of $\F$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
K_m(\F) =  \min_{\sigma} \max_{f \in \F}  \left| \frac{1}{m} \sum_{i=1}^{m}\sigma_i f(x_i)\right|
\]
\end{definition}
The discrepancy of $f$ or $\F$ without a reference a set $\{x_1,\ldots,x_m\}$ is the upper bound on any such set.
Throughout the manuscript we assume a one to one mapping between $\F$ and $\Q$. 
Specifically, $f \in \F$ if there exists $q \in \Q$ such that $f(x) = f(x,q)$. This is an obvious equivalence when one thinks about $q$ is the model parameters. 


\noindent Notice that these definition are very similar to $R_m(\F)$ the Rademacher complexity.
The only difference is that we consider the minimal value over $\sigma$ rather than its expectation for a random $\sigma$.
Rademacher complexity uses random signs because it measures the effect of data being chosen uniformly at random (or drawn from an unknown distribution). See \cite{Bartlett:2003:RGC:944919.944944} for other notions of complexity and relationships between them.\footnote{\cite{Bartlett:2003:RGC:944919.944944} use the same ``discrepancy'' to define a different quantity than the one above.}
 We claim that $K_m$ is the measure more suitable for creating coresets and for creating mergeable sketches.
Trivially,  $K_m \le R_m$. We show, however, that $K_m(\F) = o(R_m(\F))$ for a wide range of interesting problems in machine learning.


To understand our motivation, consider the following informal explanation of the Radamacher Complexity applied to ML problems. 
In PAC learning there exists a set of examples (with labels).
We aim to find a regressor/classifier from a given family that suffers the least loss on the set. 
Having a low Radamacher complexity means that we can optimize over a sample of roughly half the examples at random (each w.p.\ $1/2$).
Low Rademacher complexity guaranties that, in expectation, twice the loss on the sample is roughly the same as the loss on the entire set.
This translates to a generalization bound. 
In other words, the Radamacher complexity gives a guarantee for the loss of coresets chosen uniformly at random.

Coming back to discrepancy. Having the ability to choose the signs arbitrarily lets us choose an advantageous subset of examples.
We can algorithmically choose to minimize the induced error and guaranty to have (roughly) the same performance on the entire set. 
This set is in fact a coreset. The discrepancy of a problem helps us determine what coreset sized are possible. 
We will show in the following sections several examples for which a coreset can be significantly smaller than the random sample while maintaining the same guarantees. This will be done by showing that for a wide range of interesting problems in machine learning $K_m(\F) = o(R_m(\F))$.
This intuition is restated more explicitly in the next section.

\subsection{General Framework for Coresets}
In what follows it will be convenient to use the notation $f(x,q)$ and consider $K_m(f)$.
%
%to represent the function family as a family of vectors representing the functions. The vector family will be denoted as $Q$, where we use $q$ to denote a single vector corresponding to a function. We a universal function $f$ such that $f(x,q)$ corresponds to the value of the function matching $q$ when applied to $x$. We will assume from hereon that for any $x,q$ $|f(x,q)| \leq 1$; this allows to simplify the presentation. Our results can extend to $|f(x,q)| \leq R$ and will have dependencies on the bound $R$. Achieving these bounds is a purely technical task that we defer for the sake of readability. 
Consider the function $F(q) = \sum_{i=1}^{m} f(x_i,q)$ and the signed sum of error function $E(q) = \sum_{i=1}^{m} \sigma_i f(x_i,q)$ where $\sigma_i \in \{-1,1\}$.
Now consider $\tilde F_{+}(q) = F(q) + E(q)   = \sum_{i | \sigma_i=1} 2 f(x_i,q)$ and similarly $\tilde F_{-}(q) = F(q) - E(q)  =  \sum_{i | \sigma_i=-1} 2 f(x_i,q)$. We have that both $\tilde F_{+}(q)$ and $\tilde F_{-}(q)$ are approximations for $F(q)$ obtained by coresets of item weights of $2$ and an error at most $|\tilde F_{\pm}- F(q)| =  |E(q)|$.

%
The above is true for any choice of signs $\sigma$, specifically, for those minimizing $\max_q | E(q)|$.
Note that we can select signs such that $|E(q)| \le m K_m(f)$.
The following facts are true for the common cases where $K_m = O(c/m)$ or $K_m = O(c/\sqrt{m})$.
The constant $c$ does not depend on $m$ but could depend on other parameters of the problem. 

\begin{fact}
Let $f$ be a function with a corresponding discrepancy $K_m(f) = O(c/m)$. Let $F(q) = \sum_{i=1}^{n}f(x_i, q)$. There exists a coreset for $F$ of size $O(c/\eps)$ whose error is at most $\eps n$.
\end{fact}
\begin{fact}
Let $f$ be a function with a corresponding discrepancy $K_m(f) = O(c/\sqrt{m})$. Let $F(q) = \sum_{i=1}^{n}f(x_i, q)$. There exists a coreset for $F$ of size $O(c^2/\eps^2)$ whose error is at most $\eps n$.
\end{fact}

\noindent Proving both facts is trivial and known in the folklore as ``the halving trick". 
At step one, create a coreset of size at most $n/2$ of items of weight $2$ and incur an error of at most $c$ or $c \sqrt{n}$.
Then, we repeat the process and create a coreset of size $n/4$ of items of weight $4$. Here, you incur error of $2c$ or $2c\sqrt{n/2}  = c\sqrt{2n}$.
Note that the sum of errors is a geometric sequence that is asymptotically dominated by its last element. 
Halting the compression at $\Theta(c/\eps)$ or $\Theta(c^2/\eps^2)$ items respectively achieves the goal.



\subsection{General Framework for Sketching}\label{sec:sketch}
We claim that low discrepancy implies concise streaming mergeable coresets as well. 
This section assumes the existence of an algorithm for finding low discrepancy signs $\sigma$.

\begin{theorem} \label{thm:streaming11}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/m)$ there exists an fully-mergeable streaming coreset deterministic algorithm of size $O\left(c\log^2(\eps n/c)/\eps\right)$ whose error is at most $\eps n$.
\end{theorem}

\begin{theorem} \label{thm:streaming21}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/\sqrt{m})$ there exists a fully-mergeable streaming coreset deterministic algorithm of size $O\left(c^2\log^3(\eps^2 n/c) /\eps^2\right)$ whose error is at most $\eps n$.
\end{theorem}

\begin{theorem} \label{thm:streaming12}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/m)$ there exists an fully-mergeable streaming coreset randomized algorithm of size $O\left(c\log^2\log(n/\delta)/\eps\right)$ whose error for any fixed function $f \in \F$ is at most $\eps n$ with probability at least $1-\delta$. 
\end{theorem}


\begin{theorem} \label{thm:streaming22}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/\sqrt{m})$ there exists a fully-mergeable streaming coreset randomized algorithm of size $O\left(c^2\log^3\log(n/\delta) /\eps^2\right)$ whose error for any fixed function $f \in \F$ is at most $\eps n$ with probability at least $1-\delta$. 
\end{theorem}

Notice that the randomized versions above hold w.h.p.\ for any fixed $f \in \F$. Although $\F$ is typically inifinite, it is almost always the case that there exists an $\eps$-net over $\F$ whose size is exponential in the input parameters. Since the dependence over the error probability is doubly logarithmic, the randomized versions coupled with a union bound provides a coreset for an entire family that matches the non-streaming version up to poly-logarithmic terms.

The algorithms must be given $\eps, \delta$ as input but do not need to know the length of the stream, $n$, in advance. 
The proofs of the above two theorems are quite technical and delegated to Appendix~\ref{app:sketch proof}. Most of the ideas are already present in the recent breakthrough work on streaming quantiles \cite{DBLP:conf/focs/KarninLL16}. Generalizing the construction at  \cite{DBLP:conf/focs/KarninLL16} required resolving some added complications. The main ideas are as follows. 
In the last section we argued that both $F_1$ and $F_{-1}$ are both good approximations for $F$. We can also take $F'$ which is $F_1$ or $F_{-1}$ equally likely. Clearly $|F' - F| \le |E|$ as before. But now, $\E[F'] = F$ as well. In the streaming algorithm, we apply this compaction (converting $F$ to $F'$) many times over small subset of items from the stream. This allows us to use standard concentration results to bound the overall error. The main departure from \cite{DBLP:conf/focs/KarninLL16} is that $F'$ no longer has exactly half the size $F$. In the general setting, this only true in expectation.

\section{Related Work}
\begin{itemize}
\item coresets - motivation, some papers, streaming algorithms for coresets

\item Radamacher complexity \cite{Bartlett:2003:RGC:944919.944944} is a known method to estimate the generalization ability of an empirical risk minimizer (ERM) of a family of function over a finite set of i.i.d sampled points. A carefully selected set of points can potentially obtain a better generalization bound, and that is the usefulness of coresets in the context of machine learning. We are not aware of any analog to Radamacher complexity that aims to measure the generalization ability of a coreset of a fixed finite size. There are papers such as \cite{tolochinsky2018coresets} and references within that tie the coreset to the VC dimension of the family function, and the average sensitivity of the dataset. However, these relationships come up more as tools for constructing a coreset rather than a single complexity measure aimed to characterize some generalization ability.
%
% There is literature about the complexity of active learning for the supervised setting but that is out of scope as we deal with the setting where all labels are available, or the problem is unsupervised. For the complexity of coresets, the total sensitivity of the dataset and the VC dimension of the function class is often used as tools for construction \zk{cite something?}. However, to the best of our knowledge there is no clear characterization of the size of coresets based on some complexity measure of the dataset and function class.

%
%\item Radamacher complexity \cite{Bartlett:2003:RGC:944919.944944}, There is literature about the complexity of active learning for the supervised setting but that is out of scope as we deal with the setting where all labels are available, or the problem is unsupervised. For the complexity of coresets, the total sensitivity of the dataset and the VC dimension of the function class is often used as tools for construction \zk{cite something?}. However, to the best of our knowledge there is no clear characterization of the size of coresets based on some complexity measure of the dataset and function class.

\item Kernel density estimation is a popular tool in data analysis aimed to estimate a continuous distribution with a finite set of points. Among other applications, this tool is used for outlier detection \cite{schubert2014generalized}, regression \cite{fan2018local}, and clustering \cite{rinaldo2010generalized}. A thorough survey could be found in \cite{silverman2018density}. Given a set of $n$ data points $x_1,\ldots,x_n$ and a query $q$, the density estimate of $q$ is the average $\frac{1}{n} \sum_i K(x_i,q)$ of the kernel evaluations with the data points. The task of finding a coreset for kernel density estimates consists of obtaining a set of points $z_1,\ldots,z_m$ with weights $w_1,\ldots,w_m$ for some $m \ll n$ such that for any query $q$ it holds that 
$\frac{1}{n} \sum_i K(x_i,q) = \frac{1}{m} \sum_i w_i K(z_i,q) \pm \eps$. For this task, the state-of-the-art is given by \cite{DBLP:journals/corr/abs-1802-01751}, achieving $m=\sqrt{d\log(1/\eps)}/\eps$ where $d$ is the dimension of the original data points. Their result holds for Lipchitz bounded positive semi-definite kernels. The result is constructive though it is polynomial rather than (quasi-)linear in the data size. The authors give an almost matching lower bound of $\sqrt{d}/\eps$ and pose an open question for closing the gap between the bounds.

\item Logistic Regression is an extremely popular technique for classification. We are given a set of $n$ pairs $x_i,y_i$ with $x_i \in \R^d$ and $y_i \in \{-1,1\}$. The objective is to find a regressor $q \in \R^d$ minimizing the logistic loss defined as $\E_i \log(1+\exp(x_i^\top q y_i))$. A coreset for logistic regression is a set of $m \ll n$ pairs $z_i, v_i$ in $\R^d, \{-1,1\}$ correspondingly along with weights $w_i$, with the property that for any regressor $q$, either $\E_i \log(1+\exp(x_i^\top q y_i)) = \E_i \log(1+\exp(z_i^\top q v_i)) \pm \eps$ or $\E_i \log(1+\exp(x_i^\top q y_i)) = \E_i \log(1+\exp(z_i^\top q v_i)) (1 \pm \eps)$. A recent paper \cite{DBLP:journals/corr/abs-1805-08571} provides a coreset with a multiplicative guarantee that is based on an average sensitivity property of the dataset. They provide a lower bound for the size of a multiplicative coreset showing that in general it is not possible to achieve $m \ll n$. The coreset they build is of cardinality $m \approx \mu\sqrt{nd^3}/\eps^2$ where $\mu \geq 1$ is complexity measure of the dataset.  \cite{tolochinsky2018coresets} gives a generic multiplicative coreset construction for any monotonic function with $\ell_2^2$ regularization. The dependence they get is $\tilde O(d/\eps^2)$ ignoring logarithmic factors.  

\item Linear Regression \cite{DBLP:journals/jmlr/DerezinskiW18}
%\item coreset of logistic regression  \cite{DBLP:journals/corr/abs-1805-08571}, \cite{DBLP:conf/nips/HugginsCB16}. Give multiplicative approximation. This is impossible in general so they depend on a different restriction on the regressor or dataset. Based on sensitivity scores. \cite{DBLP:conf/nips/HugginsCB16} use a heuristic of using $k$-means to approximate sensitivity scores and prove that given good sensitivity scores they get a good coreset. \cite{tolochinsky2018coresets} gives a generic coreset construction for any monotonic function as long as it comes with an $\ell_2^2$ regularization. The dependence they get is, ignoring polynomial factors, $d/\eps^2$.  \cite{DBLP:journals/corr/abs-1805-08571} have a different method, relying on a measure of complexity of the dataset and obtain roughly $\sqrt{n}d^{3/2}/\eps^2$ sized dataset. 
\end{itemize}




\section{The Complexity of Analytic Functions of Dot Products} \label{sec:analytic}

Now that we proved the usefulness of discrepancy we move to upper bound it for common family functions. We provide a coreset suitable for analytical functions of the inner product $\ip{q,x}$ or squared Euclidean distance $\|q-x\|^2$. 

The idea is to find a set of signs that simultaneously balance $\ip{q,x}^k$ for all powers $k$ and unit vectors\footnote{We Assume that $\|x\|,\|q\| \leq 1$ for ease of presentation. As above our results extend to generic bounds on the radius of $q$} $q$. By controlling all powers of $\ip{q,x}$ we control any sum of these powers. It follows that this coreset can be used to control, for example, the logistic loss function $L(q,x) = \log(1+\exp(\ip{q,x}))$ or the gaussian Kernel $K(q,x) = \exp(-\lambda \|q-x\|^2)$. 


We start with some notation and trivial properties. 
For a vector $q \in \R^d$ let $q^{\otimes k}$ represent the $k$-dimensional tensor obtained from the outer product of $q$ with itself $k$ times. For a $k$ dimensional tensor with $d^k$ entries $X$ we consider the measure
$\|X\|_{T_k} = \max_{q \in \R^d, \|q\|=1} \left| \langle X, q^{\otimes k}\rangle \right|$.
\begin{fact}
$\|X\|_{T_k}$ is a norm
\end{fact}
\begin{proof}
We prove the claim directly from the definition of a norm.
Notice that for any $X \neq 0$, $\ip{X, q^{\otimes}}$ is a non-zero polynomial in $q$. It follows that there must be $q$ for which its value is non-zero, meaning that $\|X\|_{T_k}=0$ iff $X=0$. For a scalar $a$, we clearly have by definition that
$\|aX\|_{T_k} = |a|\|X\|_{T_k}$.  Lastly, by the max definition we  have
$ \|X+Y\|_{T_k} =  \max_q \left| \langle X+Y, q^{\otimes k}\rangle \right| \leq 
\max_q \left| \langle X, q^{\otimes k}\rangle \right| + \max_q\left| \langle Y, q^{\otimes k}\rangle \right| = \|X\|_{T_k} + \|Y\|_{T_k}$
\end{proof}

We are now ready for the lemma controlling all powers of inner products simultaneously. 

\begin{lemma}\label{uc}
For any set of vectors $x_i \in \R^d$ with $\|x_i\| \leq 1$ there exist a set of signs $\sigma_i$ such that for all $k$ simultaneously $\left\| \sum_i \sigma_i x_i^{\otimes k} \right\|_{T_k} \le O(\sqrt{d k\log^{3}{k}})$ (the $3$ power of the term $\log(k)$ can be reduced to any constant power larger than $2$). 
\end{lemma}
\begin{proof}
The proof will use Banaszczyk's theorem \cite{Banaszczyk}. 
Let $\mathcal K$ be a convex body in Euclidean space with Gaussian measure at least 1/2 ($\Pr[g \in \mathcal K] \ge 1/2$ when $g$ is i.i.d.\ Gaussian).
Let $x_1,\ldots,x_n$ be vectors with $\|x_i\| \leq 1$. 
Then, there exist signs $\sigma_i$ such that $\sum \sigma_i x_i \in C \mathcal K$ for some constant $C$.

To use Banaszczyk's theorem we begin with defining our convex body.
Define the norm $\|\psi\|_T$ of a vector $\psi$ as follows. Look at the first $d$ coordinates of $\psi$ as a vector $\psi_1$, the next $d^2$ coordinates of $\psi$ as a matrix $\psi_2$ the next $d^3$ coordinates as a three tensor $\psi_3$ etc.
We define $\|\psi\|_T = \max_k \|\psi_k\|_{T_k} /\sqrt{\log(k)}$. 
Here, $\|\cdot\|_{T_k}$ is the special spectral norm defined in the beginning of the section.
The maximum over norms of subvectors is clearly a norm in itself, meaning that $\|\cdot \|_T$ is indeed a norm. It follows that  the set $\mathcal K  = \{\psi \; | \; \|\psi\|_T \le c\sqrt{d}\}$ is convex. 

We now need to show that the Gaussian measure of $\mathcal K$ is at least $1/2$. 
That is, with probability at least $1/2$ a vector of random Gaussian entrees $g$ belongs to $\mathcal K$.
Consider a random i.i.d.\ Gaussian Tensor $g_k \in \R^{d^k}$. 

A trivial modification of Theorem 1 from \cite{tomioka2014spectral} shows that $\Pr[\|g_k\|_{T_k} \ge c\sqrt{d\log(k)}] \le 1/10k^2$ for some constant $c$. The only change needed in the proof is the size of the epsilon net which changes from $(2\log(3/2)/k)^{kd}$ for \cite{tomioka2014spectral} to $(2\log(3/2)/k)^d$. The reason we require a net over a smaller space is due to us bounding the inner product with a rank one tensor rather than rank $k$. Union bounding on all values of $k$ we get $\sum_k 1/10k^2 \le 1/2$ which shows $g = [g_1, \operatorname{flat}(g_2), \operatorname{flat}(g_3), \ldots]$ belongs to $\mathcal K$ with probability at least $1/2$, where $\operatorname{flat}(g_k)$ is the flattening of the tensor into a one dimensional vector. 
%
We now define a mapping $\psi(x)$ of $x\in \R^d$ to a high dimensional space. 
%The function $\operatorname{flat}(X)$ simply strings the entry values of the tensor $X$ into a vector.
$$\psi(x) = \left[x, \frac{\operatorname{flat}(x^{\otimes 2})}{\sqrt{2\log^2(2)}}, \frac{\operatorname{flat}(x^{\otimes 3})}{\sqrt{3\log^2(3)}}, \ldots,\frac{\operatorname{flat}(x^{\otimes k})}{\sqrt{k\log^2(k)}},\ldots \right]$$

\noindent Note that for $\|x\| \le 1$ we have $\|\psi(x)\|_2 = (\sum_k  1/k\log^2(k))^{1/2} = O(1)$.


We are now ready to apply Banaszczyk's theorem. 
There exist signs $\sigma_i$ such that $\psi  = \sum_i \sigma_i \psi(x_i) \in C \mathcal K$.
Since $\psi_k = \sum_i \sigma_i x_i^{\otimes k}/\sqrt{k \log^2{k}}$ we get that 
$$\max_k \frac{\|\sum_i \sigma_i  x_i^{\otimes k}\|_{T_k}}{\sqrt{k \log^{3}(k)}} \le O\left( \sqrt{d} \right)$$
\end{proof}

\begin{lemma} \label{lem:komlos anl}
Let $f$ be a function of the inner product $f(x,q) = f(\langle x,q\rangle)$ and let $f = \sum_k \alpha_k \langle x,q\rangle^k$ be its Taylor expansion. 
The discrepancy of $f$ indexed by $\|q\| \leq 1$ is bounded by
\[
K_m = \min_\sigma \sum_i \sigma_i f(x_i,q) =O\left( \sqrt{d} \sum_k  |\alpha_k|\sqrt{k\log^3(k)}\right)
\]
For general $\|q\| \leq R$ we get
\[
K_m = \min_\sigma \sum_i \sigma_i f(x_i,q) =O\left( \sqrt{d} \sum_k  |\alpha_k| R^k \sqrt{ k\log^3(k)}\right)
\]
\end{lemma}
\begin{proof}
The proof follows from combining the above.
$$
\sum_i \sigma_i f(x_i,q) = \sum_k \alpha_k \sum_i \sigma_i \ip{ x_i,q}^k =  \sum_k \alpha_k  \ip{  \sum_i \sigma_i x_i^{\otimes k},q^{\otimes k}} \le $$
$$\sum_k |\alpha_k| \cdot \left\| \sum_i \sigma_i x_i^{\otimes k}\right\|_{T_k} \cdot \|q\|^k
$$
By Lemma \ref{uc} we can find signs $\sigma$ such that 
$\left\| \sum_i \sigma_i x_i^{\otimes k}\right\|_{T_k} \le c\sqrt{d k \log^3(k)}$. Substituting into the above, the lemma follows.
\end{proof}

\begin{theorem}\label{analitic1}
Let $f:\R\rightarrow\R$ be analytic. There exist a radius $R$ such that the function family of functions $f = f(\ip{q,x})$, indexed by $\|q\| \leq R$, has discrepancy $O(\sqrt{d}/m)$. 
\end{theorem}
\begin{proof}
Recall that for analytic functions $f$ we have $\left| \frac{d^k f}{dz^k}(z) \right|  \leq C^{k+1} k! $
for some constant $C$. Considering the taylor expansion of $f$ near zero, for $R < 1/C$ the sum
$ \sum_k  |\alpha_k| R^k \sqrt{ k\log^3(k)} \leq C \sum_k  (CR)^k \sqrt{ k\log^3(k)}$
corresponding to Lemma~\ref{lem:komlos anl} converges to a constant. The result follows.
\end{proof}

\begin{corollary}
The discrepancy of the Logistic function $f(\ip{q,x}) = \log(1+\exp(\ip{q,x}))$ in dimension $d$, for $\|q\| \leq 1$ is $O(\sqrt{d}/m)$.
\end{corollary}

\begin{corollary}
The discrepancy of the covariance function $f(\ip{q,x}) = \ip{q,x}^2$ in dimension $d$, for $\|q\| \leq 1$ is $O(\sqrt{d}/m)$. This gives coresets for matrix column subset selection such that $\|XX^T - \tilde X \tilde X^T\| \le \eps n$ where $\tilde X$ contains $O(\sqrt{d}/\eps)$ rescaled columns of the matrix $X$.
\end{corollary}



\begin{theorem} \label{thm:analytic2}
Let $f:\R\rightarrow\R$ be analytic. There exist a radius $R$ such that the function family of functions $f_q(x) = f(\|x-q\|^2)$, indexed by $\|q\| \leq R$, has discrepancy $O(\sqrt{d}/m)$. 
\end{theorem}
\begin{proof}
By transforming $x$ to $\tilde{x} = (1, \sqrt{2}x, \|x\|^2)$ and $q$ to $\tilde{q} = (\|q\|^2, -\sqrt{2}q, 1)$ we get $\ip{\tilde{x},\tilde{q}} = \|q-x\|^2$. Moreover, $\|q\| \le R$ gives $\|\tilde q\| \le R^2+1$. The result follows from applying Theorem~\ref{analitic1} to $f(\ip{ \tilde q, \tilde x}) = f(\|q-x\|^2)$.
\end{proof}

\begin{corollary}
The discrepancy of the Gaussian kernel $K(q,x) = \exp(-\gamma \|x-q\|^2)$ in dimension $d$ is $O((1+\gamma)\exp(\gamma)\sqrt{d}/m)$.
\end{corollary} 
This improves upon the recent result of \cite{DBLP:journals/corr/abs-1802-01751} by proving the existence of $\eps$ approximation corsets of size $\sqrt{d}/\eps$ for Gaussian kernel density, in the case where $\gamma$ is constant. 
This also resolves the open problem raised by \cite{DBLP:journals/corr/abs-1802-01751} and matches their lower bound.   

\begin{proof}
W.l.o.g. the maximum distance $\|q-x\|$ is 1. The Taylor series of the Gaussian kernel $K$ becomes
$ \sum_{k=0}^\infty \frac{(-\gamma)^k}{k!} $.
Plugging into the equation in the proof of Theorem~\ref{thm:analytic2} we get that the sum determining the constant is upper bounded by
$ \sum_{k=0}^\infty \frac{\gamma^{k}\sqrt{ k\log^3(k)}}{k!} = O\left((1+\gamma) \exp(\gamma)\right)$.
\end{proof}
%\end{proof}


\section{A Simple Algorithm for Kernel Density Estimation}
From section \ref{sec:analytic} we know that the discrepancy of the Gaussian kernel is $K_m = O(\sqrt{d}/m)$. 
%The result however is non-constructive in the sense that we are not aware of an algorithm that can obtain the $\sigma$ signs efficiently. 
Here, we provide a computationally efficient bound that can be achieved with a straightforward algorithm of complexity $O(m^2)$. Together with the results of Section~\ref{sec:sketch} this provides an efficient sketching algorithm for Kernel Density Estimation. 
In fact, we show that for any positive kernel $K_m = O(1/\sqrt{m})$. This bound is superior to that of the previous section for high dimensions $d > \sqrt{m}$. More importantly though, there is a very simple, intuitive, and deterministic algorithm for computing the signs $\sigma$. 
Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $f: \R^d \rightarrow \R$ of a point $q$ is defined as 
$$ F(q) = \sum_{i=1}^{n} K(x_i,q) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include
$$ K(x,q) = \exp(- \|x-y\|_2^2/\lambda^2)\;\;\; K(x,q) = \exp(- \|x-q\|/\lambda) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-q\|/_2^2/\lambda^2)^{-1}$$
where $\lambda$ is a scaling parameter. For simplicity, we assume that $K(x,x) \leq 1$ for all data points. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.

\begin{algorithm}[H]
 \KwData{Kernel function $K:(\R^d,\R^d)\rightarrow[0,1]$, points  $x_1,\ldots,x_m$}
 \KwResult{$\sigma \in \{-1,1\}^m$ such that $\max_q |\sum_i \sigma_i K(x_i,q) | \le \sqrt{m}$}
 $\sigma_1 = 1$;\\
 \For{$i = 2,\ldots,m$}
 {$\sigma_i = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j  K(x_j, x_i))$}
% \EndFor
 \caption{Low Discrepancy Algorithm for Positive Kernels}
\end{algorithm}


%The algorithm works as follows: Set $\sigma_1 = 1$. For $i=2,\ldots,m$ set $\sigma_i = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j  K(x_j, x_i))$.
\begin{theorem}
The algorithm above achieves $\max_q |\sum_i \sigma_i K(x_i,q) | \le \sqrt{m}$.
\end{theorem}

\begin{proof}
For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$ K(x,q) = \ip{\phi(x), \phi(q)} $.
Using this function $\phi$ our objective function becomes
\[
|\sum_{i=1}^m \sigma_i K(x_i,q)| = |\sum_{i=1}^m \sigma_i \ip{\phi(x_i), \phi(q)} | = \left| \ip{ \sum_{i=1}^m \sigma_i \phi(x_i), \phi(q)}\right| \leq  \|\phi(q)\| \cdot \left\|  \sum_{i=1}^m \sigma_i \phi(x_i) \right\| 
\]
Since $\|\phi(q)\| \leq 1$ we reduced the problem to bounding the norm of $ \sum_{i=1}^m \sigma_i \phi(x_i) $.
%
We show by induction on $i$ that 
$$\left\| \sum_{j=1}^i \sigma_j \phi(x_j) \right\|^2 \le \sum_{j=1}^i \left\|\phi(x_j)\right\|^2 \leq i$$
This is trivially true for $i=1$ since $\|\phi(x)\| \leq 1$. 
Using our induction assumption we get
\begin{eqnarray*}
\left\| \sum_{j=1}^{i}\sigma_j \phi(x_j)\right\|^2 &=& \left\|\sum_{j=1}^{i-1}\sigma_j \phi(x_j)\right\|^2 + \|\phi(x_i)\|^2 + 2\ip{ \sum_{j=1}^{i-1}\sigma_j \phi(x_j), \sigma_i \phi(x_i)} \\
&\le& \sum_{j=1}^{i-1} \|\phi(x_j)\|^2 + \|\phi(x_i)\|^2 + 2\sigma_i \sum_{j=1}^{i-1}\sigma_j K(x_j, x_i)\\
&\le& \sum_{j=1}^{i} \|\phi(x_j)\|^2 - 2|\sum_{j=1}^{i-1}\sigma_j K(x_j, x_i)| \le \sum_{j=1}^{i} \|\phi(x_j)\|^2 \\
\end{eqnarray*}
The first equality simply unpacks the squared vector norm, the second transition is due to the induction assumption and the last substitutes our choice of %$\sigma$. This completes the proof that $|\sum_{i=1}^m \sigma_i K(x_i,q)| \le \sqrt{m}$ for all $q$.
\end{proof}

Using the framework above this provides a deterministic coreset construction for kernel density estimation of size $O(1/\eps^2)$ such that $\forall \;q\;\; |\tilde F(q) - F(q)| \le \eps n$. This matches, and greatly simplifies, the results achieved by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751}. Along with an $\eps$-net argument, it also gives rise to a streaming algorithm for kernel density estimation with a memory requirement of $O\left(\log\left(d\log(1/\eps)\right)^3/\eps^2\right)$ vectors.
\el{We should confirm this statement. I believe the Lipshitz constant of $m$ kernels can be $m$ so we might need another $d$ inside the inner $log$.}

\paragraph{Note} The Theorem above provides an upper bound of $\sqrt{m}$ for the sign discrepancy. One can in fact construct an example where this upper bound is indeed what the algorithm achieves. However, for real data with vectors clustered together, as one should expect when requiring a density estimation algorithm, we expect this algorithm to perform much better than the worst-case bound. We leave it to future work to define properties of the data that ensure better guarantees by our algorithm.


\bibliography{density}


\appendix

%\section{Proofs for Section~\ref{sec:sketch}, Sketching Coresets} 
%
%\begin{lemma} \label{lem:compactor}
%Let $k$ be an integer and assume we have black box access to a solver that for any $k$ inputs $x_1,\ldots,x_k$ obtains $k$ signs $\sigma_1,\ldots,\sigma_k$ such that
%$$\max_q \left| \sum_{i=1}^{k} \sigma_i f(x_i, q)\right| \leq K_k$$
%Then there exist a streaming algorithm requiring a memory buffer of $k$ input items that given a stream of length $n$ outputs a stream $z_1,\ldots,z_m$ with the following properties
%\begin{itemize}
%\item $\{z_i\}_i$ is a subset of $\{x_i\}$
%\item $\E[m] = n/2$
%\item For $\rho >0$, $\Pr[m \geq (1+\rho)n/2] \leq \exp \left( -O\left(\frac{n\rho^2}{k}\right)\right)$
%\item For any fixed query $q$, the error associated with $q$, defined as
%$$\text{Err}(q) = 2\sum_{i=1}^m f(z_i,q) - \sum_{i=1}^n f(x_i,q)  $$
%Can be decomposed as a sum
%$$\text{Err}(q) = \sum_{j=1}^{n/k} \text{Err}_j(q)$$
%where the different $\text{Err}_j(q)$ are independent, have mean 0 and $|\text{Err}_j(q)| \leq K_k$
%\end{itemize}
%\end{lemma}
%
%Note that signs achieving $max_q \left| \sum_{i=1}^{k} \sigma_i f(x_i, q)\right| \leq K_k$ are guarantied to exist by the definition of the discrepancy. In fact, the quantity above is potentially much smaller than its upper bound because it is computed for a specific set of points $x_i$ as opposed to the worst such possible set. 
%
%\begin{proof}
%The algorithm operates as follows. It keeps a buffer of $k$ items. Once the buffer fills with $x_1,\ldots,x_k$ it obtains the signs guaranteeing
%$$\max_q \left| \sum_i \sigma_i f(x_i, q)\right| \leq K_k$$
%Then, the algorithm output to the stream the with probability $1/2$ each data points $\{x_i \; | \; \sigma_i = 1\}$ or $\{x_i \; | \; \sigma_i = -1\}$ with twice the weight.
%$$\sum_{i ,\; \sigma_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} \sigma_i f(x_i, q)$$
%$$\sum_{i ,\; \sigma_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} \sigma_i f(x_i, q)$$
%
%%Now, consider the set $P=\{i:\sigma_i=1\}$ and $N=\{i:\sigma_i=-1\}$.
%%$$ \sum_i \sigma_i f(x_i, q) = \sum_i \sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in P} f(x_i,q) - \sum_i f(x_i, q)   $$
%%$$ -\sum_i \sigma_i f(x_i, q) = \sum_i -\sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in N} f(x_i,q) - \sum_i f(x_i, q)   $$
%%Thus, we draw a random coin and either output to the stream the data points of $P$ or $N$. We call this operation a single compaction. 
%%\el{this is clunky way to say this...}
%The expected output length is $k/2$ for every $k$ inputs, insuring $\E[m]=n/2$. Moreover, the output length is a sum of $n/k$ independent random variables with mean $k/2$ and a maximum value of $k$. Chernoff-Hoeffding's bound can guarantee the stated concentration around $\E[m]$.
%As for the error term w.r.t.\ a fixed query $q$, it is clearly the sum of errors accumulated in each compaction. The error incurred in a compaction is either 
%$\sum_i \sigma_i f(x_i, q)$ or $-\sum_i \sigma_i f(x_i, q)$ with equal probability. It follows that it is a Radamacher r.v. scaled by a magnitude of at most $K_k$ as required.
%\end{proof}
%
%Notice in the above lemma the last property of the streaming algorithm; it ensures that deterministically $|\text{Err}(q)| \leq (K_k/k) n$, and that w.h.p.\ (via Chernoff's inequality) for a fixed $q$ 
%$$|\text{Err}(q)| \lesssim K_k \sqrt{n/k} = \sqrt{\frac{K_k^2 n}{k}}$$
%This high probability bound will be used below for our streaming algorithm. In high level the idea is to have a sequence of streaming boxes that we call compactors, each receiving an input stream and outputting a stream that is of half the size of the input (in expectation). Eventually we will get to a stream that is small enough to store in memory. The randomized bound above shows that for the compactors that handle a large stream, we can afford to have a very crude sketch because the error that they incur is roughly $\sqrt{n}$ even if all they do is uniformly sample half of the stream. However, for the compactors that observe the shorter streams that high probability bound is no longer as strong and we will have to make use of the fact that $K_k \ll k$.
%
%
%\begin{proof} [Proof of Theorems~\ref{thm:streaming} and~\ref{thm:streaming2}]
%Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.
%
%The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ and hierarchy level $h$ we analyze the error
%$$\text{Err}_h(q) = 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \ .$$
%Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. $K_h$ is used to denote the discrepancy of each compaction using a buffer of $k_h$ points. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. The overall error for the query $q$ is the sum over all levels of its errors
%$$ \text{Err}(q) = \sum_{h=0}^H \text{Err}_h(q)$$
%
%To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
%$$ \text{Err}(q) \leq (K_k/k) \log_2(n/k) n $$
%
%We are however interested in a high probability bound. Specifically we would like to guarantee for $\delta > 0$ that for any query $q$ the error is bounded by $\eps n$ w.p.\ at least $1-\delta$. To this end we consider a different analysis for the top and bottom layers. For the top layers, the stream can be arbitrarily short so we will use the deterministic analysis as above. For the bottom layers, we can guarantee a minimal length to the stream and obtain tighter bounds.
%
%The bottom layers will be those of height $h=0$ up to $h=H'$ where $H'$ is set adaptively in a way that $H'$ is the maximum integer such that for all $h \leq H'$, 
%\begin{equation} \label{eq:Htag_nh}
% \forall h \leq H' , \ \ n_h \geq c_1 k \log^2(n) \log(\log(n)/(c_2 \delta ))
%\end{equation}
%for some sufficiently large constant $c_1$ and small constant $c_2$ to be determined later.
%
%According to Lemma~\ref{lem:compactor} we have for $ h\leq H'$ that for some appropriate constant $c_1$ in  \eqref{eq:Htag_nh},
%$$ \Pr[n_{h+1} \geq (1+1/\log(n))n_{h}/2] \leq \exp\left( -\frac{n_{h}}{k_h \log^2(n)}  \right) \leq c_2\delta/  \log(n) $$
%We can now union bound over all 
%$$h \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta\log(n))) \right)\right) + 2 \leq \log_2(n)$$
%and conclude that w.p.\ at least $1-\delta/2$
%\begin{equation} \label{eq:Htag}
%H' \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) + 2 \leq \log_2(n)
%\end{equation}
%and
%\begin{equation} \label{eq:nh small}
%\forall h \leq H', \ \ n_{h} \leq 3 \cdot n/2^{h}
%\end{equation}
%
%
%Let's proceed to bound the error of layer $h$. Since this error is a sum of i.i.d.\ bounded variables of mean zero we can use Chernoff- Hoeffding's inequality and obtain that
%$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] =$$
%$$ \Pr\left[   \left| 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( O\left( -\frac{ \eps^2 n^2}{ 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } K_h^2}   \right)\right) =$$
%$$\exp \left( -O\left(\frac{\eps^2 n^2}{n_h 2^{2h} K_h^2/k_h  } \right) \right) 
%$$
%We now make use of the the bound on $n_h$ in Equation~\eqref{eq:nh small}. Notice that the random events determining the length $n_h$ do not affect the realization of the random variables used in layer $h$, so we can indeed use the bound on $n_h$ for controlling the error at level $h$. We get that since $n_h \leq 3n/2^h$
%$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq 
%\exp \left( -O\left(\frac{\eps^2 n}{ 2^h K_h^2/k_h  } \right) \right) 
%$$
%In particular, for $\eps_h = O\left( \sqrt{\frac{2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}} \right)$  it holds that
%$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq \delta/4(H'-h+1)^2$$
%and a union bound ensures that with probability at least $1-\delta$, in addition to Equations~\eqref{eq:Htag} and~\eqref{eq:nh small} we have for all $h \leq H'$ that
%$$\left| \text{Err}_h(q) \right| \leq \eps_h n$$
%and 
%$$\left| \sum_{h=0}^{H'} \text{Err}_h(q) \right| \leq \left(\sum_{h=0}^{H'} \eps_h\right)n $$
%
%Let's proceed to bound the error on the top layers $H'+1$ to $H$. In these top layers we use a deterministic algorithm guaranteeing the stream is cut at least times 2 from layer to layer. We assign all top layers a budget of $k_h=k$ for the compactors. Since $2^{H+1} < n/k$ this gives a bound of 
%$$
%H-H' \leq \log_2(n/k2^{H'}) = O\left( \log\left( c_1 \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) = 
%$$
%$$
%O\left( \log \log(n/\delta) \right)
%$$
%leading to a bound of 
%$$ \left| \sum_{h=H'+1}^H \text{Err}_h(q) \right| \leq O\left( \log \log(n/\delta) (K_k/k) n \right) $$
%and an overall bound of
%\begin{equation} \label{eq:err total}
%|\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(K_k/k) + \sum_{h=0}^{H'} \eps_h \right)
%\end{equation}
%
%We are now left with the task of defining $k_h$ for the lower layers; we do so differently depending on the value of $K_k$ as a function of $k$. Recall that we aim to deal with two settings. In the first $K_k=\min\{\komconstant,k\}$ and in the second $K_k=\min\{\komconstant \sqrt{k}, k\}$. In both cases  $\komconstant$ is independent of $k$. We start by dealing with the first scenario.
%
%We set $k_h = \max\{2, \ceil{(2/3)^{H'-h}k}\}$. By using the definition of $H'$ in Equation~\eqref{eq:Htag} we see that
%\begin{align*}
%\eps_h = & O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k^2 \log((H'-h+1)^2/\delta)}{k_h K_k^2 n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)) \\
%  	   &  O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k }{k_h K_k^2 2^{H'}}} \right) = \\
%    	   &  O\left( \frac{K_k}{k} \sqrt{\frac{K_h^2 k }{k_h K_k^2 2^{H'-h}}} \right)
%\end{align*}
%We move to control the sum over $\eps_h$ by bounding the multiple of $K_k/k$. For $k_h \geq \komconstant$
%$$
%\frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} \leq \frac{\komconstant^2 k}{2^{H'-h} \komconstant^2 (2/3)^{H'-h}k} =
%(3/4)^{H'-h}
%$$
%For $k_h \leq \komconstant$ we must have $h \leq H''$ with $(2/3)^{H'-H''} \leq \komconstant/k$, meaning that $2^{H'-H''} \geq k/\komconstant$. For such $h$ we get
%\begin{align*}
% \frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} = &  O\left( \frac{ k_h k}{2^{H'-h} \komconstant^2 } \right) =  \\
%    	   &  O\left( \frac{ k_h k}{2^{H''-h} k\komconstant } \right) = & k_h \leq \komconstant\\
%	   &  O\left( (1/2)^{H''-h}   \right) 
%\end{align*}
%
%If follows that 
%$$ \sum_{h=0}^{H'} \eps_h = O(K_k/k) = O(\komconstant/k) $$
%translating to an overall bound for the error of
%$$ |\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(\komconstant/k)  \right)$$
%The overall memory consumption for the bottom layers is $O(k)$ since all layers of size 2 can be implemented jointly by sampling. The top layers require $O(\log\log(n/\delta)k)$ memory. If follows that by setting $k= O\left(\komconstant \log\log(n/\delta) / \eps\right)$ the error is bounded by $\eps n$ w.p. $1-\delta$ and the overall memory requirement is 
%$$ O\left( \log \log(n/\delta)^2 (\komconstant/\eps)  \right) $$
%
%
%We are now ready for the case where $K_k = \min\{k, \komconstant\sqrt{k}\}$. We set $k_h=2$, resulting in $K_h=2$, for all $h \leq H'$ and obtain
%\begin{align*}
%\eps_h = & O\left( \sqrt{\frac{2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)), \ K_h^2/k_h=O(1) \\
%  	   &  O\left( \sqrt{2^{h-H'}/k} \right) 
%\end{align*}
%and 
%$$ \sum_h \eps_h = O\left(1/\sqrt{k}\right) $$
%Plugging this to the overall error in Equation~\eqref{eq:err total} we get
%$$ \text{Err}(q) = n \cdot O\left( \frac{\log \log(n/\delta)\komconstant + 1}{\sqrt{k}}  \right) =  n \cdot O\left( \frac{\log \log(n/\delta)\komconstant }{\sqrt{k}}  \right) $$
%
%Since layers of $k_h=2$ can all be implemented in constant memory the memory requirement is dominated by the top layers and is in the order of $k \cdot \log\log(n/\delta)$, hence by setting 
%$$k \approx \frac{\komconstant^2 \log^2\log(n/\delta)}{\eps^2}$$ 
%we get a bound of $\eps n$ for the error and a memory usage of 
%$$ O\left( \frac{\komconstant^2 \log^3\log(n/\delta)}{\eps^2}\right) $$
%\end{proof}
%


\section{Appendix Appendix Appendix...}


The proofs of Theorems \ref{thm:streaming11}, \ref{thm:streaming12}, \ref{thm:streaming21}, and \ref{thm:streaming22} all use the basic concept of a compactor. A compactor consumes a stream of items and outputs another stream. 
The output stream contains at most half (in expectation) the items from the input stream with double the weight. 
It does so by keeping a buffer of a certain capacity $m$. When a new item is inserted into the compactor it is added to its buffer. 
If the buffer is full, a compaction operation takes place. 
The compaction takes the elements in the buffer $x_1,\ldots,x_m$ and finds a low discrepancy assignment $\sigma$ such that 
$\max_q |\sum_i \sigma_i f(x_i,q)| = O(mK_m)$. 
Note that such a sequence is guarantied to exist by the definition of the discrepancy. 
However, an algorithm for finding such a sequence might not exist. 
From this point onwards, we assume the existence of such an algorithm.
%In other words, our theorems hold for problems which admit such a solution.
The compactor then outputs either $\{ x_i | \sigma_i = 1\}$ or  $\{ x_i | \sigma_i = -1\}$ with double the weight.

We begin with showing some basic properties of deterministic compactors which always output the smaller of the two sets  $\{ x_i | \sigma_i = 1\}$ or  $\{ x_i | \sigma_i = -1\}$. Consider a single compaction and let $\tilde F_{+}$ denote the function evaluated on $\{ x_i | \sigma_i = 1\}$ (similarly $\tilde F_{-}$).
Let $E(q) = \sum_i \sigma_i f(x_i,q)$ for the signs $\sigma$ computed by the algorithm above. 
$$\tilde F_{+}(q) = \sum_{i ,\; \sigma_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} \sigma_i f(x_i, q) = F(q) + E(q)$$
$$\tilde F_{-}(q) = \sum_{i ,\; \sigma_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} \sigma_i f(x_i, q) = F(q) - E(q)$$
Note that $|\tilde F_{\pm}(q) - F(q)| = |E(q)| \le \max_q |\sum_i \sigma_i f(x_i,q)| = O(mK_m)$. 
Now, aggregating over the $n/m$ compactions we get that $|\tilde F(q) - F(q)| \le n K_m$. Moreover, the stream length is clearly cut (at least) in half.

These facts alone already allow us to prove Theorems~\ref{thm:streaming11} and~\ref{thm:streaming21}. 
The algorithms are a direct extension the well know MRL algorithm \cite{MRL} for quantile sketching. 
Note that for quantiles, $f(x,q) = 1$ if $q > x$ and $0$ else. 
A low discrepancy sequence is achieved simply by sorting the values and assigning $\sigma_i = 1$ for all evenly positioned values in the sorted order and $\sigma_i=-1$ to the odd positions. The above gives a discrepancy of $1/m$ for quantile approximation.
Below we generalize this algorithm to any low discrepancy class.

\paragraph{Theorem~\ref{thm:streaming11}}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/m)$ there exists an fully-mergeable streaming coreset deterministic algorithm of size $O\left(c\log^2(\eps n/c)/\eps\right)$ whose error is at most $\eps n$.
\begin{proof}
Consider feeding the output of the first compactor into a second one etc. Number the compactors $0,\ldots,H$.
The weight of item to compactors $h$ have weight $w_h = 2^h$. The length of the input stream seen by compactor is $n_h \le n/2^h$.
Each compactor contributes at most $w_h n_h K_m \le n K_m$ error. Moreover since $n_h \le n/2^h$ we know that $H \le log(n/m)$.
The total error is therefore $H n K_m \le log(n/m) n K_m$. 
Setting $m \ge c\log(\eps n/c)/\eps$ and replacing $K_m = c/m$ we get that the error is at most $log(n/m) n K_m \le \eps n$ as required. 
Since we have $H \le log(\eps n)$ such compactors the overall space complexity is $O(c\log^2(\eps n/c)/\eps)$. \el{TODO: double check the math.}
\end{proof}

\paragraph{Theorem~\ref{thm:streaming21}}
For any function family $\F$ with a corresponding discrepancy $K_m = O(c/\sqrt{m})$ there exists a fully-mergeable streaming coreset deterministic algorithm of size $O\left(c^2\log^3(\eps^2 n/c) /\eps^2\right)$ whose error is at most $\eps n$. 
\begin{proof}
The proof is identical to the one above except for the variable setting of
Setting $m \ge c^2\log^2(\eps^2 n/c^2)/\eps^2$ and replacing $K_m = c/\sqrt{m}$. 
We get that the error is at most $log(n/m) n K_m \le \eps n$ as required. 
Since we have $H \le log(\eps^2 n)$ such compactors the overall space complexity is $O\left(c^2\log^3(\eps^2 n/c) /\eps^2\right)$. \el{TODO: double check the math.}
\end{proof}

Proving Theorem~\ref{thm:streaming12} and Theorem~\ref{thm:streaming22} is more involved. 
\el{Add the description of the randomized algorithms.}

\section{Proofs for Section~\ref{sec:sketch}, Sketching Coresets} \label{app:sketch proof}

\begin{lemma} \label{lem:compactor}
Assume we have black box access to a solver that for any integer $m$ and inputs $x_1,\ldots,x_m$ obtains signs $\sigma_1,\ldots,\sigma_m$ such that $\max_q \left| \sum_{i=1}^{m} \sigma_i f(x_i, q)\right| \leq K_m$ for some function $f$ whose discrepancy is $K_m$.\footnote{An constant factor approximation algorithm achieving $\max_q \left| \sum_{i=1}^{m} \sigma_i f(x_i, q)\right| = O(K_m)$ would also clearly suffice.} 
Then there exist a streaming algorithm requiring a memory buffer of $m$ input items that given a stream of length $n$ outputs a stream $z_1,\ldots,z_{n'}$ with the following properties
\begin{itemize}
\item $\{z_i\}_i$ is a subset of $\{x_i\}$
\item $\E[n'] = n/2$
\item For $\rho >0$, $\Pr[n' \geq (1+\rho)n/2] \leq \exp \left( -O\left(\frac{n\rho^2}{m}\right)\right)$
\item For any fixed query $q$, the error associated with $q$, defined as
$$\text{Err}(q) = 2\sum_{i=1}^{n'} f(z_i,q) - \sum_{i=1}^n f(x_i,q)  $$
Can be decomposed as a sum
$$\text{Err}(q) = \sum_{j=1}^{n/m} \text{Err}_j(q)$$
where the different $\text{Err}_j(q)$ are independent, have mean 0 and $|\text{Err}_j(q)| \leq K_k$
\end{itemize}
\end{lemma}

Note that signs achieving $max_q \left| \sum_{i=1}^{m} \sigma_i f(x_i, q)\right| \leq K_m$ are guarantied to exist by the definition of the discrepancy. In fact, the quantity above is potentially much smaller than its upper bound because it is computed for a specific set of points $x_i$ as opposed to the worst such possible set. 


\el{changed notation until here...}
\begin{proof}
The algorithm operates as follows. It keeps a buffer of $k$ items. Once the buffer fills with $x_1,\ldots,x_k$ it obtains the signs guaranteeing
$$\max_q \left| \sum_i \sigma_i f(x_i, q)\right| \leq K_k$$
Then, the algorithm output to the stream the with probability $1/2$ each data points $\{x_i \; | \; \sigma_i = 1\}$ or $\{x_i \; | \; \sigma_i = -1\}$ with twice the weight.
$$\sum_{i ,\; \sigma_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} \sigma_i f(x_i, q)$$
$$\sum_{i ,\; \sigma_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} \sigma_i f(x_i, q)$$

%Now, consider the set $P=\{i:\sigma_i=1\}$ and $N=\{i:\sigma_i=-1\}$.
%$$ \sum_i \sigma_i f(x_i, q) = \sum_i \sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in P} f(x_i,q) - \sum_i f(x_i, q)   $$
%$$ -\sum_i \sigma_i f(x_i, q) = \sum_i -\sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in N} f(x_i,q) - \sum_i f(x_i, q)   $$
%Thus, we draw a random coin and either output to the stream the data points of $P$ or $N$. We call this operation a single compaction. 
%\el{this is clunky way to say this...}
The expected output length is $k/2$ for every $k$ inputs, insuring $\E[m]=n/2$. Moreover, the output length is a sum of $n/k$ independent random variables with mean $k/2$ and a maximum value of $k$. Chernoff-Hoeffding's bound can guarantee the stated concentration around $\E[m]$.
As for the error term w.r.t.\ a fixed query $q$, it is clearly the sum of errors accumulated in each compaction. The error incurred in a compaction is either 
$\sum_i \sigma_i f(x_i, q)$ or $-\sum_i \sigma_i f(x_i, q)$ with equal probability. It follows that it is a Radamacher r.v. scaled by a magnitude of at most $K_k$ as required.
\end{proof}

Notice in the above lemma the last property of the streaming algorithm; it ensures that deterministically $|\text{Err}(q)| \leq (K_k/k) n$, and that w.h.p.\ (via Chernoff's inequality) for a fixed $q$ 
$$|\text{Err}(q)| \lesssim K_k \sqrt{n/k} $$
This high probability bound will be used below for our streaming algorithm. 
The high level idea is to have a sequence of streaming objects that we call compactors. 
Each compactor receives an input stream and outputs a stream that is of half the size of the input (in expectation). 
Eventually we get to a stream that is small enough to store in memory. 
The randomized bound above shows that for the compactors that handle a large stream, 
we can afford to have a very crude sketch because the error that they incur is roughly $\sqrt{n}$. 
That is true even if all they do is uniformly sample half of the stream. 
However, for the compactors that observe the shorter streams, a high probability bound is no longer strong enough. 
For those we will make use of the fact that $K_k \ll k$.


\begin{proof} [Proof of Theorems~\ref{thm:streaming} and~\ref{thm:streaming2}]
Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.

The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ and hierarchy level $h$ we analyze the error
$$\text{Err}_h(q) = 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \ .$$
Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. $K_h$ is used to denote the discrepancy of each compaction using a buffer of $k_h$ points. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. The overall error for the query $q$ is the sum over all levels of its errors
$$ \text{Err}(q) = \sum_{h=0}^H \text{Err}_h(q)$$

To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
$$ \text{Err}(q) \leq (K_k/k) \log_2(n/k) n $$

We are however interested in a high probability bound. Specifically we would like to guarantee for $\delta > 0$ that for any query $q$ the error is bounded by $\eps n$ w.p.\ at least $1-\delta$. To this end we consider a different analysis for the top and bottom layers. For the top layers, the stream can be arbitrarily short so we will use the deterministic analysis as above. For the bottom layers, we can guarantee a minimal length to the stream and obtain tighter bounds.

The bottom layers will be those of height $h=0$ up to $h=H'$ where $H'$ is set adaptively in a way that $H'$ is the maximum integer such that for all $h \leq H'$, 
\begin{equation} \label{eq:Htag_nh}
 \forall h \leq H' , \ \ n_h \geq c_1 k \log^2(n) \log(\log(n)/(c_2 \delta ))
\end{equation}
for some sufficiently large constant $c_1$ and small constant $c_2$ to be determined later.

According to Lemma~\ref{lem:compactor} we have for $ h\leq H'$ that for some appropriate constant $c_1$ in  \eqref{eq:Htag_nh},
$$ \Pr[n_{h+1} \geq (1+1/\log(n))n_{h}/2] \leq \exp\left( -\frac{n_{h}}{k_h \log^2(n)}  \right) \leq c_2\delta/  \log(n) $$
We can now union bound over all 
$$h \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta\log(n))) \right)\right) + 2 \leq \log_2(n)$$
and conclude that w.p.\ at least $1-\delta/2$
\begin{equation} \label{eq:Htag}
H' \leq \log_2\left( n / \left( c_1 k \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) + 2 \leq \log_2(n)
\end{equation}
and
\begin{equation} \label{eq:nh small}
\forall h \leq H', \ \ n_{h} \leq 3 \cdot n/2^{h}
\end{equation}


We proceed to bound the error of layer $h$. Since this error is a sum of i.i.d.\ bounded variables of mean zero we can use Chernoff- Hoeffding's inequality and obtain that
$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] =$$
$$ \Pr\left[   \left| 2^h \sum_{j=1}^{\floor{n_h/k_h} } K_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( O\left( -\frac{ \eps^2 n^2}{ 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } K_h^2}   \right)\right) =$$
$$\exp \left( -O\left(\frac{\eps^2 n^2}{n_h 2^{2h} K_h^2/k_h  } \right) \right) 
$$
We now make use of the the bound on $n_h$ in Equation~\eqref{eq:nh small}. Notice that the random events determining the length $n_h$ do not affect the realization of the random variables used in layer $h$, so we can indeed use the bound on $n_h$ for controlling the error at level $h$. We get that since $n_h \leq 3n/2^h$
$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq 
\exp \left( -O\left(\frac{\eps^2 n}{ 2^h K_h^2/k_h  } \right) \right) 
$$
In particular, for $\eps_h = O\left( \sqrt{\frac{2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}} \right)$  it holds that
$$ \Pr\left[   \left| \text{Err}_h(q) \right| \geq \eps n \right] \leq \delta/4(H'-h+1)^2$$
and a union bound ensures that with probability at least $1-\delta$, in addition to Equations~\eqref{eq:Htag} and~\eqref{eq:nh small} we have for all $h \leq H'$ that
$$\left| \text{Err}_h(q) \right| \leq \eps_h n$$
and 
$$\left| \sum_{h=0}^{H'} \text{Err}_h(q) \right| \leq \left(\sum_{h=0}^{H'} \eps_h\right)n $$

We proceed to bound the error on the top layers $H'+1$ to $H$. In these top layers we use a deterministic algorithm guaranteeing the stream is cut in half from layer to layer. We assign all top layers a budget of $k_h=k$ for the compactors. Since $2^{H+1} < n/k$ this gives a bound of 
$$
H-H' \leq \log_2(n/k2^{H'}) = O\left( \log\left( c_1 \log^2(n) \log(1/(c_2 \delta \log(n))) \right)\right) = 
$$
$$
O\left( \log \log(n/\delta) \right)
$$
leading to a bound of 
$$ \left| \sum_{h=H'+1}^H \text{Err}_h(q) \right| \leq O\left( \log \log(n/\delta) (K_k/k) n \right) $$
and an overall bound of
\begin{equation} \label{eq:err total}
|\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(K_k/k) + \sum_{h=0}^{H'} \eps_h \right)
\end{equation}

We are now left with the task of defining $k_h$ for the lower layers; we do so differently depending on the value of $K_k$ as a function of $k$. Recall that we aim to deal with two settings. In the first $K_k=\min\{\komconstant,k\}$ and in the second $K_k=\min\{\komconstant \sqrt{k}, k\}$. In both cases  $\komconstant$ is independent of $k$. We start by dealing with the first scenario.

We set $k_h = \max\{2, \ceil{(2/3)^{H'-h}k}\}$. By using the definition of $H'$ in Equation~\eqref{eq:Htag} we see that
\begin{align*}
\eps_h = & O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k^2 \log((H'-h+1)^2/\delta)}{k_h K_k^2 n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)) \\
  	   &  O\left( \frac{K_k}{k} \sqrt{\frac{2^h K_h^2 k }{k_h K_k^2 2^{H'}}} \right) = \\
    	   &  O\left( \frac{K_k}{k} \sqrt{\frac{K_h^2 k }{k_h K_k^2 2^{H'-h}}} \right)
\end{align*}
We move to control the sum over $\eps_h$ by bounding the multiple of $K_k/k$. For $k_h \geq \komconstant$
$$
\frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} \leq \frac{\komconstant^2 k}{2^{H'-h} \komconstant^2 (2/3)^{H'-h}k} =
(3/4)^{H'-h}
$$
For $k_h \leq \komconstant$ we must have $h \leq H''$ with $(2/3)^{H'-H''} \leq \komconstant/k$, meaning that $2^{H'-H''} \geq k/\komconstant$. For such $h$ we get
\begin{align*}
 \frac{K_h^2 k}{2^{H'-h} K_k^2 k_{h}} = &  O\left( \frac{ k_h k}{2^{H'-h} \komconstant^2 } \right) =  \\
    	   &  O\left( \frac{ k_h k}{2^{H''-h} k\komconstant } \right) = & k_h \leq \komconstant\\
	   &  O\left( (1/2)^{H''-h}   \right) 
\end{align*}

If follows that 
$$ \sum_{h=0}^{H'} \eps_h = O(K_k/k) = O(\komconstant/k) $$
translating to an overall bound for the error of
$$ |\text{Err}(q)| \leq n \cdot O\left( \log \log(n/\delta)(\komconstant/k)  \right)$$
The overall memory consumption for the bottom layers is $O(k)$ since all layers of size 2 can be implemented jointly by sampling. The top layers require $O(\log\log(n/\delta)k)$ memory. If follows that by setting $k= O\left(\komconstant \log\log(n/\delta) / \eps\right)$ the error is bounded by $\eps n$ w.p. $1-\delta$ and the overall memory requirement is 
$$ O\left( \log^2 \log(n/\delta) (\komconstant/\eps)  \right) $$


We are now ready for the case where $K_k = \min\{k, \komconstant\sqrt{k}\}$. We set $k_h=2$, resulting in $K_h=2$, for all $h \leq H'$ and obtain
\begin{align*}
\eps_h = & O\left( \sqrt{\frac{2^h K_h^2 \log((H'-h+1)^2/\delta)}{k_h n}} \right) = & n=\Omega(2^{H'}k\log(H'/\delta)), \ K_h^2/k_h=O(1) \\
  	   &  O\left( \sqrt{2^{h-H'}/k} \right) 
\end{align*}
and 
$$ \sum_h \eps_h = O\left(1/\sqrt{k}\right) $$
Plugging this to the overall error in Equation~\eqref{eq:err total} we get
$$ \text{Err}(q) = n \cdot O\left( \frac{\log \log(n/\delta)\komconstant + 1}{\sqrt{k}}  \right) =  n \cdot O\left( \frac{\log \log(n/\delta)\komconstant }{\sqrt{k}}  \right) $$

Since layers of $k_h=2$ can all be implemented in constant memory the memory requirement is dominated by the top layers and is in the order of $k \cdot \log\log(n/\delta)$, hence by setting 
$$k \approx \frac{\komconstant^2 \log^2\log(n/\delta)}{\eps^2}$$ 
we get a bound of $\eps n$ for the error and a memory usage of 
$$ O\left( \frac{\komconstant^2 \log^3\log(n/\delta)}{\eps^2}\right) $$
\end{proof}


\end{document}
