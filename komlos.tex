\documentclass[anon,12pt]{colt2019} % Anonymized submission
% \documentclass[12pt]{colt2019} % Include author names

% The following packages will be automatically loaded:
% amsmath, amssymb, natbib, graphicx, url, algorithm2e

\title[Komols Complexity, Coresets, and Sketches in Machine Learning]{Komols Complexity, Coresets, and Sketches in Machine Learning}
\usepackage{times}
% Use \Name{Author Name} to specify the name.
% If the surname contains spaces, enclose the surname
% in braces, e.g. \Name{John {Smith Jones}} similarly
% if the name has a "von" part, e.g \Name{Jane {de Winter}}.
% If the first letter in the forenames is a diacritic
% enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

% Two authors with the same address
% \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
%  \Name{Author Name2} \Email{xyz@sample.com}\\
%  \addr Address}

% Three or more authors with the same address:
% \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
%  \Name{Author Name2} \Email{an2@sample.com}\\
%  \Name{Author Name3} \Email{an3@sample.com}\\
%  \addr Address}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{ amssymb }

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}
%\newtheorem*{definition*}{Definition}
%\newtheorem{definition}{Definition}
%\newtheorem{claim}{Claim}
%\newtheorem{theorem}{Theorem}


\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
\newcommand{\F}{\mathcal{F}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Q}{\mathcal{Q}}
%\usepackage{ntheorem}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}
\newcommand{\disc}{\text{disc}}





% Authors with different addresses:
\coltauthor{%
 \Name{Zohar Karnin} \Email{zkarnin@amazon.com}%\\ \addr Address 1
 \AND
 \Name{Edo Liberty} \Email{libertye@amazon.com}%\\ \addr Address 2%
}


\begin{document}

\maketitle

\begin{abstract}
This paper presents the Komlos complexity for families of functions.
We argue that this measure of complexity captures properties like minimal coreset sizes, streaming mergable sketches, and batch active learning.  
We show that for many well known problems the Komlos complexity is asymptotically lower than their Rademacher complexity.
This proves the existence smaller coresets than those achievable by random sampling. 
These include frequent item counting, quantile approximation, kernel density estimation, matrix covariance approximation, and linear and logistic regression.
For all of the above we match the state of the art or improve on it with the above generic framework.
This makes an explicit connection between low discrepancy, streaming algorithms, coresets, and learnability. 
We also asymptotically improve on the generic merge-reduce framework for creating mergeable sketches for all the above problems. 
\end{abstract}


\section{Introduction}

In many machine learning and sketching problems our goal is often to approximate the sum (or expectation) of functions.
Specifically, function of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i \in \mathcal X$ are either training examples or stream items and $q \in \mathcal Q$ is either the model parameters or a query from a possible set $\Q$ which is known in advance. For simplicity and throughout this manuscript we will assume $f:\X\times\Q\rightarrow [0,1]$.
The goal is to produce weights $w$ with at most $k \ll n$ non-zero such that $\tilde F(q) = \sum_{i=1}^{n}w_i f(x_i,q)$ approximates $F(q)$.
Approximation here means that $|\tilde F(q)  - F(q)| \le \eps n$ either for all possible queries $q \in \mathcal Q$ simultaneously or for every fixed $q$ with probability at least $1-\delta$. 
There are more complicated formulations such as weak coresets which we will not touch upon in this manuscript.

Generating a concise representation for $\tilde F$ allows one to optimize over $\tilde F$ instead of $F$ which is more efficient. 
Moreover, if the resulting sketches or coresets are mergable, this could be done on separate machines without the need for communication or assuming randomness in the partitioning.

%Here are some examples:
%\begin{itemize}
%\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else. Here both $x$ and $q$ belong to some finite domain.
%\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else. Here, $x$ and $q$ belong to a set which exhibits total ordering.
%\item in matrix column subset selection we have $f(x, q) = \langle x,q \rangle ^2$ where $q$ is any unit vector in $\R^d$ and $x$ are matrix rows. For simplicity, we will assume in this manuscript that $x$ is also unit norm.
%\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
%\item in logistic regression $f(x, q) = \operatorname{loss}(x,q) = \log(1 + \exp(-q^T x))$. 
%Here $q$ is the parameters of the linear model and $x$ is a training example vector multiplied by its label ($1$ or $-1$).
%\item Linear Classification $f(x,q) = 1$ if $x^Tq > 0$ and $0$ else. Here, like in Logistic Regression, $x$ is the example point multiplied by its label and $q$ is the weights of the linear classifier.
%\item Fractional Satisfiability $f(x,q)$ if $q$ satisfies $f$ and $0$ otherwise. Here $x$ in a clause in conjunctive normal form and $q$ is a boolean value assignment to the variables. For example $f(x,q) = 1$ iff $(q_1 \vee q_5  \vee \neg q_8)$ is true.
%\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q = \sum_{i,j\in E} (q_i - q_i)^2$ where $x$ is an edge and $L_x$ is the graph laplacian corresponding to the graph with a single edge $x$ and $q$ is a test vector.
%\item Probably also good for matrix approximation when we get entry updates. 
%\end{itemize}



\newcommand{\ccc}[1]{\parbox}
\begin{table}[htp]
\begin{center}
\begin{tabular}{|c|c|c|c|} \hline
Problem				& $f$ 											& $R_m$ 			& $K_m$ \\ \hline
Counting 				& $f(x, q) = 1$ if $x=q$ and zero else 					& $1/\sqrt{m}$		& $1/m$ \\ \hline
Quantiles				& $f(x, q) = 1$ if $x\le q$ and zero else  					& $1/\sqrt{m}$		& $1/m$ \\ \hline
Column Selection		& $f(x, q) = \langle x,q \rangle ^2$ where  $q,x\in \R^d$		& $1/\sqrt{m}$		& $\sqrt{d}/m$ \\ \hline
Gaussian Kernel Density 			& $f(x, q) = \exp(- \|x-y\|_2^2/\lambda^2)$ where  $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline
Laplacian Kernel Density 			& $f(x, q) = \exp(- \|x-y\|_2/\lambda)$ where  $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline
Positive Kernel Density 			& $f(x, q) = k(x,q)$ for any positive kernel $k$ and $q,x\in \R^d$				& ? 		& $1/\sqrt{m}$ \\ \hline

%Logistic Regression     	& $f(x, q) = \log(1 + \exp(q^T x))$ where  $q,x\in \R^d$ 						& $1/\sqrt{m}$ 		& $\sqrt{d}/m$ \\ \hline
%Hinge Regression     		& $f(x, q) = [1-q^T x]_+$ where  $q,x\in \R^d$							& $1/\sqrt{m}$ 		& $\sqrt{d}/m$ \\ \hline
Linear Regression		& $f(x,q) = (z^Tq - y)^2$ where $x = (z\in\R^d,y\in \R)$ and $q\in \R^d$					& ?				& ? \\ \hline
Linear Classification		& $f(x,q) = 1$ if $x^Tq > 0$ and $0$ else. $q,x\in \R^d$					& ?				& ? \\ \hline
Graph Laplacian 		& $f(x, q) = w_{ij}(q_i - q_i)^2$ for $x = (w_{ij},i,j)$, $q \in \R^n$ & ?				& ? \\ \hline
Matrix Approximation  	& $f(x, q) = a_{ij}u_iv_j$ for $x = (a_{ij},i,j)$  and $q = (u,v)$ 	& ?				& ? \\ \hline
\end{tabular}
\caption{Example problems in machine learning and sketching that fall into this framework.}
\end{center}
\label{default}
\end{table}%





For bounded functions $f$ like above, uniform sampling of $\ell = O(\log(1/\delta)/\eps^2)$ combined with a union bound over $|\mathcal Q|$ always provides a valid solution using $\ell = O(\log(|\mathcal Q|)/\eps^2)$ stream items. 
While $|\mathcal Q|$ is often infinite it reduces to a finite (albeit usually exponentially large) set through standard epsilon net arguments. 
%This is common practice in machine learning, specifically, PAC learning. However, it is often far from being the smallest cardinality coreset possible.
We argue that we can offer a solution for producing smaller coresets for all of the above problems in a unified manner. 
Moreover, our solution creates streaming algorithms with fully mergeable sketches. 
The size of the coreset above appears to be intimately tied to the discrepancy properties of the associated functions.
This is captured by its Komlos complexity.  


\section{Komlos Complexity}
We begin by formally defining the Komols complexity and by giving some general results about for problems with low Komols complexity. The Komols complexity of a sequence set is defined similarly to it Rademacher complexity.
%\begin{definition}
%Let $A \subset [0,1]^m$ and $\sigma \in \{-1,1\}^m$ uniformly. 
%The Rademacher complexity of $\F$ is 
%\[
%R_m(\F) =  \frac{1}{m}\E_{\sigma} \sup_{a \in A}  \sum_{i=1}^{m}\sigma_i a_i
%\]
%\end{definition}
%
%\begin{definition}
%Let $A \subset [0,1]^m$ and $\sigma \in \{-1,1\}^m$. 
%The Komlos complexity of $A$ is 
%\[
%K_m(A) =  \frac{1}{m}\min_{\sigma} \sup_{a \in A}  \left|\sum_{i=1}^{m}\sigma_i a_i\right|
%\]
%\end{definition}

\begin{definition}
Let $\F$ be a family of functions $f:\X\rightarrow\R$ and $\sigma \in \{-1,1\}^m$ uniformly. 
The Rademacher complexity of $\F$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
R_m(\F) =  \frac{1}{m}\E_{\sigma} \sup_{f \in \F}  \sum_{i=1}^{m}\sigma_i f(x_i)
\]
\end{definition}
%
\begin{definition}
Let $\F$ be a family of functions $f:\X\rightarrow\R$ and $\sigma \in \{-1,1\}^m$. 
The Komlos complexity of $\F$ w.r.t.\ $\{x_1,\ldots,x_m\} \subset \X$ is 
\[
K_m(\F) =  \frac{1}{m}\min_{\sigma} \sup_{f \in \F}  \left|\sum_{i=1}^{m}\sigma_i f(x_i)\right|
\]
The Komlos Complexity $K_m(\F)$ without a reference set $\{x_1,\ldots,x_m\}$ is the upper bound on any such set.
\end{definition}



\noindent It well known that $R_m$ is a good measure for generalization when data is chosen uniformly at random (or drawn from an unknown distribution). See \cite{Bartlett:2003:RGC:944919.944944} for other notions of complexity and relationships between them. We claim that $K_m$ is the measure more suitable for active learning, for creating coresets, and for creating mergeable sketches.

\el{Add a much longer explanation about the connection to discrepancy and the ideas behind this construction} The name Komlos Complexity is due to the Komlos Conjecture. It claims that $\min_\sigma \max_f \|\sum_i \sigma_i f_i\|_\infty = O(1)$ for unit vectors $f$. The correctness of the Komlos Conjecture is still a fundamental open problem in the study of discrepancy theory.  The definition is useful, however, regardless of the correctness or provability of the conjecture.  
It is obvious that  $K_m(\F) \le R_m(\F)$ for any family $\F$. We show later that $K_m(\F) = o(R_m(\F))$ for a wide range of interesting problems in machine learning.


%\begin{theorem}
%Let $F(a) = \sum_i a_i$ for $a \in A$ and let $K_m(A) = O(c/m^\gamma)$ for any constant $\gamma \in (0,1]$.
%There exist a coreset for $A$ such that $\tilde{F}(a) = \sum_{i} w_i a_i$, $\operatorname{supp}(w) \le m$ and 
%$$
%\forall \; a \in A \;\;\;  |F(a) - \tilde F(a)| \le \eps n
%$$
%\end{theorem}
%\begin{proof}
%By halving the points repeatedly \el{TODO}.
%\end{proof}
\subsection{General Framework for Coresets}
Consider the function $F = \sum_{i=1}^{m} f(x_i,q)$ and the signed sum of error function $E(q) = \sum_{i=1}^{m} \sigma_i f(x_i,q)$ where $\sigma_i \in \{-1,1\}$.
Now consider $F_1(q) = F(q) + E(q) = \sum_{i=1}^{m} f(x_i,q)  + \sum_{i=1}^{m} \sigma_i f(x_i,q)  = 2 \sum_{i | \sigma_i=1} f(x_i,q)$ and similarly $F_{-1}$. We have that both $F_1(q), F_{-1}(q)$ are approximations for $F(q)$ obtained by coresets of item weights of $2$ and an error at most $|F_1(q) - F(q)| =  |E(q)|$.
%
The above is true for any choice of signs $\sigma_i$, specifically, for those minimizing $\max_q | E(q)|$.
Note that we can select signs such that $|E(q)| \le m K_m(\F)$.
A function is sketch-able if $K_m(\F) = O(m^\gamma)$ for any constant $\gamma \in [0,1)$.
Typically we see that $K_m(\F) = O(c/m)$ or $K_m(\F)= c/\sqrt{m}$ for some $c$ which does not depend on $m$.

\begin{fact}
For any function family $\F$ which a corresponding Komlos Complexity $K_m(\F) = O(c/m)$ there exists a coreset of size 
$O(c/\eps)$ whose error is at most $\eps n$.
\end{fact}
\begin{fact}
For any function family $\F$ which a corresponding Komlos Complexity $K_m(\F) = O(c/\sqrt{m})$ there exists a coreset of size 
$O(c/\eps^2)$ whose error is at most $\eps n$.
\end{fact}

\noindent Proving both facts is trivial. 
At step one, create a coreset of size at most $n/2$ of items of weight $2$ and incur an error of at most $c$ or $c \sqrt{n}$.
Then, we repeat the process and create a coreset of size $n/4$ of items of weight $4$. Here, you incur error of $2c$ or $2c\sqrt{n/2}  = \sqrt{2n}$.
Note that the sum of errors is asymptotically dominated by last iterations. 
Halting the compression at $\Theta(c/\eps)$ or $\Theta(c/\eps^2)$ items respectively achieves the goal.


%\begin{theorem}
%Let $A$ be of the special form $a_i = f(x_i,q) \in [0,1]$ and $K_m(A) = O(c/\sqrt{m})$.
%There exist and streaming mergeable coreset for $A$ with cardinality at most $\ell = O(c\log^{3}\log(1/\delta)/\eps^2)$.
%That is, let $F(q) = \sum_i f(x_i,q)$. One can process a stream of item $x_1,\ldots,x_n$ and produce $\tilde{F}(q) = \sum_{i} w_i a_i$ with $\operatorname{supp}(w) \le \ell$ such that 
%$$
%\forall \; q \in Q \text{\;\;with probability\;\;} 1-\delta  \;\;\;  |F(q) - \tilde F(q)| \le \eps n
%$$
%\end{theorem}
%
%
%\begin{theorem}
%Let $A$ be of the special form $a_i = f(x_i,q) \in [0,1]$ and $K_m(A) = O(c/m)$.
%There exist and streaming mergeable coreset for $A$ with cardinality at most $\ell = O(c\log^{2}\log(1/\delta)/\eps)$.
%That is, let $F(q) = \sum_i f(x_i,q)$. One can process a stream of item $x_1,\ldots,x_n$ and produce $\tilde{F}(q) = \sum_{i} w_i a_i$ with $\operatorname{supp}(w) \le \ell$ such that 
%$$
%\forall \; q \in Q \text{\;\;with probability\;\;} 1-\delta  \;\;\;  |F(q) - \tilde F(q)| \le \eps n
%$$
%\end{theorem}



\el{I believe that the correctness of the Komlos conjecture implies equivalence between coresets limited to the input and general coresets. Specifically, a constructive positive resolution will give an algorithm for converting any coresets to signs $\sigma$ and select inputs from a sequence.}

\subsection{General Framework for Sketching}


The high level idea is the following. 
Consider a stream of the data points $x_i$. 
We maintain a buffer that can hold up to $m$ points.
Once collecting $m$ data points it performs a \emph{compact} operation that outputs at most $m/2$ data points of weight $2$ representing the original $m$ inputs. The compact operation partitions the $m$ points into two sets whose corresponding sums are very close. 
This is made formal with the following definitions: 

 %
In what follows, we prove the following.
\begin{theorem} \label{thm:streaming}
For any function $f$ with a corresponding Komlos Complexity $K_m = O(c/m)$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(c\log^2\left(\min(n, \log(1/\delta)) \right)/\eps\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require a priori knowledge of the stream length $n$.
\end{theorem}


\begin{theorem}
For any function $f$ with a corresponding Komlos Complexity of $K_m = O(c/\sqrt{m})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(c\log^3\left(\min(n, \log(1/\delta)) \right)/\eps^2\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require a priori knowledge of the stream length $n$.
\end{theorem}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
%$O(\log^2(\log(1/\delta))/\eps^2)$ whose error is at most $\eps n$ and failure probability at most $\delta$.
%\end{fact}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(1)$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps)$ whose error is at most $\eps n$.
%\end{fact}
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps^2)$ whose error is at most $\eps n$.
%\end{fact}

\begin{lemma} \label{lem:compactor}
Let $m$ be an integer and assume we have black box access to a solver that for any $m$ inputs $x_1,\ldots,x_m$ obtains $m$ signs $\sigma_1,\ldots,\sigma_m$ such that
$$\max_q \left| \sum_{i=1}^{m} \sigma_i f(x_i, q)\right| \in O(K_m)$$
Note that such signs are guarantied to exist by the definition of the Komols Complexity. 
In fact, the quantity above is likely much smaller than $K_m$ because it is computed for a specific set of points $x_i$ and the worst such possible set.


Then there exist a streaming algorithm requiring a memory buffer of $k$ input items that given a stream of length $n$ outputs a stream $z_1,\ldots,z_m$ with the following properties
\begin{itemize}
\item $\{z_i\}_i$ is a subset of $\{x_i\}$
\item $\E[m] = n/2$
\item For any fixed query $q$, the error associated with $q$, defined as
$$\text{Err}(q) = 2\sum_{i=1}^m f(z_i,q) - \sum_{i=1}^n f(x_i,q)  $$
Can be decomposed as a sum
$$\text{Err}(q) = \sum_{j=1}^{n/k} \text{Err}_j(q)$$
where the different $\text{Err}_j(q)$ are independent, have mean 0 and $|\text{Err}_j(q)| \leq \rho k$
\end{itemize}
The last property ensures that deterministically $|\text{Err}(q)| \leq \rho n$, and that w.h.p.\ (via Chernoff's inequality) for a fixed $q$ 
$$|\text{Err}(q)| \lesssim \rho k\sqrt{n/k} = \rho \sqrt{kn}$$
\end{lemma}
\begin{proof}
The algorithm operates as follows. It keeps a buffer of $k$ items. Once the buffer fills with $x_1,\ldots,x_k$ it obtains the signs guaranteeing
$$\max_q \left| \sum_i \sigma_i f(x_i, q)\right| \leq \rho k$$
Then, the algorithm output to the stream the with probability $1/2$ each data points $\{x_i \; | \; \sigma_i = 1\}$ or $\{x_i \; | \; \sigma_i = -1\}$ with twice the weight.
$$\sum_{i ,\; \sigma_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} \sigma_i f(x_i, q)$$
$$\sum_{i ,\; \sigma_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} \sigma_i f(x_i, q)$$

%Now, consider the set $P=\{i:\sigma_i=1\}$ and $N=\{i:\sigma_i=-1\}$.
%$$ \sum_i \sigma_i f(x_i, q) = \sum_i \sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in P} f(x_i,q) - \sum_i f(x_i, q)   $$
%$$ -\sum_i \sigma_i f(x_i, q) = \sum_i -\sigma_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in N} f(x_i,q) - \sum_i f(x_i, q)   $$
%Thus, we draw a random coin and either output to the stream the data points of $P$ or $N$. We call this operation a single compaction. 
%\el{this is clunky way to say this...}
The expected output length is $k/2$ for every $k$ inputs, insuring $\E[m]=n/2$. 
As for the error term w.r.t.\ a fixed query $q$, it is clearly the sum of errors accumulated in each compaction. The error incurred in a compaction is either 
$\sum_i \sigma_i f(x_i, q)$ or $-\sum_i \sigma_i f(x_i, q)$ with equal probability. It follows that it is a Bernoulli r.v. with magnitude of at most $\rho k$ as required.
\end{proof}


\begin{proof} [Proof of Theorem~\ref{thm:streaming}]
Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.

The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ we analyze the error
$$\text{Err}(q) = \sum_{h=0}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \ .$$
Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. 

To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
$$ \text{Err}(q) \leq \rho(k) \log_2(n/k) n $$

If however, we are interested in a high probability bound of $1-\delta$, we decompose the error to two components
$$\text{Err}(q) = \sum_{h=0}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho k_h Y_{ij} + \sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}$$
 Here, $H' = H - \ceil{\log(c \log(1/ \delta))}$ where $c$ is some universal constant we set later. Notice that the right summand contains roughly $\log(1/\delta)$ random variables. Since they are all Bernoulli we get that any combination of them is possible with probability $\geq \delta$ hence we must use a deterministic bound for these upper layers. We thus set $k_h=k$ for $h > H'$ and use the bound
$$ \text{Err}^{\text{top}}(q) = \left|\sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}\right| \leq O\left( \rho(k) \left(H-H'\right) n \right) =  O\left( \rho(k) \log \log(1/ \delta) n \right) $$
Now, for the bottom layers we use Chernoffs bound
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h)^2 k_h^2}   \right) =$$
$$\exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} } \right) $$

\zk{$n_h$ being random is a problem. The weight of the stream might increase with time. The analysis now works for a completely balanced case }
Consider the sum in the denominator. 
$$ \sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} = O(\sum_{h=0}^{H'} 2^{2h} \rho(k_h)^2 k_h n_h ) $$
Since $n_h \leq n_{h-1}/2$ and we have $k_h \leq k$ we have 
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2  ) $$
Now, if $\rho(k_h) \geq 1/\sqrt{k_h}$ we set $k_h = 2$ for all $h \leq H'$ and obtain
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h  ) = O(n^2/ (k \log(1/\delta))) $$
and we get
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) \right)   \right) $$
Hence, by setting $k \geq c/\eps^2$ for sufficiently large constant $c$ the probability is upper bounded by $\delta$.

Now, for $\rho(k) < 1/\sqrt{k}$ we have $\eta(k) = \rho(k)^2 k = o(1)$. We can exploit that to allow for $k = o(1/\eps^2)$. For $\eta(k)=O(k^{-p})$ we\footnote{We do not discuss other scenarios as we are not aware of any setting where they occur. Also, extending the analysis to them is a purely technical exercise} use $k_h = \max(2, \ceil{(2/3)^{H'-h}k})$, leading to a bound of
$$\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2 \leq \sum_{h=0}^{H'} 2^h \eta(k) \cdot (3/2)^{H'-h} = (3/2)^{H'} \eta(k) \sum_{h=0}^{H'} (4/3)^h \leq  $$
$$(3/2)^{H'} \eta(k) \cdot (4/3)^{H'} \cdot 3 = 3 \cdot 2^{H'} \eta(k) = O(\eta(k)n/(k\log(1/\delta))$$
Plugging this in the probability bound gives
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) / \eta(k) \right)   \right) = $$
$$ \exp \left( -O\left( \eps^2 \log(1/\delta) / \rho(k)^2 \right)   \right) $$
Hence, setting $k$ large enough so that $\rho(k) \leq c\eps$ for some universal constant $c$ leads to the required bound.

In particular for $\rho(k) = O(1/k)$ we see that $k=1/\eps$ suffices, for general $p>1/2$ with $\rho(k) = k^{-p}$ we require $k=\eps^{-1/p}$.


Now, consider the overall error and memory of the entire process. For the bottom layers, for any $\rho(k)$ we require $O(k+H')=O(k+\log(n))$ memory. A closer inspection actually shows that once $k_h=2$ we are simply performing uniform sampling, and all of those layers can be done with constant memory. Hence, the memory footprint of the bottom layers is $O(k)$. The top layers require $O(\log\log(1/\delta)k)$ memory leading to a total memory usage of $O(\log\log(1/\delta)k)$.

The error for the bottom layers is $O(\min\{\rho(k), 1/\sqrt{k}\} n)$. The error of the top layers is $O(\log\log(1/\delta) \rho(k) n)$, meaning that the overall error is $O(\log\log(1/\delta) \rho(k) n)$.

Hence, if we aim for an error of $\eps n$ w.p.\ $1-\delta$ we set $k$ large enough such that $\rho(k) \leq c \eps / \log\log(1/\delta)$. Denoting by $\rho^{\dagger}$ the inverse function to $\rho$ this leads to a total memory requirement of 
$$ \rho^{\dagger}(c \eps / \log\log(1/\delta)) \log\log(1/\delta) $$

\end{proof}




\section{Connections to streaming Counting and Quantiles}






\section{Kernel Density Estimation}

Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $\rho: \R^d \rightarrow \R$ of a point $y$ is defined as 
$$ \rho(y) = \sum_{i=1}^{n} K(x_i,y) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include

$$ K(x,y) = \exp(- \|x-y\|_2^2/\lambda^2)\;\;\; K(x,y) = \exp(- \|x-y\|/\lambda) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-y\|/_2^2/\lambda^2)^{-1}$$

where $\lambda$ is a scaling parameter. What we discuss in what follows applies for any positive definite kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.


Using our framework, we need to bound $K_m = \frac{1}{m}\sum_{i=1}^m \sigma_i K(x_i,y)$. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^m \sigma_i K(x_i,y) = \sum_{i=1}^m \sigma_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^m \sigma_i \phi(x_i), \phi(y)} \leq \|\phi(y)\| \cdot \left\|  \sum_{i=1}^m \sigma_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^m \sigma_i \phi(x_i) \right\| $$
This upper bound allows us to find values for $\sigma_i$ which bound $K_n$ using a simple algorithm.
Given $m$ vectors $z_1,\ldots,z_m$ in an inner product space, compute signs minimizing $\| \sum_i \sigma_i z_i \|$.
We first notice that for i.i.d.\ uniform signs, it holds that
$$\E[\| \sum_i \sigma_i z_i \|^2] = \E[\sum_{i,j} \sigma_i \sigma_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. By Markov with probability at least $3/4$ we have $\| \sum_i \sigma_i z_i \| \le 2\sqrt{\sum_i \|z_i\|^2}$.
%
We can also use the method of expectation minimization to achieve this bound deterministically.
Specifically, we will guaranty that $\| \sum_{j=1}^i \sigma_j z_j \|^2 \le \sum_{j=1}^i \|z_j\|^2$ for all $i$.
This is trivially true for $i=1$. 
For another value $i$ set $\sigma_i = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j \langle z_j, z_i \rangle)$ 
and  assume by induction that $\| \sum_{j=1}^{i-1} \sigma_j z_j\|^2 \le \sum_{j=1}^{i-1} \|z_j\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}\sigma_j z_j\|^2 &=& \|\sum_{j=1}^{i-1}\sigma_j z_j\|^2 + \|z_i\|^2 + 2\langle \sum_{j=1}^{i-1}\sigma_j z_j, \sigma_i z_i\rangle \\
&\le& \sum_{j=1}^{i-1} \|z_j\|^2 + \|z_i\|^2 + 2\sigma_i \sum_{j=1}^{i-1}\sigma_j \langle  z_j,  z_i\rangle \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|z_j\|^2 - 2|\sum_{j=1}^{i-1}\sigma_j \langle  z_j,  z_i\rangle| \le \sum_{j=1}^{i} \|z_j\|^2
\end{eqnarray*}
This completes the proof that $\| \sum_{j=1}^{k}\sigma_j z_j\| \le \sqrt{\sum_{j=1}^{i} \|z_j\|^2}$ for our choice of values for $\sigma$. 
%
Let us now translate this to an algorithm w.r.t.\ the $d$ dimensional points $x_1,\ldots, x_k$. 
The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ \sigma_i = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j \langle \phi(x_j), \phi(x_i) \rangle) = -\operatorname{sign} (\sum_{j=1}^{i-1}\sigma_j  K(x_j, x_i))$$
This guarantees that 
$$ \forall y \in \R^d \;\;\; \sum \sigma_i K(x_i, y) \leq \sqrt{ \sum \|\phi(x_i)\|^2 } = \sqrt{m} $$

Using the framework above this provides a streaming coreset construction for kernel density estimation of size $O(1/\eps^2)$ such that 
$\forall \;q\;\; |\tilde \rho(q) - \rho(q)| \le \eps n$.
This matches the results achieved by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751}.
\el{The running time of this algorithm is amortized.... I hope it will be better than Jeff's result, especially in the randomized shrinking buffers setting.}
\zk{We actually ended up using the Radamacher complexity, at least for the bound. Namely, we used random signs. Perhaps there is a kernel for which it is easy to prove something better? One that comes to mind is apx low rank data. By low rank I mean in the kernel space, meaning the $z_i$'s. To find the core-set we can first run PCA on the $z_i$'s, reduce their dimension from $n$ to being the minimal such that we suffer a loss of $\eps$. Then, we use theory on the sum of unit vectors of low dimension giving $\sqrt{k}/\eps$ where $k$ is the dimension we got. In the worst case if the kernel matrix is isotropic we will get $k \approx n$ and gain nothing, but this way we can squeeze something from the data that wasn't squeezed before.. We can take this further and look into the details of the proof that we can balance unit vectors to have a norm of $\sqrt{d}$ regardless of the number of vectors. The key in that proof is that by considering continuous $s_i$'s we can have the sum equal to zero while keeping only $d$ points with non $\{-1,1\}$ $s_i$'s. Consider a split of top $k$ and bottom rest directions. If we restrict the $s_i$'s to only have the sum to be zero in the top $k$ we get $k$ non integral $s_i$'s but possibly horrible performance on the bottom $k$. What if only restrict $d-2k$ and not $d-k$ points? Then we get some freedom in choosing the integer $s_i$'s. The bound on the bottom singular values can become smaller than the trivial one.}







\section{Row (or Column) Subset Selection}
Assume you are getting the rows $x_i$  of matrix $X$ in a stream and you want to compute $Z$ such that $\|Z^TZ - X^TX\| \le \eps \operatorname{Tr}(X^TX)$.
For simplicity, assume $x_i$ are all unit length so $\eps \operatorname{Tr}(X^TX) = \eps n$. Moreover, assume each vector in $Z$ must be one of the rows in $X$ up to some constant factor.
This is called the row subset selection problem. 
Since you are trying to minimize the quadratic form, you get that $f(x, q) = \langle x,q \rangle ^2$ and 
$$E(q) = \sum_i \sigma_i \langle x_i,q \rangle ^2 = q^T (\sum_i \sigma_i x_i x_i^T ) q \le \|\sum_i \sigma_i x_i x_i^T\| \le c\sqrt{d}$$

\noindent The last inequality is a direct application of the following Theorem.
\begin{theorem}\label{BansalInDaHouz}
For any set of matrices $X_1,...,X_n$ in $\R^{d_1 \times d_2}$ such that $\|X_i\|_{F} \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i \sigma_i X_i||_{2} \le O(\sqrt{d_1 + d_2})$.
\end{theorem}
\begin{proof}
First, recount Banaszczyk's theorem. 
Let $\mathcal K$ be a convex body in $\R^d$ with Gaussian measure at least 1/2 ($\Pr[g \in \mathcal K] \ge 1/2$ where $g$ is i.i.d.\ Gaussian).
Let $x_1,\ldots,x_n$ be vectors in $\R^d$ with $\|x_i\| \le 1$. 
Then, there exist signs $\sigma_i$ such that $\sum \sigma_i x_i \in C \mathcal K$ for some universal constant $C$.

To invoke Banaszczyk's theorem here we set $x_i \in \R^{d_1d_2}$ to be the flattening of $X_i$ into a vector by concatenating its rows. 
Let $\mathcal K$ be the set of vectors in $\R^{d_1d_2}$ whose stacking (the inverse of flattening) results in a matrix whose spectral norm is less then $c\sqrt{d_1 + d_2}$.
More accurately $\mathcal K = \{x  \in \R^{d_1d_2} |\;\; ||\operatorname{stack}(x)||_2 \le c\sqrt{d_1+d_2}\}$. 
Due to $\operatorname{stack}$ being linear it is obvious that $\mathcal K$ is convex. 
Moreover $\Pr[g \in \mathcal K] \ge 1/2$ due to standard results on the operator norm of random Gaussian matrices \el{add citation \cite{}}.
Banaszczyk's theorem says that there exist signs $\sigma_i$ such that $\sum \sigma_i \operatorname{flatten}(X_i) \in C \mathcal K$.
In other words $\| \sum \sigma_i X_i\| = \| \operatorname{stack}( \sum \sigma_i \operatorname{flatten}(x_i x_i^T)) \| \le C \sqrt{d}$. 
\end{proof}
The algorithm for finding the signs above is, unfortunately, not as simple as with kernels. 
In fact, Banaszczyk's theorem was not constructive and a matching algorithms is a very recent breakthrough result by Bansal, Dadush, Garg, and Lovett \cite{DBLP:conf/stoc/BansalDGL18}.

Using the general framework above gives a matrix covariance coreset of cardinality $O(\sqrt{d}/\eps)$.
Alternatively, we obtain a streaming mergeable algorithm for creating coresets of size $\log^{2}(\min(n, \log(1/\delta)) \sqrt{d}/\eps$.



\el{Selecting $o(1/\eps^2)$ is claimed to be hard by \cite{DBLP:conf/focs/DeshpandeR10} and \cite{DBLP:conf/soda/DeshpandeRVW06}. We need to see if these hold in our setting or only hold for sampling.  Also, \cite{DBLP:conf/soda/GhashamiP14} showed that the impossibility result is limited to frequent directions like settings. Their assumptions don't hold here.} 

\subsection{Streaming Graph Approximation}
Streaming graphs have received a lot of attention in the last several years \cite{}. 
In the standard setting, the edges of a graph $G(V,E)$ are given to an algorithm one ofter the other in arbitrary order.
Here we show how to approximate all quadratic forms $\sum_{(i,j) \in E} (q_i - q_j)^2$ where the values $q$ are not known in advance.
Let $L_e$ be the graph Laplacian of the edge $e$. More accurately, $L_e(i,i) = L_e(j,j) = 1$ and  $L_e(i,j) = L_e(j,i) = -1$ and all other values are zeros.
Then, $\sum_{(i,j) \in E} (q_i - q_j)^2 = q^T \sum_{e \in E}L_e q = q^T L q$. 
In this formulation, this problem reduced to that of row (column) subset selection. 
Note that $L_e = v_ev_e^T$ where $v_e(i) = 1$, $v_e(j)=-1$ and zero else.
Applying the results of the last section we get a streaming mergeable algorithm of approximating graph quadratic forms.
The number of retained edges in the coreset $E'$ is $|E'| = O(\log^{2}(n) \sqrt{n}/\eps)$ and it guaranties that 
$|\sum_{(i,j) \in E} (q_i - q_j)^2  - c \sum_{(i,j) \in E'} (q_i - q_j)^2| \le \eps n$ for all unit norm vectors $q$.



\section{Linear Regression}

\section{Optimal Design of Experiments}
We discuss the problem of linear regression with mean squared error. In the classic setting we are given $n$ points $x_1,\ldots,x_n \in \R^d$ along with $n$ labels $y_1,\ldots,y_n \in \R$ and wish to find $w \in \R^d$ minimizing
$$ \sum_i (w^Tx_i - y_i)^2 $$
In the field of optimal design of experiments, the labels are not given with the data. Instead, the learner must choose in advance a subset of datapoints for which they query the labels. The subset is chosen to provide the least error in a generalization bound, assuming $(x_1,y_1),\ldots,(x_n,y_n)$ are i.i.d from an unknown distribution.

The way this is done in optimal design of experiments is as follows. Let $X$ be the set of $n$ points and consider a subset $B \subseteq X$. The empirical risk minimizer (ERM) for $B$ is 
$$ w_B = \left( \sum_{x \in B} xx^T \right)^{\dagger} y_B $$
%It is known that $w_B$ is an unbiased estimator of $w^*$, the optimal regressor for the distribution, where the noise comes from the realizations of the $y_i$'s. It follows that to minimize 
%$$ E_{x \sim {\cal D}} \ip{w-w^*,x}^2 $$
%which is the variance of $w$ measured by the distribution of data points, it suffice to minimize the second moment
%$$ E_{x \sim {\cal D}} \ip{w,x}^2  \approx w^T \left( \frac{1}{n} \sum_{i=1}^n x_ix_i^T \right) w $$


One can show that for $w_B$,
$$ \sum_i (w_B^Tx_i - y_i)^2 = C \cdot \text{Tr} \left(\left( \sum_{x \in B} xx^T \right)^{\dagger} \right) $$
\zk{This only works when $\sum_{x \in B} xx^T$ is full rank, otherwise we need to introduce some regularization and then things get messy.}

We can use the results above for optimal design in the case where $w$ is restricted to have a unit Euclidean norm. Something that is commonly practiced. Since we will solve the problem for the subset we choose in an optimal way for gaussian noise, we end up with an unbiased estimator for $w^*$. The error is the variance of that unbiased estimator and minimizing the variance for an unbiased estimator is the same as minimizing its second moment we can aim to minimize
$$ F(w) = w^T \left( \frac{1}{n} \sum_{i=1}^n x_ix_i^T \right) w $$
This leads to the exact same objective as in column subset selection. Meaning that an $\eps$ core-set for the $x$'s gives a guaranteed error for a subset in the optimal design setting. Specifically we get that we can obtain a core-set of size
$$ \min(1/\eps^2, \sqrt{d}/\eps)$$
For the case of $\eps < 1/\sqrt{d}$, which is an interesting case in linear regression, a random sample of $\sqrt{d}/\eps$ points will not give a generalization bound of $\eps$ (there are known lower bounds, http://jmlr.org/papers/volume16/shamir15a/shamir15a.pdf contradicting this), meaning that we have a provable non-trivial guarantee for active learning. I'm not sure what statements are known already. 

In the optimal design setting, they require keeping the covariance matrix. We could come up with a hueristic or even sketching algorithm that avoids maintaining $d^2$ points.


\section{Logistic Regression}
The following paper claims no streaming algorithms or coresets are available \cite{DBLP:journals/corr/abs-1805-08571}.
This paper claims to have done that \cite{DBLP:conf/nips/HugginsCB16}.

In \cite{DBLP:journals/corr/abs-1802-01751} (see paragraph before Lemma 2.2) they manage to obtain a coreset (with equal weights) of size $\sqrt{d \log(1/\eps)}/\eps$ for any kernel with certain properties. They do so by considering a matrix whose rows correspond to all the vectors in an $\eps$-net over the space, and columns correspond to the $n$ data points. Calling that matrix $A$, $A_{q,x} = K(q,x)$. They bound the discrepancy of the matrix defined as $\min_s \|As\|_\infty$ where $s$ is a sign vector by using a bound 
$$ \min_s \|As\|_\infty \leq \sqrt{\log(N)} \|A\|_{\gamma_2} $$
Here, $N$ is the size of the $\eps$ net and the $\gamma_2$ norm is 
$$ \|A\|_{\gamma_2} = \min_{B,C: A=BC^T} \text{max norm of a row of B or C}$$
They use the fact that for psd kernel $K$, $\|A\|_{\gamma_2} \leq 1$ (in fact it is equal to 1). Now, back to our case. For any fixed power $k$ we know that $K(x,q)=\ip{x,q}^k$ is a psd kernel, hence its corresponding matrix $A_k$ has $\gamma_2$ norm bounded by $1$. Now, the logistic function can be written in a taylor expansion whose coefficients, in absolute value, sum to a constant $C$. It follows that the $A$ matrix for the logistic function has a $\gamma_2$ norm bounded by $C$. The same arguments as in  \cite{DBLP:journals/corr/abs-1802-01751} can now prove that its discrepancy is bounded by $\sqrt{\log(N)} \approx \sqrt{d\log(1/\eps)}$

A useful fact about the $\gamma_2$ norm is that it is indeed a norm. This is proven in \cite{tomczak1989banach}. In \cite{matouvsek2014factorization} this is listed in Section 2, Proposition 2.2 along with other known properties of the $\gamma_2$ norm. 

%\paragraph{Reduction to the Kernel case}
%This is really the Kernel density estimation problem with $K(x,y) = \ip{x,y}^2$. We might be able to squeeze a bit more due to the fact that $\phi(x) = xx^T$ is a very special $d \times d$ matrix: a rank $1$ matrix. That being said, a coreset of $1/\eps^2$ can be obtained by simply using the kernel technique. The special structure can either give better dependence on $\eps$ or exploit low rank structure e.g.\ if the vectors form a nearly rank $k$ matrix.


%
%\subsection{notes about row selection}
%The problem is 
%$$ \min_s \| \sum_i \sigma_i x_ix_i^T\| $$
%where $\|x_i\| \leq 1$. Consider the following greedy approach: At point $x_i$, let
%$$ C_i = \sum_{j<i} \sigma_j x_j x_j^T, \ \  l_+ = \|C_i + x_i x_i^T \|, \ l_- = \|C_i - x_i x_i^T\|$$
%We choose $\sigma_i = 1$ if $l_+ < l_i$, otherwise $\sigma_i=-1$.
%
%This approach can be shown to be better than random. Random will result in an answer of up to 
%
%Consider the following example. 
%$$x_1=e_1, x_2=e_2*\eps, x_3=e_2, x_4=e_1/\sqrt{2} + e_2/\sqrt{2}$$
%where $\eps \to 0$. We get that the covariance matrix after inserting $x_3$ has two eigenvalues of +1 and -1. Now when inserting $x_4$ we have to increase the magnitude of one of them and we exceed the limit of 1. 
%


\section{Coreset for linear functions (based on $\gamma_2$ norm)}

Let $x_1,\ldots,x_n \in R^d$ be and $y_1,\ldots,y_n$ be labels associated to the data points. Let $K$ be a psd kernel. Consider a problem of learning a linear function aiming to minimize the loss of 
$$\min_q \sum_{i=1}^n L(K(x_i, q), y_i)$$

Our objective is to find a core-set of the $n$ data points. That is, we aim to find a set $S \subseteq [n]$ such that for every $q$ it holds that 
$$ \left| \sum_{i=1}^n L(K(x_i, q), y_i) - \frac{n}{|S|} \sum_{i \in S} L(K(x_i, q), y_i) \right| \leq \eps n $$

For a function $f(q,x)$, let $A_f$ be a matrix whose rows are an $\eps$-net over the Euclidean unit ball of $R^d$, and columns correspond to the different $x_i$'s. The value of $A_f$ at an entry corresponding to $(q,x)$ if $f(q,x)$.

\begin{definition}
For a matrix $A$,
$$ \|A\|_{\gamma_2} = \min_{B,C: A=BC^T} \|B\|_{2 \to \infty} \|C\|_{2 \to \infty}$$
is the $\gamma_2$ norm of $A$. 
\end{definition}
 \cite{tomczak1989banach} prove that this is indeed a norm, in particular that 
 $\|A_1+A_2\|_{\gamma_2} \leq \|A_1\|_{\gamma_2} + \|A_2\|_{\gamma_2}$. 


\begin{lemma} \label{lem:powers}
For $k \geq 0$ let $f_k(q,x) = \ip{q,x}^k$. If holds that $\|A_{f_k}\|_{\gamma_2} \leq 1$
\end{lemma}
\begin{proof}
For $k=0$ the statement is obvious. For $k>0$, notice that $f_k$ is a kernel, and in particular there exists a mapping $\phi_k: \R^d \to \R^{d^k}$ such that
$$ \ip{q,x}^k = \ip{\phi_k(q), \phi_k(x)} $$
Furthermore, 
$$ \|\phi_k(x)\| = \sqrt{\ip{\phi_k(x), \phi_k(x)}} = \sqrt{\|x\|^{2k}} = \|x\|^k \leq 1 $$
Since all $x,q$ have at most unit norm we get a bound over the $\gamma_2$ norm by viewing the $B,C$ matrices obtained from the $\phi_k$ embeddings of the different $x,q$'s
\end{proof}

\begin{lemma}
Let $L:\R \to \R$ be a function that can be written as 
$$ L(z) = \sum_{k=0}^\infty \alpha_k z^k$$
with $\sum_{k=0}^{\infty} |\alpha_k| \leq C$. For $f_L(q,x) = L(\ip{q,x})$ it holds that 
$$ \|A_{f_L}\|_{\gamma_2} \leq C $$
\end{lemma}
\begin{proof}
This is a direct corollary from $\gamma_2$ being a norm, and Lemma~\ref{lem:powers}
\end{proof}

This lemma in particular holds for the logistic function as its Taylor series has elements that converge. In particular, evaluating the function at $i$ and summing the real and imaginary parts of the result gives the sum of the absolute values of the coefficients, which is (roughly) $0.461$
\begin{corollary}
For $L(z) = \log(1+e^z)$ we have $\|A_{f_L}\|_{\gamma_2} \leq 0.5$
\end{corollary}

\begin{definition}
For a matrix $A$ of dimensions $m \times n$, $\disc(A)$ is defined as 
$$ \min_{s \in \{-1,1\}^n} \|As\|_\infty $$
\end{definition}
In \cite{matouvsek2014factorization} they prove that 
$$\|A\|_{\gamma_2} / \log(m) \leq \disc(A) \leq \|A\|_{\gamma_2}  \sqrt{\log(m)}$$
This gives a way to control the discrepancy of a matrix by representing it as a sum of matrices with known discrepency or $\gamma_2$ norm. In particular, for the logistic function we get that 
$$\disc(A_{f_L}) \leq \sqrt{\log(|Q|)} $$
where $Q$ is an $\eps$-net over the Euclidean unit sphere. This gives the bound of 
$$\disc(A_{f_L}) \leq O(\sqrt{d\log(1/\eps)}) $$

Let's discuss what this means. For a logistic regression problem, by observing the weights and labels we can change the signs of the inputs so that the logistic loss is always $\log(1+\exp(\ip{q,x}))$, and we can now convert the discrepancy result into a coreset of size $\sqrt{d\log(1/\eps)}/\eps$, as opposed to a random sample of $1/\eps^2$. 

Let $K$ be a psd kernel, meaning for $x,q \in \R^d$ we have for some mapping $\phi$ into a Hilbert space $K(q,x) = \ip{\phi_K(q), \phi_K(x)}$. Notice that for $\phi_k$ defined above
$$ \ip{\phi_k(\phi_K(q)), \phi_k(\phi_K(x))} = \ip{\phi_K(q), \phi_K(x)}^k = K(q,x)^k $$
meaning that $K^k(q,x)$ is a psd kernel for all $k \geq 0$. By using the exact same arguments before we can define $A_{K,k} \in \R^{|Q|,n}$ with $A_{K,k}(q,i) = K(q, x_i)^k$ and conclude that $\|A_{K,k}\|_{\gamma_2} \leq 1$. Then, for the matrix $A_{K,f_L}$ with
$$ L(z) = \sum_{k=0}^\infty \alpha_k z^k, A_{K,f_L}(q,i) = L(K(q,x_i))$$
we have 
$$\|A_{K,f_L}\|_{\gamma_2} \leq \sum_{k=0}^{\infty} |\alpha_k| $$

\begin{corollary}
For $L(z)=\log(1+\exp(z))$ and $L(z) = z^2$, and any psd kernel $K:\R^d \times \R^d \to \R$ we have
$$\|A_{K,f_L}\|_{\gamma_2} \leq 1 $$ 
and
$$\disc(A_{K,f_L}) \leq O(\sqrt{d\log(1/\eps)}) $$ 
\end{corollary}


\bibliographystyle{plain}
\bibliography{density}

\section{Appendix 1}
For completeness we recap a result from \cite{barany2008}.
\begin{theorem}[Simplified from B\'ar\'any \cite{barany2008} Theorem 4.1]
For any set of vector $x_1,...,x_n$ in $\R^d$ such that $\|x_i\| \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i \sigma_i x_i || \le \sqrt{d}$.
\end{theorem}
\begin{proof}
Consider the feasible region for $\sum_i \alpha_i x_i = 0$ and all $\alpha_i \in [-1,1]$.
This feasible region is not empty because it contains the origin. 
Consider an extreme point $\alpha^*_i$ and set $\sigma_i = 1$ w.p.\ $(1+\alpha^*_i)/2$ and $\sigma_i = -1$ w.p.\ $(1-\alpha^*_i)/2$.
$$
\E[\|\sum \sigma_i x_i\|^2] =  \E[\|\sum (\sigma_i - \alpha^*_i) x_i\|^2] = \sum \E[(\sigma_i - \alpha^*_i)^2] \|x_i\|^2= \sum (1-(\alpha_i^*)^2) \le d
$$
The first equality is due to $\sum_i \alpha_i x_i = 0$. The second used $\E[\sigma_i] = \alpha^*_i $.
The last inequality holds because the values $\alpha^*_i$ are non-integer in at most $d$ places. 
Assuming otherwise would entail more than $d$ linearly independent vectors in $\R^d$ due to $\alpha^*$ being an extreme point.
\end{proof}

Extension to the apx low rank case: The vectors are $[x_1,y_1],\ldots,[x_n,y_n]$. The dimension of $x$'s is $d$ and of $y$'s is infinite. Both $\|x_i\|, \|y\| \leq 1$. However, the overall norm of the $y$'s is bounded $\sum \|y_i\|^2 \leq \rho n$. Naively we can ignore the $y$'s and get a bound on the norm of $\sqrt{d + \sqrt{\rho} n}$ or something like that. If $y$ is obtained as the low components of PCA we can squeeze some more.
Consider another approach: Asking for $\sum x_i \alpha_i =0 $ gives $d$ linear restrictions. Let $\alpha^{(1)}$ be a feasible point. Let $\alpha^{(2)}$ be another feasible point that is restricted to being orthogonal to $\alpha^{(1)}$, and so on up to $\alpha^{(d)}$. For all these $\alpha$'s we have a bound of $2d$ to their rounding w.r.t to $x$. Now, for the $y$'s we can choose any one of these $\alpha$'s

However, we can search for $\alpha \in [-1,1]^n$ minimizing


%\section{Attempt at Cleaner Definitions}
%Let $\mathcal F$ be a family functions from a domain $\X$ to $\R$.
%We define the discrepancy of $\F$. Let $f \subset{\F}$ to a subset of $\F$.
%$$\mathcal D_m(\F) := \sup_{x\in\X} \min_{s\in \{0,1\}^m} \sup_{f\subset \F, |f|=m}  \sum_{i}\sigma_i f_i (x)$$  \el{need to double check the order here...}
%
%We claim that a very common task is to approximate the function $F = \sum_i f_i$ using a concise representation.  
%Specifically, using coresets we can approximate $F$ with $\tilde F = \sum_i w_i f_i$ where $w$ is non-zero in at most $k$ places.
%
%In this paper we show that if a family of functions exhibits low discrepancy then finding a good coreset is always possible.
%The size of the coresets we find with the proposed algorithm is always smaller than that achievable by uniform sampling.
%Moreover, we show that this framework includes a wide variety of problems including quantile approximation, approximate counting, machine learning classification and regressing, kernel density estimation and many more. 
%Finally, we show that coresets can always be created in a streaming fashion using fully mergable sketches with only a $\operatorname{poly}\log\log$ factor loss in space.
%
%
%In what follows, we subscript the function $f$ with its parametrization.
%\begin{itemize}
%\item In approximate counting we have $f_a(x) = 1$ if $a=x$ and zero else. Here both $a$ and $x$ belong to some finite domain.
%\item In quantile approximation we have $f_a(x) = 1$ if $a<x$ and zero else. Here, $a$ and $x$ belong to a set which exhibits total ordering.
%\item In matrix column subset selection we have $f_z(a,q) = \langle z,x \rangle ^2$ where $a,x \R^d$. For simplicity, we assume $\|a\| = \|x\| = 1$.
%\item In density estimation $f_a(x) = \exp(- \|a-x\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
%\item In logistic regression $f_a(x) = \operatorname{loss}(a,x) = \log(1 + \exp(-a^T x))$. 
%Here $x$ is the parameters of the linear model and $a$ is a training example vector multiplied by its label ($1$ or $-1$).
%\item Linear Classification $f_a(x) = 1$ if $a^Tx > 0$ and $0$ else. Here, like in Logistic Regression, $a$ is the example point multiplied by its label and $x$ is the weights of the linear classifier.
%\item Fractional Satisfiability $f(x,q)$ if $q$ satisfies $f$ and $0$ otherwise. Here $x$ in a clause in conjunctive normal form and $q$ is a boolean value assignment to the variables. For example $f(x,q) = 1$ iff $(q_1 \vee q_5  \vee \neg q_8)$ is true.
%\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q = \sum_{i,j\in E} (q_i - q_i)^2$ where $x$ is an edge and $L_x$ is the graph laplacian corresponding to the graph with a single edge $x$ and $q$ is a test vector.
%\item Probably also good for matrix approximation when we get entry updates. 
%\end{itemize}
%



\end{document}



%Specifically, we set $\sigma_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $\sigma_j$'s up to and not including $i$,
%$$\E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} \sigma_j z_j$.
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_{j=1}^k \sigma_j z_j \|^2] = \E[\| v + \sum_{j=i}^k \sigma_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $\sigma_i$ deterministically,
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + \sigma_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{\sigma_i,\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j \sigma_j z_j \|^2] $$
%we need
%$$ 2\ip{m,\sigma_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + \sigma_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ \sigma_i = -\text{sign} \ip{m,z_i}$$

