\documentclass{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{fact}[theorem]{Fact}

\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\text{Density}}
\newcommand{\eps}{\epsilon}
%\usepackage{ntheorem}
%\usepackage{amsthm}


%\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\title{Sketching for Density Estimation}
\date{\nonumber}

\author{
Zohar Karnin\\Amazon\\zkarnin@amazon.com
\and
Edo Liberty\\Amazon\\libertye@amazon.com
}

\begin{document}

\maketitle

\begin{abstract}
abs\end{abstract}

\section{Problem Definition}
Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density of a point $y$ is defined as 
$$ \D(y) = \sum_{x \in X} K(y,x) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent example is 
$$ K(x,y) = \exp(-\lambda \|x-y\|_2^2) $$
where $\lambda$ is a scaling parameter. What we discuss in what follows applies for any kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.

Our objective is obtaining a core-set $z_1,\ldots, z_\ell \in \R^d$, $\ell \ll n$ along with weights $w_1,\ldots, w_\ell$ such that for any point $y \in \R^d$
$$ \left| \sum_{i=1}^\ell w_i \cdot K(z_i, y) - \D(y) \right| < \eps n $$
We are interested in achieving this in the streaming setting, while bounding the amount of memory used

\section{Compactor}
We use the compactor approach. We collect $k$ data points $x_1,\ldots, x_k$ and output $m \approx k/2$ points $\tilde{x}_1,\ldots, \tilde{x}_m$ such that 
$$ \left| \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) \right| = o(k)$$
This can be used to transform a stream of $n$ to $n/2$, then the $n/2$ stream to $n/4$, until we have a stream we can keep in memory. We move to define the compactor. First, consider the error induced by a single compaction
$$ \text{Err}(y) = \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) $$
If the $\tilde{x}_i$'s are chosen as a subset of the $x_i$'s the error can be re-written as
$$ \text{Err}(y) = \sum_{i=1}^k s_i K(x_i,y)  $$
where $s_i \in \{-1, 1\}$ according to whether $x_i$ is in the set of $\tilde{x}$'s. It follows that our objective is the following function
$$ \min_{s \in \{-1,1\}^k} \max_y \sum_{i=1}^k s_i K(x_i,y) $$

We now exploit a property of kernel functions to reformulate the problem. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^k s_i K(x_i,y) = \sum_{i=1}^k s_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^k s_i \phi(x_i), \phi(y)} \leq $$
$$ \|\phi(y)\| \cdot \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| $$
This upper bound allows to optimize a much easier minimization function. 

We continue to solve the following problem: Given $k$ vectors $z_1,\ldots,z_k$ in an inner product space, compute signs minimizing 
$$\| \sum_i s_i z_i \| $$
We first notice that for i.i.d uniform signs, it holds that
$$\E[\| \sum_i s_i z_i \|^2] = \E[\sum_{i,j} s_i s_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. We use the method of expectation minimization to achieve this bound deterministically. Specifically, we set $s_i$ sequentially for $i$ while minimizing the expectation. We prove by induction on $i$ that for the fixed $s_j$'s up to and not including $i$,
$$\E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
For $i=1$ this is trivial. For $i>1$ let $m=\sum_{j=1}^{i-1} s_j z_j$.
$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \E[\| m + \sum_{j=i}^k s_j z_j \|^2] = \|m\|^2 + \sum_{j=i}^k \|z_j\|^2$$
It follows that by fixing $s_i$ deterministically,
$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2  $$
so in order to make sure that 
$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] $$
we need
$$ 2\ip{m,s_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2 \geq 0 $$
We thus set 
$$ s_i = -\text{sign} \ip{m,z_i}$$

Let us now translate this to an algorithm w.r.t the $d$ dimensional points $x_1,\ldots, x_k$. The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ s_i = -\text{sign} \left( \ip{\sum_{j<i} s_j \phi(x_j), \phi(x_i)} \right) = -\text{sign} \left( \sum_{j<i} s_j K(x_j, x_i) \right) $$
This guarantees that for any $y \in \R^d$,
$$ \sum_i s_i K(x_i, y) \leq \sqrt{ \sum_i \|\phi(x_i)\|^2 } = \sqrt{k} $$

For the coreset, this means that we can choose the $\tilde{x}$'s as either the $x$ vectors with positive $s_i$'s or negative $s_i$'s.







\end{document}
