\documentclass{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}
\usepackage{ amssymb }

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{claim}[theorem]{Claim}
\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{fact}[theorem]{Fact}

\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
%\usepackage{ntheorem}
\newcommand{\floor}[1]{\left \lfloor #1 \right \rfloor}
\newcommand{\ceil}[1]{\left \lceil #1 \right \rceil}


%\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
%\newtheorem{theorem}{Theorem}


\title{Sketching for Density Estimation}
\date{\nonumber}

\author{
Zohar Karnin\\Amazon\\zkarnin@amazon.com
\and
Edo Liberty\\Amazon\\libertye@amazon.com
}

\begin{document}

\maketitle

\begin{abstract}
abc
\end{abstract}

\section{introduction}
def

\section{General framework for sketching}
In many sketching problems our goal is to approximate a function of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i \in \mathcal X$ are all the stream items and $q \in \mathcal Q$ is any query point from a possible set $\mathcal Q$ which is known in advance.
The goal is to produce a sketch of $\ell$ points $z \subset \mathcal X$ and real weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$, either for all possible queries $q \in \mathcal Q$ simultaneously or for every fixed $q$ with high probability at least $1-\delta$. There are more complicated formulations such as weak coresets which we will not touch upon in this manuscript.
 
If $z \subset x$ the problem is further restricted to being a subset selection or a coreset selection problem. 
For bounded functions $f$, uniform sampling combined with a union bound over $|\mathcal Q|$ always provides a valid solution. 
While $|\mathcal Q|$ is often infinite it reduces to a finite (albeit exponential) set through a net argument. 
This is common practice in machine learning, specifically, PAC learning. 
However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 

Here are some examples:
\begin{itemize}
\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else. Here both $x$ and $q$ belong to some finite domain.
\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else. Here, $x$ and $q$ belong to a set which exhibits total ordering.
\item in matrix column subset selection we have $f(x, q) = \langle x,q \rangle ^2$ where $q$ is any unit vector in $\R^d$ and $x$ are matrix rows. For simplicity, we will assume in this manuscript that $x$ is also unit norm.
\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
\item in logistic regression $f(x, q) = \operatorname{loss}(x,q) = 1/(1 + \exp(-q^T x))$. 
Here $q$ is the parameters of the linear model and $x$ is a training example vector multiplied by its label ($1$ or $-1$).
\item Linear Classification $f(x,q) = 1$ if $x^t > 0$ and $0$ else. Here, like in Logistic Regression, $x$ is the example point multiplied by its label and $q$ is the weights of the linear classifier.
\item Fractional Satisfiability $f(x,q)$ if $q$ satisfies $f$ and $0$ otherwise. Here $x$ in a clause in conjunctive normal form and $q$ is a boolean value assignment to the variables. For example $f(x,q) = 1$ iff $(q_1 \vee q_5  \vee \neg q_8)$ is true.
\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q$ where $x$ is an edge and $L_x$ is the graph laplacian corresponding to the graph with a single edge $x$ and $q$ is a test vector.
\item Probably also good for matrix approximation when we get entry updates. 
\end{itemize}

We argue that we can offer a solution for producing coresets for all of the above problems in a unified manner. 
Moreover, our solution creates streaming algorithms with fully mergeable sketches. 

The high level idea is the following. Consider a stream of the data points $x_i$. We wish to to maintain a buffer that can hold up to $k$ points, and output a stream of data points having half the amount of the original stream, yet at the same time maintain a similar value for 
$$ \sum_i f(x_i, q) $$
for any query $q$. To do so our buffer, once collecting $k$ data points, performs a \emph{compact} operation that outputs $k/2$ data points of weight $2$ representing the original $k$ inputs. The compact operation partitions the $k$ points into two (roughly) sets, whose corresponding sum are very close. This is made formal with the following definitions: Let us consider a signed sum of error function $E(q) = \sum_{i=1}^{n} s_i f(x_i,q)$ where $s_i \in \{-1,1\}$ are signs.
Now consider $F_1(q) = F(q) + E(q) = \sum_{i=1}^{n} f(x_i,q)  + \sum_{i=1}^{n} s_i f(x_i,q)  = 2 \sum_{i | s_i=1} f(x_i,q)$ and the corresponding $F_{-1}$. We have that both $F_1(q), F_{-1}(q)$ are approximations for $F(q)$ obtained by coresets of item weights of $2$ and an error at most $|F_1(q) - F(q)| \le  | E(q)|$.

%
%We begin with describing the basic operation of compaction and the notion of flat error functions. 
%
%Let us consider a signed sum of error function $E(q) = \sum_{i=1}^{n} s_i f(x_i,q)$.
%Now consider $\tilde F(q) = F(q) + E(q) = \sum_{i=1}^{n} f(x_i,q)  + \sum_{i=1}^{n} s_i f(x_i,q)  = 2 \sum_{i | s_i=1} f(x_i,q)$.
%Therefore $\tilde F(q)$ is a coreset for $F(q)$ with item weights of $2$ and error at most $|\tilde F(q) - F(q)| \le \max_q | E(q)|$.
Note that the above is true for any choice of signs $s_i$, specifically, for those minimizing $\max_q | E(q)|$.
\begin{definition}
We define the discrepancy $\Delta_k$ with respect to a function $f(x,q)$ to be  
\[\Delta_k = \min_s \max_{q,x} \sum_{i=1}^{k} s_i f(x_i,q). \] 
\end{definition}
\noindent We claim that a function is sketch-able if $\Delta_k = o(k)$. 
As a warmup exercise, consider the non-streaming case. Throughout we discuss functions $f$ with the property $|f(x,q)| \leq 1$ for all $x,q$.


\begin{fact}
For any function $f$ which a corresponding discrepancy of $\Delta_k = O(1)$ there exists a coreset of size 
$O(1/\eps)$ whose error is at most $\eps n$.
\end{fact}
\begin{fact}
For any sum of functions $f$ which a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists a coreset of size 
$O(1/\eps^2)$ whose error is at most $\eps n$.
\end{fact}

\noindent Proving both facts is trivial. 
At step one, create a coreset of size at most $n/2$ of items of weight $2$ and incur an error of at most $c_1$ or $c_2 \sqrt{n}$.
Then, we repeat the process and create a coreset of size $n/4$ of items of weight $4$. Here, you incur error of $2c_1$ of $2\sqrt{n/2}  = \sqrt{2n}$.
Note that the sum of errors is asymptotically dominated by last iterations. 
Halting the compression at $O(1/\eps)$ or $O(1/\eps^2)$ items respectively achieves the goal.
 
In what follows, we prove the following.
\begin{theorem} \label{thm:streaming}
For any function $f$ with a corresponding discrepancy of $\Delta_k = O(1)$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(\log^2\left(\min(n, \log(1/\delta)) \right)/\eps\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require knowledge of the stream length $n$.
\end{theorem}


\begin{theorem}
For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
$O\left(\log^3\left(\min(n, \log(1/\delta)) \right)/\eps^2\right)$ whose error is at most $\eps n$ and failure probability at most $\delta$. For $\delta=0$ this results in a deterministic algorithm. The algorithm must be given $\eps, \delta$ as input and does not require knowledge of the stream length $n$.
\end{theorem}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists an unbiased fully-mergeable streaming coreset algorithms of size 
%$O(\log^2(\log(1/\delta))/\eps^2)$ whose error is at most $\eps n$ and failure probability at most $\delta$.
%\end{fact}
%
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(1)$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps)$ whose error is at most $\eps n$.
%\end{fact}
%\begin{fact}
%For any function $f$ with a corresponding discrepancy of $\Delta_k = O(\sqrt{k})$ there exists a fully-mergeable deterministic streaming coreset algorithms of size 
%$O(\log^2(n)/\eps^2)$ whose error is at most $\eps n$.
%\end{fact}

\begin{lemma} \label{lem:compactor}
Let $k$ be an integer and assume we have black box access to a solver that for any $k$ inputs $x_1,\ldots,x_k$ obtains $k$ signs $s_1,\ldots,s_k$ such that
$$\max_q \left| \sum_i s_i f(x_i, q)\right| \leq \rho k$$
Then there exist a streaming algorithm requiring a memory buffer of $k$ input items that given a stream of length $n$ outputs a stream $z_1,\ldots,z_m$ with the following properties
\begin{itemize}
\item $\{z_i\}_i$ is a subset of $\{x_i\}$
\item $\E[m] = n/2$
\item For any fixed query $q$, the error associated with $q$, defined as
$$\text{Err}(q) = 2\sum_{i=1}^m f(z_i,q) - \sum_{i=1}^n f(x_i,q)  $$
Can be decomposed as a sum
$$\text{Err}(q) = \sum_{j=1}^{n/k} \text{Err}_j(q)$$
where the different $\text{Err}_j(q)$ are independent, have mean 0 and $|\text{Err}_j(q)| \leq \rho k$
\end{itemize}
The last property ensures that deterministically $|\text{Err}(q)| \leq \rho n$, and that w.h.p.\ (via Chernoff's inequality) for a fixed $q$ 
$$|\text{Err}(q)| \lesssim \rho k\sqrt{n/k} = \rho \sqrt{kn}$$
\end{lemma}
\begin{proof}
The algorithm operates as follows. It keeps a buffer of $k$ items. Once the buffer fills with $x_1,\ldots,x_k$ it obtains the signs guaranteeing
$$\max_q \left| \sum_i s_i f(x_i, q)\right| \leq \rho k$$
Then, the algorithm output to the stream the with probability $1/2$ each data points $\{x_i \; | \; s_i = 1\}$ or $\{x_i \; | \; s_i = -1\}$ with twice the weight.
$$\sum_{i ,\; s_i=1} 2f(x_i, q) = \sum_{i} f(x_i, q) +  \sum_{i} s_i f(x_i, q)$$
$$\sum_{i ,\; s_i=-1} 2f(x_i, q) = \sum_{i} f(x_i, q) - \sum_{i} s_i f(x_i, q)$$

%Now, consider the set $P=\{i:s_i=1\}$ and $N=\{i:s_i=-1\}$.
%$$ \sum_i s_i f(x_i, q) = \sum_i s_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in P} f(x_i,q) - \sum_i f(x_i, q)   $$
%$$ -\sum_i s_i f(x_i, q) = \sum_i -s_i f(x_i, q) + \sum_i f(x_i,q) - \sum_i f(x_i,q) = 2\sum_{i \in N} f(x_i,q) - \sum_i f(x_i, q)   $$
%Thus, we draw a random coin and either output to the stream the data points of $P$ or $N$. We call this operation a single compaction. 
%\el{this is clunky way to say this...}
The expected output length is $k/2$ for every $k$ inputs, insuring $\E[m]=n/2$. 
As for the error term w.r.t.\ a fixed query $q$, it is clearly the sum of errors accumulated in each compaction. The error incurred in a compaction is either 
$\sum_i s_i f(x_i, q)$ or $-\sum_i s_i f(x_i, q)$ with equal probability. It follows that it is a Bernoulli r.v. with magnitude of at most $\rho k$ as required.
\end{proof}


\begin{proof} [Proof of Theorem~\ref{thm:streaming}]
Denote the streaming algorithm of Lemma~\ref{lem:compactor} as a compactor. The streaming algorithm operates as follows. At any given time it maintains a hierarchy of compactors where the hierarchy is measured in levels, starting from 0. The Compactor of level $h$ receives inputs of weight $2^h$ and outputs items of weight $2^{h+1}$. In the beginning we have a single compactor at level $h=0$. Once it outputs items to its output stream we open a compactor at level $h=1$ and so on. After observing $n$ items let $H$ be the level of the final compactor, meaning the compactor that never began an output stream.

The sketch at that point contains all the data points contained in the buffers of the different compactors, weighted according to the level of the compactor. For a query $q$ we analyze the error
$$\text{Err}(q) = \sum_{h=0}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \ .$$
Here, $k_h$ is the capacity of the buffers at level $h$, $n_h$ is the length of the stream observed at level $h$. The multiplier of $2^h$ is there since level $h$ observes items of weight $2^h$. The sum over $j$ is over the number of compactions at level $h$. Finally, the $Y_{ij}$ are independent random variables with mean zero and absolute value of at most $1$. 

To achieve a deterministic bound we set all $k_h=k$, and set the compactors to choose the signed set with the least cardinality, ensuring $n_h \leq n_{h-1}/2$ and $H \leq \log_2(n/k)$, thus
$$ \text{Err}(q) \leq \rho(k) \log_2(n/k) n $$

If however, we are interested in a high probability bound of $1-\delta$, we decompose the error to two components
$$\text{Err}(q) = \sum_{h=0}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho k_h Y_{ij} + \sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}$$
 Here, $H' = H - \ceil{\log(c \log(1/ \delta))}$ where $c$ is some universal constant we set later. Notice that the right summand contains roughly $\log(1/\delta)$ random variables. Since they are all Bernoulli we get that any combination of them is possible with probability $\geq \delta$ hence we must use a deterministic bound for these upper layers. We thus set $k_h=k$ for $h > H'$ and use the bound
$$ \text{Err}^{\text{top}}(q) = \left|\sum_{h=H'+1}^H 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij}\right| \leq O\left( \rho(k) \left(H-H'\right) n \right) =  O\left( \rho(k) \log \log(1/ \delta) n \right) $$
Now, for the bottom layers we use Chernoffs bound
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h)^2 k_h^2}   \right) =$$
$$\exp \left( -\frac{c' \eps^2 n^2}{\sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} } \right) $$

\zk{$n_h$ being random is a problem. The weight of the stream might increase with time. The analysis now works for a completely balanced case }
Consider the sum in the denominator. 
$$ \sum_{h=h}^{H'} 2^{2h} \rho(k_h)^2 k_h^2 \floor{n_h/k_h} = O(\sum_{h=0}^{H'} 2^{2h} \rho(k_h)^2 k_h n_h ) $$
Since $n_h \leq n_{h-1}/2$ and we have $k_h \leq k$ we have 
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2  ) $$
Now, if $\rho(k_h) \geq 1/\sqrt{k_h}$ we set $k_h = 2$ for all $h \leq H'$ and obtain
$$  = n  \cdot O(\sum_{h=0}^{H'} 2^h  ) = O(n^2/ (k \log(1/\delta))) $$
and we get
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) \right)   \right) $$
Hence, by setting $k \geq c/\eps^2$ for sufficiently large constant $c$ the probability is upper bounded by $\delta$.

Now, for $\rho(k) < 1/\sqrt{k}$ we have $\eta(k) = \rho(k)^2 k = o(1)$. We can exploit that to allow for $k = o(1/\eps^2)$. For $\eta(k)=O(k^{-p})$ we\footnote{We do not discuss other scenarios as we are not aware of any setting where they occur. Also, extending the analysis to them is a purely technical exercise} use $k_h = \max(2, \ceil{(2/3)^{H'-h}k})$, leading to a bound of
$$\sum_{h=0}^{H'} 2^h k_h \rho(k_h)^2 \leq \sum_{h=0}^{H'} 2^h \eta(k) \cdot (3/2)^{H'-h} = (3/2)^{H'} \eta(k) \sum_{h=0}^{H'} (4/3)^h \leq  $$
$$(3/2)^{H'} \eta(k) \cdot (4/3)^{H'} \cdot 3 = 3 \cdot 2^{H'} \eta(k) = O(\eta(k)n/(k\log(1/\delta))$$
Plugging this in the probability bound gives
$$ \Pr\left[   \left|\sum_{h=h}^{H'} 2^h \sum_{j=1}^{\floor{n_h/k_h} } \rho(k_h) k_h Y_{ij} \right| \geq \eps n \right] \leq \exp \left( -O\left( \eps^2 k \log(1/\delta) / \eta(k) \right)   \right) = $$
$$ \exp \left( -O\left( \eps^2 \log(1/\delta) / \rho(k)^2 \right)   \right) $$
Hence, setting $k$ large enough so that $\rho(k) \leq c\eps$ for some universal constant $c$ leads to the required bound.

In particular for $\rho(k) = O(1/k)$ we see that $k=1/\eps$ suffices, for general $p>1/2$ with $\rho(k) = k^{-p}$ we require $k=\eps^{-1/p}$.


Now, consider the overall error and memory of the entire process. For the bottom layers, for any $\rho(k)$ we require $O(k+H')=O(k+\log(n))$ memory. A closer inspection actually shows that once $k_h=2$ we are simply performing uniform sampling, and all of those layers can be done with constant memory. Hence, the memory footprint of the bottom layers is $O(k)$. The top layers require $O(\log\log(1/\delta)k)$ memory leading to a total memory usage of $O(\log\log(1/\delta)k)$.

The error for the bottom layers is $O(\min\{\rho(k), 1/\sqrt{k}\} n)$. The error of the top layers is $O(\log\log(1/\delta) \rho(k) n)$, meaning that the overall error is $O(\log\log(1/\delta) \rho(k) n)$.

Hence, if we aim for an error of $\eps n$ w.p.\ $1-\delta$ we set $k$ large enough such that $\rho(k) \leq c \eps / \log\log(1/\delta)$. Denoting by $\rho^{\dagger}$ the inverse function to $\rho$ this leads to a total memory requirement of 
$$ \rho^{\dagger}(c \eps / \log\log(1/\delta)) \log\log(1/\delta) $$

\end{proof}


\section{Density Estimation}

Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $\rho: \R^d \rightarrow \R$ of a point $y$ is defined as 
$$ \rho(y) = \sum_{i=1}^{n} K(x_i,y) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include

$$ K(x,y) = \exp(- \|x-y\|_2^2/\sigma^2)\;\;\; K(x,y) = \exp(-\lambda \|x-y\|/\sigma) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-y\|_2^2/\sigma^2)^{-1}$$

where $\sigma$ is a scaling parameter. What we discuss in what follows applies for any kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.


Using our framework, we need to bound $\Delta(y) = \sum_{i=1}^k s_i K(x_i,y)$. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^k s_i K(x_i,y) = \sum_{i=1}^k s_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^k s_i \phi(x_i), \phi(y)} \leq \|\phi(y)\| \cdot \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| $$
This upper bound allows us to optimize (find values for $s_i$) a simple expression.
Given $k$ vectors $z_1,\ldots,z_k$ in an inner product space, compute signs minimizing $\| \sum_i s_i z_i \|$.
We first notice that for i.i.d.\ uniform signs, it holds that
$$\E[\| \sum_i s_i z_i \|^2] = \E[\sum_{i,j} s_i s_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. By Markov with probability at least $3/4$ we have $\| \sum_i s_i z_i \| \le 2\sum_i \|z_i\|$.
%
We can also use the method of expectation minimization to achieve this bound deterministically.
Specifically, we will guaranty that $\| \sum_{j=1}^i s_j z_j \|^2 \le \sum_{j=1}^i \|z_j\|^2$ for all $i$.
This is trivially true for $i=1$. 
For another value $i$ set $s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle z_j, z_i \rangle)$ 
and  assume by induction that $\| \sum_{j=1}^{i-1} s_j z_j\|^2 \le \sum_{j=1}^{i-1} \|z_j\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}s_j z_j\|^2 &=& \|\sum_{j=1}^{i-1}s_j z_j\|^2 + \|z_i\|^2 + \langle \sum_{j=1}^{i-1}s_j z_j, s_i z_i\rangle \\
&\le& \sum_{j=1}^{i-1} \|z_j\|^2 + \|z_i\|^2 + s_i \sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|z_j\|^2 - |\sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle| \le \sum_{j=1}^{i} \|z_j\|^2
\end{eqnarray*}
This completes the proof that $\| \sum_{j=1}^{k}s_j z_j\| \le \sqrt{\sum_{j=1}^{i} \|z_j\|^2}$ for our choice of values for $s$. 
%
Let us now translate this to an algorithm w.r.t.\ the $d$ dimensional points $x_1,\ldots, x_k$. 
The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle \phi(x_j), \phi(x_i) \rangle) = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j  K(x_j, x_i))$$
This guarantees that 
$$ \forall y \in \R^d \;\;\; \sum s_i K(x_i, y) \leq \sqrt{ \sum \|\phi(x_i)\|^2 } = \sqrt{k} $$

Using the framework above this provides a streaming coreset construction for kernel density estimation of size $O(1/\eps^2)$ such that 
$\forall \;q\;\; |\tilde \rho(q) - \rho(q)| \le \eps n$.
This matches the results achieved by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751}.
\el{The running time of this algorithm is amortized.... I hope it will be better than Jeff's result, especially in the randomized shrinking buffers setting.}








\section{Row (or Column) Subset Selection}
Assume you are getting the rows $x_i$  of matrix $X$ in a stream and you want to compute $Z$ such that $\|Z^TZ - X^TX\| \le \eps n$.
For simplicity, assume $x_i$ are all unit length. Moreover, assume each vector in $Z$ must be one of the rows in $X$ up to some constant factor.
This is called the row subset selection problem. 
Since you are trying to minimize the quadratic form, you get that $f(x, q) = \langle x,q \rangle ^2$ and 
$$E(y) = \sum_i s_i \langle x_i,q \rangle ^2 = q^T (\sum_i s_i x_i x_i^T ) q \le \|\sum_i s_i x_i x_i^T\| \le c\sqrt{d}$$

\noindent The last inequality is a direct application of the following Theorem.
\begin{theorem}\label{BansalInDaHouz}
For any set of matrices $X_1,...,X_n$ in $\R^{d_1 \times d_2}$ such that $\|X_i\|_{F} \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i s_i X_i||_{2} \le O(\sqrt{d_1 + d_2})$.
\end{theorem}
\begin{proof}
First, recount Banaszczyk's theorem. 
Let $\mathcal K$ be a convex body in $\R^d$ such that $\Pr[g \in \mathcal K] \ge 1/2$ where $g$ is Gaussian i.i.d.
Let $x_1,\ldots,x_n$ be vectors in $\R^d$ with $\|x_i\| \le 1$. 
Then, there exist signs $s_i$ such that $\sum s_i x_i \in C \mathcal K$ for some universal constant $C$.

To invoke Banaszczyk's theorem here we set $x_i \in \R^{d_1d_2}$ to be the flattening of $X_i$ into a vector by concatenating its rows. 
Let $\mathcal K$ be the set of vectors in $\R^{d_1d_2}$ whose stacking (the inverse of flattening) results in a matrix whose spectral norm is less then $10\sqrt{d_1 + d_2}$.
More accurately $\mathcal K = \{x  \in \R^{d_1d_2} |\;\; ||\operatorname{stack}(x)||_2 \le 10\sqrt{d_1+d_2}\}$. 
Due to $\operatorname{stack}$ being linear it is obvious that $\mathcal K$ is convex. 
Moreover $\Pr[g \in \mathcal K] \ge 1/2$ due to standard results on the operator norm of random Gaussian matrices \cite{}.
Banaszczyk's theorem says that there exist signs $s_i$ such that $\sum s_i \operatorname{flatten}(X_i) \in C \mathcal K$.
In other words $\| \sum s_i X_i\| = \| \operatorname{stack}( \sum s_i \operatorname{flatten}(x_i x_i^T)) \| \le 10 C \sqrt{d}$. 
\end{proof}
The algorithm for finding the signs reduces to random choice for $s_i$ and then checking the norm of $\sum s_i x_i x_i^T$.
This suffices according to \cite{DBLP:conf/approx/DadushGLN16} and the corresponding convex body $\mathcal K$ being symmetric.
Using the general framework above gives a matrix covariance coreset of cardinality $O(\sqrt{d}/\eps)$.
Alternatively, we obtain a streaming mergeable algorithm for creating coresets of size $\log^{2}(\min(n, \log(1/\delta)) \sqrt{d}/\eps$.



\el{Selecting $o(1/\eps^2)$ is claimed to be hard by \cite{DBLP:conf/focs/DeshpandeR10} and \cite{DBLP:conf/soda/DeshpandeRVW06}. We need to see if these hold in our setting or only hold for sampling.  Also, \cite{DBLP:conf/soda/GhashamiP14} showed that the impossibility result is limited to frequent directions like settings. Their assumptions don't hold here.} \zk{We can get $1/\eps^2$ though which is better than $\sqrt{d}/\eps$ as long as $\eps > 1/\sqrt{d}$. If $\eps < 1/\sqrt{d}$ then $\sqrt{d}/\eps > d$ - shouldn't it be trivial to get a coreset of size $d$?}


\section{Streaming Graph Approximation}
Streaming graphs have received a lot of attention in the last several years \cite{}. 
In the standard setting, the edges of a graph $G(V,E)$ are given to an algorithm one ofter the other in arbitrary order.
Here we show how to approximate all quadratic forms $\sum_{(i,j) \in E} (q_i - q_j)^2$ where the values $q$ are not known in advance.
Let $L_e$ be the graph Laplacian of the edge $e$. More accurately, $L_e(i,i) = L_e(j,j) = 1$ and  $L_e(i,j) = L_e(j,i) = -1$ and all other values are zeros.
Then, $\sum_{(i,j) \in E} (q_i - q_j)^2 = q^T \sum_{e \in E}L_e q = q^T L q$. 
In this formulation, this problem reduced to that of row (column) subset selection. 
Note that $L_e = v_ev_e^T$ where $v_e(i) = 1$, $v_e(j)=-1$ and zero else.
Applying the results of the last section we get a streaming mergeable algorithm of approximating graph quadratic forms.
The number of retained edges in the coreset $E'$ is $|E'| = O(\log^{2}(n) \sqrt{n}/\eps)$ and it guaranties that 
$|\sum_{(i,j) \in E} (q_i - q_j)^2  - c \sum_{(i,j) \in E'} (q_i - q_j)^2| \le \eps n$ for all unit norm vectors $q$.

\section{Logistic Regression}
The following paper claims no streaming algorithms or coresets are available \cite{DBLP:journals/corr/abs-1805-08571}.
This paper claims to have done that \cite{DBLP:conf/nips/HugginsCB16}.


%\paragraph{Reduction to the Kernel case}
%This is really the Kernel density estimation problem with $K(x,y) = \ip{x,y}^2$. We might be able to squeeze a bit more due to the fact that $\phi(x) = xx^T$ is a very special $d \times d$ matrix: a rank $1$ matrix. That being said, a coreset of $1/\eps^2$ can be obtained by simply using the kernel technique. The special structure can either give better dependence on $\eps$ or exploit low rank structure e.g.\ if the vectors form a nearly rank $k$ matrix.


%
%\subsection{notes about row selection}
%The problem is 
%$$ \min_s \| \sum_i s_i x_ix_i^T\| $$
%where $\|x_i\| \leq 1$. Consider the following greedy approach: At point $x_i$, let
%$$ C_i = \sum_{j<i} s_j x_j x_j^T, \ \  l_+ = \|C_i + x_i x_i^T \|, \ l_- = \|C_i - x_i x_i^T\|$$
%We choose $s_i = 1$ if $l_+ < l_i$, otherwise $s_i=-1$.
%
%This approach can be shown to be better than random. Random will result in an answer of up to 
%
%Consider the following example. 
%$$x_1=e_1, x_2=e_2*\eps, x_3=e_2, x_4=e_1/\sqrt{2} + e_2/\sqrt{2}$$
%where $\eps \to 0$. We get that the covariance matrix after inserting $x_3$ has two eigenvalues of +1 and -1. Now when inserting $x_4$ we have to increase the magnitude of one of them and we exceed the limit of 1. 
%
\bibliographystyle{plain}
\bibliography{density}

\section{Appendix 1}
For completeness we recap a result from \cite{barany2008}.
\begin{theorem}[Simplified from B\'ar\'any \cite{barany2008} Theorem 4.1]
For any set of vector $x_1,...,x_n$ in $\R^d$ such that $\|x_i\| \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i s_i x_i || \le \sqrt{d}$.
\end{theorem}
\begin{proof}
Consider the feasible region for $\sum_i \alpha_i x_i = 0$ and all $\alpha_i \in [-1,1]$.
This feasible region is not empty because it contains the origin. 
Consider an extreme point $\alpha^*_i$ and set $s_i = 1$ w.p.\ $(1+\alpha^*_i)/2$ and $s_i = -1$ w.p.\ $(1+\alpha^*_i)/2$.
$$
\E[\|\sum s_i x_i\|^2] =  \E[\|\sum (s_i - \alpha^*_i) x_i\|^2] = \sum \E[(s_i - \alpha^*_i)^2] \|x_i\|^2= \sum (1-(\alpha_i^*)^2) \le d
$$
The last inequality holds because the values $\alpha^*_i$ are non-integer in at most $d$ places. 
Assuming otherwise would entail more than $d$ linearly independent vectors in $\R^d$ due to $\alpha^*$ being an extreme point.
\end{proof}





\end{document}



%Specifically, we set $s_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $s_j$'s up to and not including $i$,
%$$\E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} s_j z_j$.
%$$ \E_{s_i,\ldots,s_k}[\| \sum_{j=1}^k s_j z_j \|^2] = \E[\| v + \sum_{j=i}^k s_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $s_i$ deterministically,
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + s_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] $$
%we need
%$$ 2\ip{m,s_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ s_i = -\text{sign} \ip{m,z_i}$$

