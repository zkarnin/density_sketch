\documentclass{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{fact}[theorem]{Fact}

\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
%\usepackage{ntheorem}
%\usepackage{amsthm}


%\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\title{Sketching for Density Estimation}
\date{\nonumber}

\author{
Zohar Karnin\\Amazon\\zkarnin@amazon.com
\and
Edo Liberty\\Amazon\\libertye@amazon.com
}

\begin{document}

\maketitle

\begin{abstract}
abc
\end{abstract}

\section{introduction}
def

\section{General framework for sketching}
In many sketching problems our goal is to approximate a function of the of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i$ are all the stream items and and $q$ is some query point. 
The goal is to produce a sketch of $\ell$ points $z$ and weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$. 
If $z \subset x$ this is a subset selection problem. 
For bounded functions $f$, uniform sampling combined with a union bound over the space of possible values for $q$ always provides a valid solution. 
While the number of possible queries $q$ is often infinite it often reduces to a finite set through a net argument. 
This is common practice in machine learning, specifically, PAC learning. 

However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 

Here are some examples:
\begin{itemize}
\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else. Here both $x$ and $q$ belong to some finite domain.
\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else. Here, $x$ and $q$ belong to a set which exhibits strong ordering.
\item in matrix row subset selection we have $f(x, q) = \langle x,q \rangle ^2$ where $q$ is any unit vector in $\R^d$ and $x$ are matrix rows. For simplicity, we will assume in this manuscript that $x$ is also unit norm.
\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
\item in logistic regression $f(x, q) = \operatorname{loss}(x,q) = 1/(1 + \exp(-q^T x))$. 
Here $q$ is the parameters of the linear model and $x$ is a training example vector multiplied by its label ($1$ or $-1$).
\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q$ where $x$ is an edge and $L_x$ is the graph laplacian and $q$ is a test vector.
\item Probably also good for matrix approx when we get entry updates. 
\end{itemize}

We argue that we can offer a unified solution for producing coresets for all of the above problems in a unified manner. 
Moreover, our solution created streaming algorithms with fully mergeable sketches. 

We begin with describing the basic operation of compaction and the notion of flat error functions. 

Let us consider a signed sum of error function $E(q) = \sum_{i=1}^{n} s_i f(x_i,q)$.
Now consider $\tilde F(q) = F(q) + E(q) = \sum_{i=1}^{n} f(x_i,q)  + \sum_{i=1}^{n} s_i f(x_i,q)  = 2 \sum_{i | s_i=1} f(x_i,q)$.
Therefore $\tilde F(q)$ is a coreset for $F(q)$ with item weights of $2$ and error at most $|\tilde F(q) - F(q)| \le \max_q | E(q)|$.
Note that the above is true for any choice of signs $s_i$, specifically, for those minimizing $\max_q | E(q)|$.
We call $E$ flat if $\min_s \max_q |E(q)| = o(n)$. 

\begin{definition}
We define $E_n$ with respect to a function $f(x,q)$ to be  $\min_s \max_q \max_x \sum_{i=1}^{n} s_i f(x_i,q)$. 
\end{definition}

As a warmup exercise, consider the case where $E_n = 1$.
This is the case for quantiles and for counting. 
At step one, we will create a coreset of weight $2$ and size at most $n/2$ (by either adding or subtracting $E$). Here we incur an error of at most $1$.
Then, we repeat the process and incur an error of at most $2$ resulting in a stream of length at most $n/4$. 
We can continue the above process until we are left with $O(1/\eps)$ points in the coreset. At this point, the error adds up to at most $\eps n$.
Similarly, when $E_n = O(\sqrt{n})$ we get a coreset of size $O(1/\eps^2)$ whose error is at most $\eps n$.
This gives an offline algorithm.

\el{We can describe the general streaming algorithm here. I think $E_n$ is a sufficient characterization of this.} 


For all of the above, we argue that we can find flat signed error functions such that $E_n = O(1)$ or $E_n = O(\sqrt{n})$.




\section{Density estimation}

Already done by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751} in the non streaming setting.

Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $\rho: \R^d \rightarrow \R$ of a point $y$ is defined as 
$$ \rho(y) = \sum_{i=1}^{n} K(x_i,y) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include
<<<<<<< HEAD
$$ K(x,y) = \exp(- \|x-y\|_2^2/\sigma^2)\;\;\; K(x,y) = \exp(-\lambda \|x-y\|/\sigma) \; \mbox{and}\;\;\; K(x,y) = \frac{1}{1+\|x-y\|_2^2/\sigma^2)}$$
$$ K(x,y) = \max(0, 1- \|x-y\|/\sigma) $$
=======
$$ K(x,y) = \exp(- \|x-y\|_2^2/\sigma^2)\;\;\; K(x,y) = \exp(-\lambda \|x-y\|/\sigma) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-y\|_2^2/\sigma^2)^{-1}$$
>>>>>>> ce3a786138737689452723acbd3980a5d8533fbe
where $\sigma$ is a scaling parameter. What we discuss in what follows applies for any kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.

Our objective is obtaining a core-set $z_1,\ldots, z_\ell \in \R^d$, $\ell \ll n$ along with weights $w_1,\ldots, w_\ell$ such that for any point $y \in \R^d$
$$ \left| \sum_{i=1}^\ell w_i \cdot K(z_i, y) - \D(y) \right| < \eps n $$
We are interested in achieving this in the streaming setting, while bounding the amount of memory used throughout the run of the algorithm.

\section{Compactor}
We use the compactor approach. We collect $k$ data points $x_1,\ldots, x_k$ and output $m \approx k/2$ points $\tilde{x}_1,\ldots, \tilde{x}_m$ such that 
$$ \left| \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) \right| = o(k)$$
This can be used to transform a stream of $n$ to $n/2$, then the $n/2$ stream to $n/4$, until we have a stream we can keep in memory. We move to define the compactor. First, consider the error induced by a single compaction
$$ \text{Err}(y) = \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) $$
If the $\tilde{x}_i$'s are chosen as a subset of the $x_i$'s the error can be re-written as
$$ \text{Err}(y) = \sum_{i=1}^k s_i K(x_i,y)  $$
where $s_i \in \{-1, 1\}$ according to whether $x_i$ is in the set of $\tilde{x}$'s. It follows that our objective is the following function
$$ \min_{s \in \{-1,1\}^k} \max_y \sum_{i=1}^k s_i K(x_i,y) $$

We now exploit a property of kernel functions to reformulate the problem. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^k s_i K(x_i,y) = \sum_{i=1}^k s_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^k s_i \phi(x_i), \phi(y)} \leq $$
$$ \|\phi(y)\| \cdot \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| $$
This upper bound allows to optimize a much easier minimization function. 

We continue to solve the following problem: Given $k$ vectors $z_1,\ldots,z_k$ in an inner product space, compute signs minimizing 
$$\| \sum_i s_i z_i \| $$
We first notice that for i.i.d.\ uniform signs, it holds that
$$\E[\| \sum_i s_i z_i \|^2] = \E[\sum_{i,j} s_i s_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. By Markov with probability at least $3/4$ we have $\| \sum_i s_i z_i \| \le 2\sum_i \|z_i\|$.

We can also use the method of expectation minimization to achieve this bound deterministically.
Specifically, we will guaranty that $\| \sum_{j=1}^i s_j z_j \|^2 \le \sum_{j=1}^i \|z_j\|^2$ for all $i$.
This is trivially true for $i=1$. 
For another value $i$ set $s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle z_j, z_i \rangle)$ 
and  assume by induction that $\| \sum_{j=1}^{i-1} s_j z_j\|^2 \le \sum_{j=1}^{i-1} \|z_j\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}s_j z_j\|^2 &=& \|\sum_{j=1}^{i-1}s_j z_j\|^2 + \|z_i\|^2 + \langle \sum_{j=1}^{i-1}s_j z_j, s_i z_i\rangle \\
&\le& \sum_{j=1}^{i-1} \|z_j\|^2 + \|z_i\|^2 + s_i \sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|z_j\|^2 - |\sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle| \le \sum_{j=1}^{i} \|z_j\|^2
\end{eqnarray*}


Let us now translate this to an algorithm w.r.t.\ the $d$ dimensional points $x_1,\ldots, x_k$. 
The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle \phi(x_j), \phi(x_i) \rangle) = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j  K(x_j, x_i)) $$
This guarantees that for any $y \in \R^d$,
$$ \sum_i s_i K(x_i, y) \leq \sqrt{ \sum_i \|\phi(x_i)\|^2 } = \sqrt{k} $$

Before we extend this to a complete algorithm, it is worth getting an intuition for the meaning of this result.

For the coreset, this means that we can choose the $\tilde{x}$'s as either the $x$ vectors with positive $s_i$'s or negative $s_i$'s.


%
%<<<<<<< HEAD
%\section{General note about sketching and flat signed function}
%In many sketching problems our goal is to approximate a function of the of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i$ are all the stream items and and $q$ is some query point. 
%The goal is to produce a sketch of $\ell$ points $z$ and weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$. 
%If $z \subset x$ this is a subset selection problem. 
%For bounded functions $f$, uniform sampling combined with a union bound over the space of possbile values for $q$ always provides a valid solution.
%However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 
%=======
%
%>>>>>>> ce3a786138737689452723acbd3980a5d8533fbe



\section{Row subset selection}
Assume you are getting the rows $x_i$  of matrix $X$ in a stream and you want to compute $Z$ such that $\|Z^TZ - X^TX\| \le \eps n$.
for simplicity, assume $x_i$ are all unit length. Moreover, assume each vector $Z$ must be one of the rows in $X$ up to some constant factor.
This is called the row subset selection problem. 
Since you are trying to minimize the quadratic form, you get that $f(x, q) = \langle x,q \rangle ^2$ and 
$$E(y) = \sum_i s_i \langle x_i,q \rangle ^2 = q^T (\sum_i s_i x_i x_i^T ) q \le \|\sum_i s_i x_i x_i^T\|$$
When summing rank one projections matrices $x_i x_i^T$ you can think of two extreme cases. 
If $x_i$ are orthogonal to each other than $\|\sum_i s_i x_i x_i^T\| = 1$ independently of $s_i$.
If $x_i$ are all equal than a random assignment would get  $\E[\|\sum_i s_i x_i x_i^T\|] = E[|\sum_i s_i|] = \Omega(\sqrt{k})$.
But, in that case it's also easy to find a sequence of $s_i$ which gives $\|\sum_i s_i x_i x_i^T\| = O(1)$. Is that always possible?
\el{My experiments show that behaves like polylog $d$ and $n$ but who knows. I thought I proved $O(1)$ but I think it's wrong.}
If this is correct, it would give a covariance sketch is subset selection of size $1/\eps$ which would be an improvement over frequent directions.

Selecting $o(1/\eps^2)$ is claimed to be hard by \cite{DBLP:conf/focs/DeshpandeR10} and \cite{DBLP:conf/soda/DeshpandeRVW06}. We need to see if these hold in our setting or only hold for sampling.  Also, \cite{DBLP:conf/soda/GhashamiP14} showed that the impossibility result is limited to frequent directions like settings.

\paragraph{Hard example?}
$x_i$'s are gaussian vectors with mean zero and covariance $\sum_{i=1}^d \sqrt{i} \cdot e_i e_i^T$. It might be the case that any sign assignment here will result in $\sqrt{d}$ operator norm. 

\paragraph{Reduction to the Kernel case}
This is really the Kernel density estimation problem with $K(x,y) = \ip{x,y}^2$. We might be able to squeeze a bit more due to the fact that $\phi(x) = xx^T$ is a very special $d \times d$ matrix: a rank $1$ matrix. That being said, a coreset of $1/\eps^2$ can be obtained by simply using the kernel technique. The special structure can either give better dependence on $\eps$ or exploit low rank structure e.g.\ if the vectors form a nearly rank $k$ matrix.
%
%\subsection{notes about row selection}
%The problem is 
%$$ \min_s \| \sum_i s_i x_ix_i^T\| $$
%where $\|x_i\| \leq 1$. Consider the following greedy approach: At point $x_i$, let
%$$ C_i = \sum_{j<i} s_j x_j x_j^T, \ \  l_+ = \|C_i + x_i x_i^T \|, \ l_- = \|C_i - x_i x_i^T\|$$
%We choose $s_i = 1$ if $l_+ < l_i$, otherwise $s_i=-1$.
%
%This approach can be shown to be better than random. Random will result in an answer of up to 
%
%Consider the following example. 
%$$x_1=e_1, x_2=e_2*\eps, x_3=e_2, x_4=e_1/\sqrt{2} + e_2/\sqrt{2}$$
%where $\eps \to 0$. We get that the covariance matrix after inserting $x_3$ has two eigenvalues of +1 and -1. Now when inserting $x_4$ we have to increase the magnitude of one of them and we exceed the limit of 1. 
%
\bibliographystyle{plain}
\bibliography{density}

\end{document}


%Specifically, we set $s_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $s_j$'s up to and not including $i$,
%$$\E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} s_j z_j$.
%$$ \E_{s_i,\ldots,s_k}[\| \sum_{j=1}^k s_j z_j \|^2] = \E[\| v + \sum_{j=i}^k s_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $s_i$ deterministically,
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + s_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] $$
%we need
%$$ 2\ip{m,s_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ s_i = -\text{sign} \ip{m,z_i}$$

