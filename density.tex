\documentclass{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{fact}[theorem]{Fact}

\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
%\usepackage{ntheorem}
%\usepackage{amsthm}


%\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}

\title{Sketching for Density Estimation}
\date{\nonumber}

\author{
Zohar Karnin\\Amazon\\zkarnin@amazon.com
\and
Edo Liberty\\Amazon\\libertye@amazon.com
}

\begin{document}

\maketitle

\begin{abstract}
abs\end{abstract}

\section{Problem Definition}
Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $\rho: \R^d \rightarrow \R$ of a point $y$ is defined as 
$$ \rho(y) = \sum_{i=1}^{n} K(x_i,y) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include
$$ K(x,y) = \exp(- \|x-y\|_2^2/\sigma^2)\;\;\; K(x,y) = \exp(-\lambda \|x-y\|/\sigma) \; \mbox{and}\;\;\; K(x,y) = \frac{1}{1+\|x-y\|_2^2/\sigma^2)}$$
where $\sigma$ is a scaling parameter. What we discuss in what follows applies for any kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.

Our objective is obtaining a core-set $z_1,\ldots, z_\ell \in \R^d$, $\ell \ll n$ along with weights $w_1,\ldots, w_\ell$ such that for any point $y \in \R^d$
$$ \left| \sum_{i=1}^\ell w_i \cdot K(z_i, y) - \D(y) \right| < \eps n $$
We are interested in achieving this in the streaming setting, while bounding the amount of memory used throughout the run of the algorithm.

\section{Compactor}
We use the compactor approach. We collect $k$ data points $x_1,\ldots, x_k$ and output $m \approx k/2$ points $\tilde{x}_1,\ldots, \tilde{x}_m$ such that 
$$ \left| \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) \right| = o(k)$$
This can be used to transform a stream of $n$ to $n/2$, then the $n/2$ stream to $n/4$, until we have a stream we can keep in memory. We move to define the compactor. First, consider the error induced by a single compaction
$$ \text{Err}(y) = \sum_{i =1}^k K(x_i,y) - 2\sum_{i=1}^m K(\tilde{x}_i, y) $$
If the $\tilde{x}_i$'s are chosen as a subset of the $x_i$'s the error can be re-written as
$$ \text{Err}(y) = \sum_{i=1}^k s_i K(x_i,y)  $$
where $s_i \in \{-1, 1\}$ according to whether $x_i$ is in the set of $\tilde{x}$'s. It follows that our objective is the following function
$$ \min_{s \in \{-1,1\}^k} \max_y \sum_{i=1}^k s_i K(x_i,y) $$

We now exploit a property of kernel functions to reformulate the problem. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^k s_i K(x_i,y) = \sum_{i=1}^k s_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^k s_i \phi(x_i), \phi(y)} \leq $$
$$ \|\phi(y)\| \cdot \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| $$
This upper bound allows to optimize a much easier minimization function. 

We continue to solve the following problem: Given $k$ vectors $z_1,\ldots,z_k$ in an inner product space, compute signs minimizing 
$$\| \sum_i s_i z_i \| $$
We first notice that for i.i.d.\ uniform signs, it holds that
$$\E[\| \sum_i s_i z_i \|^2] = \E[\sum_{i,j} s_i s_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. By Markov with probability at least $3/4$ we have $\| \sum_i s_i z_i \| \le 2\sum_i \|z_i\|$.

We can also use the method of expectation minimization to achieve this bound deterministically.
Specifically, we will guaranty that $\| \sum_{j=1}^i s_j z_j \| \le \sum_{j=1}^i \|z_j\|^2$ for all $i$.
This is trivially true for $i=1$. 
For another value $i$ set $s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle z_j, z_i \rangle)$ 
and  assume by induction that $\| \sum_{j=1}^{i-1} s_j z_j\|^2 \le \sum_{j=1}^{i-1} \|z_j\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}s_j z_j\|^2 &=& \|\sum_{j=1}^{i-1}s_j z_j\|^2 + \|z_i\|^2 + \langle \sum_{j=1}^{i-1}s_j z_j, s_i z_i\rangle \\
&\le& \sum_{j=1}^{i-1} \|z_j\|^2 + \|z_i\|^2 + s_i \sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|z_j\|^2 - |\sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle| \le \sum_{j=1}^{i} \|z_j\|^2
\end{eqnarray*}


Let us now translate this to an algorithm w.r.t.\ the $d$ dimensional points $x_1,\ldots, x_k$. 
The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle \phi(x_j), \phi(x_i) \rangle) = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j  K(x_j, x_i)) $$
This guarantees that for any $y \in \R^d$,
$$ \sum_i s_i K(x_i, y) \leq \sqrt{ \sum_i \|\phi(x_i)\|^2 } = \sqrt{k} $$

Before we extend this to a complete algorithm, it is worth getting an intuition for the meaning of this result.

For the coreset, this means that we can choose the $\tilde{x}$'s as either the $x$ vectors with positive $s_i$'s or negative $s_i$'s.




\section{General note about sketching and flat signed function}
In many sketching problems our goal is to approximate a function of the of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i$ are all the stream items and and $q$ is some query point. 
The goal is to produce a sketch of $\ell$ points $z$ and weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$. 
If $z \subset x$ this is a subset selection problem. 
For bounded functions $f$, uniform sampling combined with a union bound over the space of possbile values for $q$ always provides a valid solution.
However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 

Here are some examples:
\begin{itemize}
\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else.
\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else.
\item in matrix column subset selection we have $f(x, q) = \langle x,q \rangle ^2$.
\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see above)
\item in machine learning $f(x, q) = \operatorname{loss}(x,q)$ where $q$ is the parameters of the model and $x$ is a training example and its label.
\end{itemize}
For all of the above, we can find flat error functions $E(q) = \sum_{i=1}^{k} s_i f(x_i,q)$ such that $\sup_q E(q)$ is small. 
We argue that this gives a general mechanism for sketching in a diverse set of scenarios. 
\end{document}




Let us choose the value  and assume by induction that



%Specifically, we set $s_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $s_j$'s up to and not including $i$,
%$$\E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} s_j z_j$.
%$$ \E_{s_i,\ldots,s_k}[\| \sum_{j=1}^k s_j z_j \|^2] = \E[\| v + \sum_{j=i}^k s_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $s_i$ deterministically,
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + s_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] $$
%we need
%$$ 2\ip{m,s_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ s_i = -\text{sign} \ip{m,z_i}$$

