\documentclass{article} % For LaTeX2e
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{amsfonts}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{conjecture}{Conjecture}[section]
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{observation}[theorem]{Observation}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{fact}[theorem]{Fact}

\newcommand{\zk}[1]{\textcolor{red}{ZK: #1}}
\newcommand{\el}[1]{\textcolor{blue}{EL: #1}}

\newcommand{\ip}[1]{\left \langle #1 \right \rangle}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{\rho}
\newcommand{\eps}{\epsilon}
%\usepackage{ntheorem}



%\newtheorem*{definition*}{Definition}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}
%\newtheorem{theorem}{Theorem}


\title{Sketching for Density Estimation}
\date{\nonumber}

\author{
Zohar Karnin\\Amazon\\zkarnin@amazon.com
\and
Edo Liberty\\Amazon\\libertye@amazon.com
}

\begin{document}

\maketitle

\begin{abstract}
abc
\end{abstract}

\section{introduction}
def

\section{General framework for sketching}
In many sketching problems our goal is to approximate a function of the of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i$ are all the stream items and and $q$ is some query point. 
The goal is to produce a sketch of $\ell$ points $z$ and weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$. 
If $z \subset x$ this is a subset selection problem. 
For bounded functions $f$, uniform sampling combined with a union bound over the space of possible values for $q$ always provides a valid solution. 
While the number of possible queries $q$ is often infinite it often reduces to a finite set through a net argument. 
This is common practice in machine learning, specifically, PAC learning. 

However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 

Here are some examples:
\begin{itemize}
\item in approximate counting we have $f(x, q) = 1$ if $x=q$ and zero else. Here both $x$ and $q$ belong to some finite domain.
\item in quantile approximation we have $f(x, q) = 1$ if $x<q$ and zero else. Here, $x$ and $q$ belong to a set which exhibits strong ordering.
\item in matrix row subset selection we have $f(x, q) = \langle x,q \rangle ^2$ where $q$ is any unit vector in $\R^d$ and $x$ are matrix rows. For simplicity, we will assume in this manuscript that $x$ is also unit norm.
\item in density estimation $f(x, q) = \exp(- \|x-y\|_2^2/\sigma^2)$ or any other kernel (see \cite{})
\item in logistic regression $f(x, q) = \operatorname{loss}(x,q) = 1/(1 + \exp(-q^T x))$. 
Here $q$ is the parameters of the linear model and $x$ is a training example vector multiplied by its label ($1$ or $-1$).
\item Quadratic forms of the graph Laplacian are $f(x, q) = q^T L_x q$ where $x$ is an edge and $L_x$ is the graph laplacian and $q$ is a test vector.
\item Probably also good for matrix approx when we get entry updates. 
\end{itemize}

We argue that we can offer a unified solution for producing coresets for all of the above problems in a unified manner. 
Moreover, our solution created streaming algorithms with fully mergeable sketches. 

We begin with describing the basic operation of compaction and the notion of flat error functions. 

Let us consider a signed sum of error function $E(q) = \sum_{i=1}^{n} s_i f(x_i,q)$.
Now consider $\tilde F(q) = F(q) + \Delta(q) = \sum_{i=1}^{n} f(x_i,q)  + \sum_{i=1}^{n} s_i f(x_i,q)  = 2 \sum_{i | s_i=1} f(x_i,q)$.
Therefore $\tilde F(q)$ is a coreset for $F(q)$ with item weights of $2$ and error at most $|\tilde F(q) - F(q)| \le \max_q | \Delta(q)|$.
Note that the above is true for any choice of signs $s_i$, specifically, for those minimizing $\max_q | \Delta(q)|$.
We call $E$ flat if $\min_s \max_q |\Delta(q)| = o(n)$. 

\begin{definition}
We define $E_n$ with respect to a function $f(x,q)$ to be  $\min_s \max_q \max_x \sum_{i=1}^{n} s_i f(x_i,q)$. 
\end{definition}

As a warmup exercise, consider the case where $E_n = 1$.
This is the case for quantiles and for counting. 
At step one, we will create a coreset of weight $2$ and size at most $n/2$ (by either adding or subtracting $E$). Here we incur an error of at most $1$.
Then, we repeat the process and incur an error of at most $2$ resulting in a stream of length at most $n/4$. 
We can continue the above process until we are left with $O(1/\eps)$ points in the coreset. At this point, the error adds up to at most $\eps n$.
Similarly, when $E_n = O(\sqrt{n})$ we get a coreset of size $O(1/\eps^2)$ whose error is at most $\eps n$.
This gives an offline algorithm.

\el{We can describe the general streaming algorithm here. I think $E_n$ is a sufficient characterization of this.} 
For all of the above, we argue that we can find flat signed error functions such that $E_n = O(1)$ or $E_n = O(\sqrt{n})$.


\section{Density Estimation}

Given a collection of data points $X = x_1,\ldots, x_n$ in $\R^d$ the density function $\rho: \R^d \rightarrow \R$ of a point $y$ is defined as 
$$ \rho(y) = \sum_{i=1}^{n} K(x_i,y) $$
Here, $K$ is a \emph{positive definite kernel} function, typically based on the distance between $x,y$. The most frequent examples include

$$ K(x,y) = \exp(- \|x-y\|_2^2/\sigma^2)\;\;\; K(x,y) = \exp(-\lambda \|x-y\|/\sigma) \; \mbox{and}\;\;\; K(x,y) = (1+\|x-y\|_2^2/\sigma^2)^{-1}$$

where $\sigma$ is a scaling parameter. What we discuss in what follows applies for any kernel, an depends on a bound on $K(x,x)$. For simplicity we assume that $K(x,x) \leq 1$ for all datapoints. Notice that for any kernel based on distance we have $K(x,x)=1$ exactly for all $x \in \R^d$.


Using our framework, we need to bound $\Delta(y) = \sum_{i=1}^k s_i K(x_i,y)$. For any kernel $K$ there exist a mapping $\phi: \R^d \to {\cal V}$ to an inner product space $\cal V$ such that 
$$ K(x,y) = \ip{\phi(x), \phi(y)} $$
Using this function $\phi$ our objective function becomes
$$\sum_{i=1}^k s_i K(x_i,y) = \sum_{i=1}^k s_i \ip{\phi(x_i), \phi(y)} =  \ip{ \sum_{i=1}^k s_i \phi(x_i), \phi(y)} \leq \|\phi(y)\| \cdot \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| \leq  \left\|  \sum_{i=1}^k s_i \phi(x_i) \right\| $$
This upper bound allows us to optimize (find values for $s_i$) a simple expression.
Given $k$ vectors $z_1,\ldots,z_k$ in an inner product space, compute signs minimizing $\| \sum_i s_i z_i \|$.
We first notice that for i.i.d.\ uniform signs, it holds that
$$\E[\| \sum_i s_i z_i \|^2] = \E[\sum_{i,j} s_i s_j \ip{z_i, z_j}] = \sum_i \|z_i\|^2 $$
leading to a randomized approach. By Markov with probability at least $3/4$ we have $\| \sum_i s_i z_i \| \le 2\sum_i \|z_i\|$.
%
We can also use the method of expectation minimization to achieve this bound deterministically.
Specifically, we will guaranty that $\| \sum_{j=1}^i s_j z_j \|^2 \le \sum_{j=1}^i \|z_j\|^2$ for all $i$.
This is trivially true for $i=1$. 
For another value $i$ set $s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle z_j, z_i \rangle)$ 
and  assume by induction that $\| \sum_{j=1}^{i-1} s_j z_j\|^2 \le \sum_{j=1}^{i-1} \|z_j\|^2$.

\begin{eqnarray*}
\| \sum_{j=1}^{i}s_j z_j\|^2 &=& \|\sum_{j=1}^{i-1}s_j z_j\|^2 + \|z_i\|^2 + \langle \sum_{j=1}^{i-1}s_j z_j, s_i z_i\rangle \\
&\le& \sum_{j=1}^{i-1} \|z_j\|^2 + \|z_i\|^2 + s_i \sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle \mbox{\;\;\; by the induction assumption}\\ 
&=& \sum_{j=1}^{i} \|z_j\|^2 - |\sum_{j=1}^{i-1}s_j \langle  z_j,  z_i\rangle| \le \sum_{j=1}^{i} \|z_j\|^2
\end{eqnarray*}
This completes the proof that $\| \sum_{j=1}^{k}s_j z_j\| \le \sqrt{\sum_{j=1}^{i} \|z_j\|^2}$ for our choice of values for $s$. 
%
Let us now translate this to an algorithm w.r.t.\ the $d$ dimensional points $x_1,\ldots, x_k$. 
The sign of $x_1$ is set arbitrarily as $s_1=1$. For $i>1$, we choose 
$$ s_i = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j \langle \phi(x_j), \phi(x_i) \rangle) = -\operatorname{sign} (\sum_{j=1}^{i-1}s_j  K(x_j, x_i))$$
This guarantees that 
$$ \forall y \in \R^d \;\;\; \sum s_i K(x_i, y) \leq \sqrt{ \sum \|\phi(x_i)\|^2 } = \sqrt{k} $$

Using the framework above this provides a streaming coreset construction for kernel density estimation of size $O(1/\eps^2)$ such that 
$\forall \;q\;\; |\tilde \rho(q) - \rho(q)| \le \eps n$.
This matches the results achieved by \cite{DBLP:conf/soda/PhillipsT18} and \cite{DBLP:journals/corr/abs-1802-01751}.
\el{The running time of this algorithm is amortized.... I hope it will be better than Jeff's result, especially in the randomized shrinking buffers setting.}





%
%<<<<<<< HEAD
%\section{General note about sketching and flat signed function}
%In many sketching problems our goal is to approximate a function of the of the form $F(q) = \sum_{i=1}^{n} f(x_i, q)$ where $x_i$ are all the stream items and and $q$ is some query point. 
%The goal is to produce a sketch of $\ell$ points $z$ and weights $w$ such that $\tilde F(q) = \sum_{i=1}^{\ell}w_i f(z_i,q) \approx \sum_{i=1}^{n} f(x_i, q)$. 
%If $z \subset x$ this is a subset selection problem. 
%For bounded functions $f$, uniform sampling combined with a union bound over the space of possbile values for $q$ always provides a valid solution.
%However, it is often far from being the best possible in terms of its space-accuracy tradeoff. 
%=======
%
%>>>>>>> ce3a786138737689452723acbd3980a5d8533fbe



\section{Row Subset Selection}
Assume you are getting the rows $x_i$  of matrix $X$ in a stream and you want to compute $Z$ such that $\|Z^TZ - X^TX\| \le \eps n$.
for simplicity, assume $x_i$ are all unit length. Moreover, assume each vector $Z$ must be one of the rows in $X$ up to some constant factor.
This is called the row subset selection problem. 
Since you are trying to minimize the quadratic form, you get that $f(x, q) = \langle x,q \rangle ^2$ and 
$$E(y) = \sum_i s_i \langle x_i,q \rangle ^2 = q^T (\sum_i s_i x_i x_i^T ) q \le \|\sum_i s_i x_i x_i^T\|$$
When summing rank one projections matrices $x_i x_i^T$ you can think of two extreme cases. 
If $x_i$ are orthogonal to each other than $\|\sum_i s_i x_i x_i^T\| = 1$ independently of $s_i$.
If $x_i$ are all equal than a random assignment would get  $\E[\|\sum_i s_i x_i x_i^T\|] = E[|\sum_i s_i|] = \Omega(\sqrt{k})$.
But, in that case it's also easy to find a sequence of $s_i$ which gives $\|\sum_i s_i x_i x_i^T\| = O(1)$. Is that always possible?
\el{My experiments show that behaves like polylog $d$ and $n$ but who knows. I thought I proved $O(1)$ but I think it's wrong.}
If this is correct, it would give a covariance sketch is subset selection of size $1/\eps$ which would be an improvement over frequent directions.

Selecting $o(1/\eps^2)$ is claimed to be hard by \cite{DBLP:conf/focs/DeshpandeR10} and \cite{DBLP:conf/soda/DeshpandeRVW06}. We need to see if these hold in our setting or only hold for sampling.  Also, \cite{DBLP:conf/soda/GhashamiP14} showed that the impossibility result is limited to frequent directions like settings.

\paragraph{Hard example?}
$x_i$'s are gaussian vectors with mean zero and covariance $\sum_{i=1}^d \sqrt{i} \cdot e_i e_i^T$. It might be the case that any sign assignment here will result in $\sqrt{d}$ operator norm. 

\paragraph{Reduction to the Kernel case}
This is really the Kernel density estimation problem with $K(x,y) = \ip{x,y}^2$. We might be able to squeeze a bit more due to the fact that $\phi(x) = xx^T$ is a very special $d \times d$ matrix: a rank $1$ matrix. That being said, a coreset of $1/\eps^2$ can be obtained by simply using the kernel technique. The special structure can either give better dependence on $\eps$ or exploit low rank structure e.g.\ if the vectors form a nearly rank $k$ matrix.

\zk{The Kernel argument will get us $\sqrt{k}$ and I think we really should get $\sqrt{d}$ which is possibly better.}
%
%\subsection{notes about row selection}
%The problem is 
%$$ \min_s \| \sum_i s_i x_ix_i^T\| $$
%where $\|x_i\| \leq 1$. Consider the following greedy approach: At point $x_i$, let
%$$ C_i = \sum_{j<i} s_j x_j x_j^T, \ \  l_+ = \|C_i + x_i x_i^T \|, \ l_- = \|C_i - x_i x_i^T\|$$
%We choose $s_i = 1$ if $l_+ < l_i$, otherwise $s_i=-1$.
%
%This approach can be shown to be better than random. Random will result in an answer of up to 
%
%Consider the following example. 
%$$x_1=e_1, x_2=e_2*\eps, x_3=e_2, x_4=e_1/\sqrt{2} + e_2/\sqrt{2}$$
%where $\eps \to 0$. We get that the covariance matrix after inserting $x_3$ has two eigenvalues of +1 and -1. Now when inserting $x_4$ we have to increase the magnitude of one of them and we exceed the limit of 1. 
%
\bibliographystyle{plain}
\bibliography{density}

\section{Appendix 1}
For completeness we recap a result from \cite{barany2008}.
\begin{theorem}[Simplified from B\'ar\'any \cite{barany2008} Theorem 4.1]
For any set of vector $x_1,...,x_n$ in $\R^d$ such that $\|x_i\| \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i s_i x_i || \le \sqrt{d}$.
\end{theorem}
\begin{proof}
Consider the feasible region for $\sum_i \alpha_i x_i = 0$ and all $\alpha_i \in [-1,1]$.
This feasible region is not empty because it contains the origin. 
Consider an extreme point $\alpha^*_i$ and set $s_i = 1$ w.p.\ $(1+\alpha^*_i)/2$ and $s_i = -1$ w.p.\ $(1+\alpha^*_i)/2$.
$$
\E[\|\sum s_i x_i\|^2] =  \E[\|\sum (s_i - \alpha^*_i) x_i\|^2] = \sum \E[(s_i - \alpha^*_i)^2] \|x_i\|^2= \sum (1-(\alpha_i^*)^2) \le d
$$
The last inequality holds because the values $\alpha^*_i$ are non-integer in at most $d$ places. 
Assuming otherwise would entail more than $d$ linearly independent vectors in $\R^d$ due to $\alpha^*$ being an extreme point.
\end{proof}

\noindent The conjectures appear to be provable in almost exactly the same manner
\begin{conjecture}
For any set of matrices $X_1,...,X_n$ in $\R^{d_1 \times d_2}$ such that $\|X_i\|_{2,F} \le 1$ there exists a set of signs $s_1,...,s_n$ such that $||\sum_i s_i X_i||_{2,F} \le O(\sqrt{d_1d_2\log(d_1 + d_2) })$.
\end{conjecture}
\begin{conjecture}
For any set of vector $x_1,...,x_n$ in $\R^d$ such that $\|x_i\| \le 1$ there exists a set of 
signs $s_1,...,s_n$ such that $||\sum_i s_i x_i x^T||_2 \le d\sqrt{\log{d}}$.
Here, $\|\cdot\|_2$ denotes the spectral norm of a matrix. \el{This us weak. It really should be $\sqrt{d}$ or at most $\sqrt{d\log{d}}$ }
\end{conjecture}


\end{document}


%Specifically, we set $s_i$ sequentially for $i$ while minimizing the expectation. 
%We prove by induction on $i$ that for the fixed $s_j$'s up to and not including $i$,
%$$\E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \leq \sum_j \|z_j\|^2$$
%For $i=1$ this is trivial. For $i>1$ let $v=\sum_{j=1}^{i-1} s_j z_j$.
%$$ \E_{s_i,\ldots,s_k}[\| \sum_{j=1}^k s_j z_j \|^2] = \E[\| v + \sum_{j=i}^k s_j z_j \|^2] = \|v\|^2 + \sum_{j=i}^k \|z_j\|^2$$
%It follows that by fixing $s_i$ deterministically,
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] - \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] = \|v\|^2 + \|z_i\|^2 - \|v + s_i z_i\|^2  $$
%so in order to make sure that 
%$$ \E_{s_i,\ldots,s_k}[\| \sum_j s_j z_j \|^2] \geq \E_{s_{i+1},\ldots,s_k}[\| \sum_j s_j z_j \|^2] $$
%we need
%$$ 2\ip{m,s_i z_i} = \|m\|^2 + \|z_i\|^2 - \|m + s_i z_i\|^2 \geq 0 $$
%We thus set 
%$$ s_i = -\text{sign} \ip{m,z_i}$$

